

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/blog/img/favicon.jpg">
  <link rel="icon" href="/blog/img/favicon.jpg">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Azure">
  <meta name="keywords" content="">
  
    <meta name="description" content="论文">
<meta property="og:type" content="article">
<meta property="og:title" content="经典论文">
<meta property="og:url" content="https://yoonalis.github.io/blog/2023/12/01/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87/index.html">
<meta property="og:site_name" content="Azure&#39;s blog">
<meta property="og:description" content="论文">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://yoonalis.github.io/blog/img/transformer.jpg">
<meta property="article:published_time" content="2023-12-01T12:08:05.937Z">
<meta property="article:modified_time" content="2023-12-26T12:33:50.311Z">
<meta property="article:author" content="Azure">
<meta property="article:tag" content="论文、深度学习、nlp、transformer">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://yoonalis.github.io/blog/img/transformer.jpg">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>经典论文 - Azure&#39;s blog</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hint.css@2/hint.min.css" />

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/blog/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/blog/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/blog/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"yoonalis.github.io","root":"/blog/","version":"1.9.3","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/blog/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/blog/js/utils.js" ></script>
  <script  src="/blog/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/blog/">
      <strong>Azure</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog/album/">
                <i class="iconfont icon-images"></i>
                album
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog/links/">
                <i class="iconfont icon-link-fill"></i>
                友链
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/blog/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="经典论文"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-12-01 20:08" pubdate>
          2023年12月1日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          <!-- compatible with older versions-->
          137k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          <!-- compatible with older versions-->
          1143 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">经典论文</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="经典论文"><a href="#经典论文" class="headerlink" title="经典论文"></a>经典论文</h1><h2 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h2><p>损失函数：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/35709485">https://zhuanlan.zhihu.com/p/35709485</a></p>
<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><h3 id="原始论文翻译"><a href="#原始论文翻译" class="headerlink" title="原始论文翻译"></a>原始论文翻译</h3><p>论文原文：<a target="_blank" rel="noopener" href="https://link.jianshu.com/?t=https://arxiv.org/abs/1706.03762">Attention is all you need</a></p>
<p>这篇论文是Google于2017年6月发布在arxiv上的一篇文章，现在用attention处理序列问题的论文层出不穷，本文的创新点在于抛弃了之前传统的encoder-decoder模型必须结合cnn或者rnn的固有模式，只用attention，可谓大道至简。文章的主要目的是在减少计算量和提高并行效率的同时不损害最终的实验结果，创新之处在于提出了两个新的Attention机制，分别叫做 Scaled Dot-Product Attention 和 Multi-Head Attention.</p>
<h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>现在主流的序列转导(transduction)模型不是基于复杂的循环网络或就是基于卷积网络，它们都包含一个编码器和一个解码器。表现最好的模型也是通过一个注意力机制来连接编码器和解码器。我们提出一个新的简单的网络架构，Transformer，仅基于注意力机制，完全抛弃了循环和卷积网络。在两个机器翻译任务上的实验表面该模型在质量上更加优越，更易于并行训练从而训练时间大大减少。我们的模型在WMT2014英-德翻译任务上达到28.4BLEU得分，超过了现有最好的结果2分，包括通过集成学习实现的模型。在WMT2014英-法翻译任务上，我们的模型建立了一个新的单一模型最好的BLEU得分——41.8分，在8个GPU上训练了3.5天，这个训练时间只是目前记载的最好的模型训练成本的一小部分。基于有限的训练集或大的训练集，我们的模型成功的应用到英语选区分析(constituency parsing)任务上，说明Transformer模型能很好的推广到其他任务中。</p>
<h4 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h4><p>循环神经网络，尤其是LSTM和GRU，一直以来都在序列建模和转导问题(比如语言模型和机器翻译)上保持统治地位。此后，人们不断努力提升循环网络语言模型和编码器-解码器结构的瓶颈。</p>
<p>循环模型通常是对输入和输出序列的符号位置进行因子计算。在计算期间对齐位置和时间步，基于前一时间步的隐藏状态<img src="https://math.jianshu.com/math?formula=h_%7Bt-1%7D" srcset="/blog/img/loading.gif" lazyload alt="h_{t-1}">和当前时间步<img src="https://math.jianshu.com/math?formula=t" srcset="/blog/img/loading.gif" lazyload alt="t">的输入，循环生成了一系列隐藏状态<img src="https://math.jianshu.com/math?formula=h_t" srcset="/blog/img/loading.gif" lazyload alt="h_t">。这种固有的顺序特性排除了训练的并行化，这在更长的序列中成为重要问题，因为有限的内存限制了长样本的批次化。虽然最近有学者通过因子分解和条件计算技巧重大的提升了计算效率，同时提升了模型的表现。但是序列计算的基本限制仍然存在。</p>
<p>注意力机制已经变成了序列建模和各种任务中的转导模型的必备成分，允许为依赖建模而不必考虑输入和输出序列中的距离远近。除了少数情况外，这种注意力机制都与循环神经网路结合使用。</p>
<p>本文我们提出了Transformer，一个移除循环网络、完全基于注意力机制来为输入和输出的全局依赖建模的模型。Transformer 允许更多的并行化，并且翻译质量可以达到最牛逼水平，只需要在8个P100 GPU上训练12个小时。</p>
<h4 id="2-背景"><a href="#2-背景" class="headerlink" title="2 背景"></a>2 背景</h4><p>减少序列计算的目标形成了扩展的神经网络GPU、ByteNet和ConvS2S的基础，但是它们都是基于CNN，CNN可以为所有的输入和输出并行地计算隐藏状态。在这些模型中，连接来自两个任意输入或输出位置的信号所需的操作数随着位置之间的距离增加而增加，在ConvS2S中是线性增加，在ByteNet中是对数增加。这使得很难学到远距离依赖。在我们提出的Transformer模型中，这种操作减少到固定的次数，尽管由于注意力加权位置平均化而降低了效果，但我们用多头注意力(Multi-Head Attention)抵消这种影响。</p>
<p>自注意(Self-attention)，有时被称为intra-attention，是一种关联序列的不同位置计算序列表示的注意力机制。自注意已经被成功地应用于各种任务，包括阅读理解、摘要抽取、文本蕴含和学习与任务无关的句子表征。端到端内存网络基于循环注意机制，而不是序列对齐的循环，已被证明在简单语言的问题回答和语言建模任务中表现良好。</p>
<p>基于循环注意力机制而不是序列对齐循环结构的端到端内存网络，被证明在简单语言问答和语言建模任务中表现良好。</p>
<p>然而，据我们所知，Transformer是第一个完全依靠自注意而没有使用序列对齐的RNN或卷积网络去计算输入和输出表示的转导模型。下面，我们会描述Transformer、(motivate)自注意，然后讨论它的优势。</p>
<h4 id="3-模型架构"><a href="#3-模型架构" class="headerlink" title="3 模型架构"></a>3 模型架构</h4><p>大部分有竞争力的神经网络序列转导模型都有一个编码器-解码器(Encoder-Decoder)结构。编码器映射一个用符号表示的输入序列<img src="https://math.jianshu.com/math?formula=(x_1,%5Ccdots,x_n)" srcset="/blog/img/loading.gif" lazyload alt="(x_1,\cdots,x_n)">到一个连续的序列表示<img src="https://math.jianshu.com/math?formula=z=(z_1,%5Ccdots,%20z_n)" srcset="/blog/img/loading.gif" lazyload alt="z=(z_1,\cdots, z_n)">。给定<img src="https://math.jianshu.com/math?formula=z" srcset="/blog/img/loading.gif" lazyload alt="z">，解码器生成符号的一个输出序列<img src="https://math.jianshu.com/math?formula=(y_1,%5Ccdots,y_m)" srcset="/blog/img/loading.gif" lazyload alt="(y_1,\cdots,y_m)">，一次生成一个元素。在每个时间步，模型是自回归(auto-regressive)的，在生成下个输出时消耗上一次生成的符号作为附加的输入。</p>
<p>Transformer沿用该结构并在编码器和解码器中都使用叠加的自注意和基于位置的全连接网络，分别对应图1左半部和右半部。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/26691889-e8d78dbce2bf1371.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/532" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>图1：Transformer模型架构</p>
<h4 id="3-1-编码器和解码器栈"><a href="#3-1-编码器和解码器栈" class="headerlink" title="3.1 编码器和解码器栈"></a>3.1 编码器和解码器栈</h4><p><strong>编码器</strong>： 编码器是由<img src="https://math.jianshu.com/math?formula=N=6" srcset="/blog/img/loading.gif" lazyload alt="N=6">个相同的层(参数独立)堆叠而成的。每层都有两个子层(sub-layer)，第一个子层是多头注意力层，第二个是简单的基于位置的全连接前馈神经网络。我们在两个子层周围先进行残差连接，然后进行层归一化(Layer Normalization)。这样，我们每个子层的输出是<img src="https://math.jianshu.com/math?formula=LayerNorm(x%20+%20Sublayer(x))" srcset="/blog/img/loading.gif" lazyload alt="LayerNorm(x + Sublayer(x))">，其中<img src="https://math.jianshu.com/math?formula=Sublayer(x)" srcset="/blog/img/loading.gif" lazyload alt="Sublayer(x)">是子层自己实现的函数。为了利用残差连接，该模型中的所有子层和嵌入层，输出的维度都统一为<img src="https://math.jianshu.com/math?formula=d_%7Bmodel%7D=512" srcset="/blog/img/loading.gif" lazyload alt="d_{model}=512">。</p>
<p><strong>解码器</strong>： 解码器也是由<img src="https://math.jianshu.com/math?formula=N=6" srcset="/blog/img/loading.gif" lazyload alt="N=6">个相同的层堆叠而成。解码器和编码器类似，除了编码器中的两个子层外，解码器增加了第三个子层，它对编码器栈输出进行多头注意力计算。我们也应用残差连接在每个子层的周围，然后是层归一化。我们还修改了解码器栈中的自注意子层，以防止当前位置注意到后面的位置。这种屏蔽(masking)，加上输出嵌入向量偏移一个位置的事实，确保了对位置<img src="https://math.jianshu.com/math?formula=i" srcset="/blog/img/loading.gif" lazyload alt="i">的预测只能依赖于位置小于<img src="https://math.jianshu.com/math?formula=i" srcset="/blog/img/loading.gif" lazyload alt="i">的已知输出。</p>
<h4 id="3-2-注意力"><a href="#3-2-注意力" class="headerlink" title="3.2 注意力"></a>3.2 注意力</h4><p>注意力函数可以说是匹配一个query和一系列key-value对到一个输出的函数。其中的query,key,value和输出都是向量。value的加权和得到输出，每个value的权重是通过一个query和相应key的某个函数计算。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/26691889-8eef8a103afec2fb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/740" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>图2：(左边)缩放点乘注意力 (右边)包含几个并行计算注意力层的多头注意力</p>
<h4 id="3-2-1-缩放点乘注意力"><a href="#3-2-1-缩放点乘注意力" class="headerlink" title="3.2.1 缩放点乘注意力"></a>3.2.1 缩放点乘注意力</h4><p>我们称我们这种特定的注意力为缩放点乘注意力(上图左边)。输入query和key的维度是<img src="https://math.jianshu.com/math?formula=d_k" srcset="/blog/img/loading.gif" lazyload alt="d_k">，value的维度是<img src="https://math.jianshu.com/math?formula=d_v" srcset="/blog/img/loading.gif" lazyload alt="d_v">。我们计算query和所有key的点乘结果，然后除以<img src="https://math.jianshu.com/math?formula=%5Csqrt%7Bd_k%7D" srcset="/blog/img/loading.gif" lazyload alt="\sqrt{d_k}">，最后应用一个softmax函数就得到value的相应权重。</p>
<p>在实践中，我们同时计算一组query的注意力函数，这一组query被压缩到一个矩阵<img src="https://math.jianshu.com/math?formula=Q" srcset="/blog/img/loading.gif" lazyload alt="Q">，key和value也分别被压缩到矩阵<img src="https://math.jianshu.com/math?formula=K" srcset="/blog/img/loading.gif" lazyload alt="K">和<img src="https://math.jianshu.com/math?formula=V" srcset="/blog/img/loading.gif" lazyload alt="V">。我们通过下面的公式计算输出矩阵：<br><img src="https://math.jianshu.com/math?formula=%5Ctext%7BAttention%7D(Q,K,V)%20=%20%5Ctext%7Bsoftmax%7D(%5Cfrac%7BQK%5ET%7D%7B%5Csqrt%7Bd_k%7D%7D)V%20%5Ctag%7B1%7D" srcset="/blog/img/loading.gif" lazyload alt="\text{Attention}(Q,K,V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V \tag{1}"><br>最常用的注意力函数是Bahdanau注意力，和点乘注意力。点乘注意力除了没有通过<img src="https://math.jianshu.com/math?formula=%5Cfrac%7B1%7D%7B%5Csqrt%7Bd_k%7D%7D" srcset="/blog/img/loading.gif" lazyload alt="\frac{1}{\sqrt{d_k}}">缩放外，和我们算法中的注意力函数相同。Bahdanau注意力通过一个单隐藏层的全连接网络计算。尽管这两个函数的复杂度都是相似的，但是点乘注意力在实际中更快、更节省空间。因为它能通过高度优化的矩阵乘法实现。</p>
<p>尽管在<img src="https://math.jianshu.com/math?formula=d_k" srcset="/blog/img/loading.gif" lazyload alt="d_k">值不大的情况下，两者性能差不多，Bahdanau注意力超过没有对大的<img src="https://math.jianshu.com/math?formula=d_k" srcset="/blog/img/loading.gif" lazyload alt="d_k">值缩放的点乘注意力，我们认为，对于大的<img src="https://math.jianshu.com/math?formula=d_k" srcset="/blog/img/loading.gif" lazyload alt="d_k">值，点乘的结果也变得非常大，导致softmax函数到极其小梯度的区域，为了防止这点，我们缩放点积结果到<img src="https://math.jianshu.com/math?formula=%5Cfrac%7B1%7D%7B%5Csqrt%7Bd_k%7D%7D" srcset="/blog/img/loading.gif" lazyload alt="\frac{1}{\sqrt{d_k}}">。</p>
<h4 id="3-2-2-多头注意力"><a href="#3-2-2-多头注意力" class="headerlink" title="3.2.2 多头注意力"></a>3.2.2 多头注意力</h4><p>我们发现将query、key和value分别用不同的、学到的线性映射<img src="https://math.jianshu.com/math?formula=h" srcset="/blog/img/loading.gif" lazyload alt="h">倍到<img src="https://math.jianshu.com/math?formula=d_k" srcset="/blog/img/loading.gif" lazyload alt="d_k">、<img src="https://math.jianshu.com/math?formula=d_k" srcset="/blog/img/loading.gif" lazyload alt="d_k">和<img src="https://math.jianshu.com/math?formula=d_v" srcset="/blog/img/loading.gif" lazyload alt="d_v">维效果更好，而不是用<img src="https://math.jianshu.com/math?formula=d_%7Bmodel%7D" srcset="/blog/img/loading.gif" lazyload alt="d_{model}">维的query、key和value执行单个attention函数。 基于每个映射版本的query、key和value，我们并行执行attention函数，产生<img src="https://math.jianshu.com/math?formula=d_v" srcset="/blog/img/loading.gif" lazyload alt="d_v">维输出值。 将它们连接并再次映射，产生最终值，如图2(右边)所示。</p>
<p>多头注意力允许模型能对齐不同表示子空间信息到不同的位置。而普通的只有一个头的注意力会因为求平均而抑制了这一点。</p>
<p><img src="https://math.jianshu.com/math?formula=%5Ctext%7BMultiHead%7D(Q,%20K,%20V)%20=%20%5Ctext%7BConcat%7D(%5Ctext%7Bhead%7D_1,%20%5Ccdots,%20%5Ctext%7Bhead%7D_h)%20W%5EO" srcset="/blog/img/loading.gif" lazyload alt="\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \cdots, \text{head}_h) W^O"><br>其中<br><img src="https://math.jianshu.com/math?formula=%5Ctext%7Bhead%7D_i%20=%20%5Ctext%7BAttention%7D(QW_i%5EQ,%20KW_i%5EK,%20VW_i%5EV)," srcset="/blog/img/loading.gif" lazyload alt="\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V),"></p>
<p>参数矩阵<img src="https://math.jianshu.com/math?formula=W%5EQ_i%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bd_%7B%5Ctext%7Bmodel%7D%7D%20%5Ctimes%20d_k%7D" srcset="/blog/img/loading.gif" lazyload alt="W^Q_i \in \mathbb{R}^{d_{\text{model}} \times d_k}">, <img src="https://math.jianshu.com/math?formula=W%5EK_i%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bd_%7B%5Ctext%7Bmodel%7D%7D%20%5Ctimes%20d_k%7D" srcset="/blog/img/loading.gif" lazyload alt="W^K_i \in \mathbb{R}^{d_{\text{model}} \times d_k}">, <img src="https://math.jianshu.com/math?formula=W%5EV_i%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bd_%7B%5Ctext%7Bmodel%7D%7D%20%5Ctimes%20d_v%7D" srcset="/blog/img/loading.gif" lazyload alt="W^V_i \in \mathbb{R}^{d_{\text{model}} \times d_v}"> and <img src="https://math.jianshu.com/math?formula=W%5EO%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bhd_v%20%5Ctimes%20d_%7B%5Ctext%7Bmodel%7D%7D%7D" srcset="/blog/img/loading.gif" lazyload alt="W^O \in \mathbb{R}^{hd_v \times d_{\text{model}}}">。</p>
<p>在本文中，我们设置<img src="https://math.jianshu.com/math?formula=h=8" srcset="/blog/img/loading.gif" lazyload alt="h=8">个并行的注意力层，或注意力头。每个头中的<img src="https://math.jianshu.com/math?formula=d_k=d_v=d_%7B%5Ctext%7Bmodel%7D%7D/h=64" srcset="/blog/img/loading.gif" lazyload alt="d_k=d_v=d_{\text{model}}/h=64">，由于每个头维度的减少，总的计算量和正常维度的单头注意力差不多(<img src="https://math.jianshu.com/math?formula=8%20%5Ctimes%2064%20=512" srcset="/blog/img/loading.gif" lazyload alt="8 \times 64 =512">)。</p>
<h4 id="3-2-3-注意力在我们模型中的应用"><a href="#3-2-3-注意力在我们模型中的应用" class="headerlink" title="3.2.3 注意力在我们模型中的应用"></a>3.2.3 注意力在我们模型中的应用</h4><p>Transformer以三种方式使用多头注意力：</p>
<ol>
<li>在编码器-解码器注意力层，query来自前一个解码器层，key和value来编码器输出。这允许解码器中每个位置能注意到输入序列中所有位置。这模仿了seq2seq模型中的典型的编码器-解码器的注意力机制。</li>
<li>编码器中的自注意层。在自注意层中，所有的key,value和query都来自同一个地方，在这里是编码器中前一层的输出，编码器中每个位置都能注意到编码器前一层的所有位置。</li>
<li>类似地，解码器中的自注意层允许解码器中的每个位置注意解码器中直到并包括该位置的所有位置。我们需要防止解码器中的左向信息流以保持自回归(auto-regressive)属性。我们在缩放点乘注意力中实现这点，通过屏蔽(mask)softmax的输入中所有不合法连接的值(设置为<img src="https://math.jianshu.com/math?formula=-%5Cinfty" srcset="/blog/img/loading.gif" lazyload alt="-\infty">)。</li>
</ol>
<h4 id="3-3-基于位置的前馈网络"><a href="#3-3-基于位置的前馈网络" class="headerlink" title="3.3 基于位置的前馈网络"></a>3.3 基于位置的前馈网络</h4><p>除了注意力子层，我们编码器和解码器中每个层都包含一个全连接前馈网络，它单独且相同地应用于每个位置。它包含了两个线性变换，其中有一个ReLU激活。<br><img src="https://math.jianshu.com/math?formula=%5Ctext%7BFFN%7D(x)%20=%20%5Cmax(0,%20xW_1%20+%20b_1)%20W_2%20+%20b_2" srcset="/blog/img/loading.gif" lazyload alt="\text{FFN}(x) = \max(0, xW_1 + b_1) W_2 + b_2"><br>尽管线性变换对于不同位置来说是相同的，但它们使用层与层之间的不同参数。还可以说是两个内核大小为1的卷积层。该前馈网络中输入和输出的维度是<img src="https://math.jianshu.com/math?formula=d_%7Bmodel%7D=512" srcset="/blog/img/loading.gif" lazyload alt="d_{model}=512">，内部层的维度<img src="https://math.jianshu.com/math?formula=d_%7Bff%7D=2048" srcset="/blog/img/loading.gif" lazyload alt="d_{ff}=2048">。</p>
<blockquote>
<p>FFN引入了非线性，变换了注意力输出的空间, 从而增加了模型的表现能力。</p>
</blockquote>
<h4 id="3-4-嵌入层和Softmax"><a href="#3-4-嵌入层和Softmax" class="headerlink" title="3.4 嵌入层和Softmax"></a>3.4 嵌入层和Softmax</h4><p>与其他序列转导模型类似，我们使用学习的嵌入层去转换输入和输出单词到<img src="https://math.jianshu.com/math?formula=d_%7Bmodel%7D" srcset="/blog/img/loading.gif" lazyload alt="d_{model}">维的词向量。我们也使用常用的线性转换和softmax函数来预测下一个单词的概率。在我们的模型中，我们在两个嵌入层之间和pre-softmax线性转换之间共享同样的权重矩阵。在嵌入层中，我们把它的权重乘了<img src="https://math.jianshu.com/math?formula=%5Csqrt%7Bd_%7Bmodel%7D%7D" srcset="/blog/img/loading.gif" lazyload alt="\sqrt{d_{model}}">。</p>
<h4 id="3-5-位置编码"><a href="#3-5-位置编码" class="headerlink" title="3.5 位置编码"></a>3.5 位置编码</h4><p>因为我们的模型不包含循环和卷积，为了使用序列顺序信息，我们必须接入一些关于序列中单词相对或绝对位置的信息 。为此，我们将位置编码添加到编码器和解码器栈底部的输入词嵌入中。位置编码和词嵌入有相同的维度<img src="https://math.jianshu.com/math?formula=d_%7Bmodel%7D" srcset="/blog/img/loading.gif" lazyload alt="d_{model}">，所以它们可以求和。有多种位置编码可以选择，例如通过学习得到的和固定的位置编码。</p>
<p>本文中，我们使用不同频率的正弦和余弦函数来表示位置编码：<br><img src="https://math.jianshu.com/math?formula=PE_%7B(pos,%202i)%7D%20=%20%5Csin(%20pos%20/%2010000%5E%7B2i%20/%20d_%7Bmodel%7D%20%7D)%20%5C%5C%20PE_%7B(pos,%202i+1)%7D%20=%20%5Ccos(%20pos%20/%2010000%5E%7B2i%20/%20d_%7Bmodel%7D%20%7D)" srcset="/blog/img/loading.gif" lazyload alt="PE_{(pos, 2i)} = \sin( pos / 10000^{2i / d_{model} }) \\ PE_{(pos, 2i+1)} = \cos( pos / 10000^{2i / d_{model} })"><br>其中<img src="https://math.jianshu.com/math?formula=pos" srcset="/blog/img/loading.gif" lazyload alt="pos">表示位置，<img src="https://math.jianshu.com/math?formula=i" srcset="/blog/img/loading.gif" lazyload alt="i">表示维度。也就是说，位置编码的每个维度都对应一个正弦曲线。波长形成一个从<img src="https://math.jianshu.com/math?formula=2%20%5Cpi" srcset="/blog/img/loading.gif" lazyload alt="2 \pi">到<img src="https://math.jianshu.com/math?formula=10000%20%5Ccdot%20%5Cpi" srcset="/blog/img/loading.gif" lazyload alt="10000 \cdot \pi">的等比数列。我们之所以选择这个函数，是因为我们假设这个函数可以让模型很容易地学到相对位置的注意力，因为对于任何固定的偏移量<img src="https://math.jianshu.com/math?formula=k" srcset="/blog/img/loading.gif" lazyload alt="k">，<img src="https://math.jianshu.com/math?formula=PE_%7Bpos%20+%20k%7D" srcset="/blog/img/loading.gif" lazyload alt="PE_{pos + k}">都能表示为<img src="https://math.jianshu.com/math?formula=PE_%7Bpos%7D" srcset="/blog/img/loading.gif" lazyload alt="PE_{pos}">的一个线性函数。</p>
<p>我们也实验了学过的位置嵌入，然后发现这两种方式产生了几乎同样的结果(见下表3 行<img src="https://math.jianshu.com/math?formula=(E)" srcset="/blog/img/loading.gif" lazyload alt="(E)">)。我们选择正弦版本是因为允许模型推断比训练期间遇到的更长的序列。</p>
<h4 id="4-为什么选择自注意"><a href="#4-为什么选择自注意" class="headerlink" title="4 为什么选择自注意"></a>4 为什么选择自注意</h4><p>在本节中，我们将自注意层的各个方面和通常用于映射一个变长序列的符号表示<img src="https://math.jianshu.com/math?formula=(x_1,%5Ccdots,x_n)" srcset="/blog/img/loading.gif" lazyload alt="(x_1,\cdots,x_n)">到另一个同样长度的序列<img src="https://math.jianshu.com/math?formula=(z_1,%5Ccdots,z_n)" srcset="/blog/img/loading.gif" lazyload alt="(z_1,\cdots,z_n)">的循环层和卷积层进行比较，其中<img src="https://math.jianshu.com/math?formula=x_i,z_i%20%5Cin%20%5CBbb%7BR%7D%5Ed" srcset="/blog/img/loading.gif" lazyload alt="x_i,z_i \in \Bbb{R}^d">，例如在典型的序列转导编码器或解码器中的一个隐藏层。我们使用自注意是考虑到满足三个需求。</p>
<p>一个是每层的计算复杂度。另一个是能并行的计算量，通过所需的最小顺序操作数来衡量。</p>
<p>第三个是网络中长距离依赖的路径长度。学习长距离依赖是很多序列转导任务的核心挑战。一个影响学习长距离依赖的关键因素是信号在网络中前向与反向传播的路径长度。计算输入和输出序列任意组合位置的路径越短，就越容易学到长距离依赖。因此我们也比较了由不同类型层组成的网络中任意两个输入和输出位置的最大路径长度。</p>
<p>表1：最大路径长度，对于不同类型层来说每层复杂度和最小顺序操作数。<img src="https://math.jianshu.com/math?formula=n" srcset="/blog/img/loading.gif" lazyload alt="n">是序列长度，<img src="https://math.jianshu.com/math?formula=d" srcset="/blog/img/loading.gif" lazyload alt="d">是表示向量的维度，<img src="https://math.jianshu.com/math?formula=k" srcset="/blog/img/loading.gif" lazyload alt="k">是卷积层的核大小，<img src="https://math.jianshu.com/math?formula=r" srcset="/blog/img/loading.gif" lazyload alt="r">是受限自注意层中邻域的大小。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/26691889-fd01c1deb3ca08a5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/886" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>image-20210806134215646</p>
<p>如表1所示，自注意层通过一个常数大小的序列执行操作次数连接了所有的位置，而一个循环网络需要<img src="https://math.jianshu.com/math?formula=O(n)" srcset="/blog/img/loading.gif" lazyload alt="O(n)">的序列操作。就计算复杂度来说，当序列长度<img src="https://math.jianshu.com/math?formula=n" srcset="/blog/img/loading.gif" lazyload alt="n">小于表示维度<img src="https://math.jianshu.com/math?formula=d" srcset="/blog/img/loading.gif" lazyload alt="d">时，自注意层比循环层更快，这是机器翻译中最先进模型常用的句子表示方式，比如单词(word-piece)表示法和字节对(byte-pair)表示法。为了提升涉及很长序列的计算性能，自注意能被限制到只考在输入序列以各自的输出位置为中心邻域大小为<img src="https://math.jianshu.com/math?formula=r" srcset="/blog/img/loading.gif" lazyload alt="r">的范围。这能增加最大路径长度到<img src="https://math.jianshu.com/math?formula=O(n/r)" srcset="/blog/img/loading.gif" lazyload alt="O(n/r)">。我们计划未来研究这种方法。</p>
<p>一个核宽度<img src="https://math.jianshu.com/math?formula=k%20%3C%20n" srcset="/blog/img/loading.gif" lazyload alt="k &lt; n">的单卷积层没有连接到输入和输出对的所有位置，为了达到这种效果，在邻近核(contiguous kernel)的情况下需要堆叠<img src="https://math.jianshu.com/math?formula=O(n/k)" srcset="/blog/img/loading.gif" lazyload alt="O(n/k)">个卷积层，或在扩展卷积(dialated convolution)的情况下需要<img src="https://math.jianshu.com/math?formula=O(%5Clog_k(n))" srcset="/blog/img/loading.gif" lazyload alt="O(\log_k(n))">个。通常卷积层比循环层开销更大，与<img src="https://math.jianshu.com/math?formula=k" srcset="/blog/img/loading.gif" lazyload alt="k">大小有关。然而，可分卷积(separable convolution)大幅减少复杂度到<img src="https://math.jianshu.com/math?formula=O(k%5Ccdot%20n%20%5Ccdot%20d%20+%20n%20%5Ccdot%20d%5E2)" srcset="/blog/img/loading.gif" lazyload alt="O(k\cdot n \cdot d + n \cdot d^2)">。然而，即使<img src="https://math.jianshu.com/math?formula=k=n" srcset="/blog/img/loading.gif" lazyload alt="k=n">，可分卷积的复杂度等于组合一个自注意层和一个基于位置的前馈层，即我们模型中采用的方法。</p>
<p>间接的好处是自注意能产生可解释性更强的模型。我们研究了我们模型中注意力的分布，然后在附录中展示。独立的多头注意力可以清楚地学到执行不同的任务，许多似乎学到了句法结构以及句子的语义结构。</p>
<h4 id="5-训练"><a href="#5-训练" class="headerlink" title="5 训练"></a>5 训练</h4><p>本节描述了我们模型的训练方法。</p>
<h4 id="5-1-训练数据和批次"><a href="#5-1-训练数据和批次" class="headerlink" title="5.1 训练数据和批次"></a>5.1 训练数据和批次</h4><p>我们对标准的 WMT2014英-德数据集进行了训练，该数据集由大约450万个句子对组成。句子是使用字节对编码，这种编码具有约37000个标记的共享源-目标词汇表。对于英语-法语，我们使用了更大的 WMT 2014英-法数据集，包括3600万个句子和分隔成32000个word-piece词汇表。句长相近的语句对被归为一个批次。每个训练批次由一组语句对组成，其中包含约25000个源单词和25000个目标单词。</p>
<h4 id="5-2-硬件和安排"><a href="#5-2-硬件和安排" class="headerlink" title="5.2 硬件和安排"></a>5.2 硬件和安排</h4><p>我们在一台有8个 NVIDIA P100 gpu 的机器上训练我们的模型。对于我们的基本模型，使用本文中描述的超参数，每个训练步大约需要0.4秒。我们训练基础模型总共100,000步或12小时。对于我们的大型模型，训练步时间是1.0秒。这些大模型被训练了30万步(3.5天)。</p>
<h4 id="5-3-优化器"><a href="#5-3-优化器" class="headerlink" title="5.3 优化器"></a>5.3 优化器</h4><p>我们使用Adam优化器，<img src="https://math.jianshu.com/math?formula=%5Cbeta_1=0.9,%20%5Cbeta_2=0.98" srcset="/blog/img/loading.gif" lazyload alt="\beta_1=0.9, \beta_2=0.98">和<img src="https://math.jianshu.com/math?formula=%5Cepsilon=10%5E%7B-9%7D" srcset="/blog/img/loading.gif" lazyload alt="\epsilon=10^{-9}">。我们根据这个公式在训练过程中改变学习率：</p>
<p><img src="https://math.jianshu.com/math?formula=lrate%20=%20d_%7Bmodel%7D%5E%7B-0.5%7D%20%5Ccdot%20%5Cmin(%5Ctext%7Bstep_num%7D%5E%7B-0.5%7D,%20%5Ctext%7Bstep_num%7D%20%5Ccdot%20%5Ctext%7Bwarmup_steps%7D%5E%7B-1.5%7D)" srcset="/blog/img/loading.gif" lazyload alt="lrate = d_{model}^{-0.5} \cdot \min(\text{step_num}^{-0.5}, \text{step_num} \cdot \text{warmup_steps}^{-1.5})"></p>
<p>即在第一个<img src="https://math.jianshu.com/math?formula=%5Ctext%7Bwarmup_steps%7D" srcset="/blog/img/loading.gif" lazyload alt="\text{warmup_steps}">训练步骤中线性地增加学习率，然后按照步数的逆平方根的比例减小。我们使用的<img src="https://math.jianshu.com/math?formula=%5Ctext%7Bwarmup_steps%7D=4000" srcset="/blog/img/loading.gif" lazyload alt="\text{warmup_steps}=4000">。</p>
<h4 id="5-4-正则化"><a href="#5-4-正则化" class="headerlink" title="5.4 正则化"></a>5.4 正则化</h4><p>我们在训练期间应用了三种正则化方法：</p>
<p><strong>残差丢弃</strong> 我们应用dropout(丢弃)到每个子层的输出，在它被加到子层的输入(残差连接)和层归一化之前。此外，我们将dropout应用于编码器和解码器栈中的嵌入和位置编码的和。对于基本模型，我们使用dropout比率为<img src="https://math.jianshu.com/math?formula=P_%7Bdrop%7D=0.1" srcset="/blog/img/loading.gif" lazyload alt="P_{drop}=0.1">。</p>
<blockquote>
<p>上面说了两个方法：残差连接和Dropout</p>
</blockquote>
<h5 id="标签平滑"><a href="#标签平滑" class="headerlink" title="标签平滑"></a>标签平滑</h5><p>在训练期间，我们应用<img src="https://math.jianshu.com/math?formula=%5Cepsilon_%7Bls%7D=0.1" srcset="/blog/img/loading.gif" lazyload alt="\epsilon_{ls}=0.1">的<a target="_blank" rel="noopener" href="https://links.jianshu.com/go?to=https://arxiv.org/abs/1512.00567">标签平滑(label smoothing)</a>，这会影响困惑度(perplexity)，因为模型学到的东西更加不确定，但是提升了准确率和BLEU得分。</p>
<h4 id="6-结果"><a href="#6-结果" class="headerlink" title="6 结果"></a>6 结果</h4><blockquote>
<p>暂略</p>
</blockquote>
<h4 id="7-总结"><a href="#7-总结" class="headerlink" title="7 总结"></a>7 总结</h4><p>本文中，我们提出了Transformer，第一个完全基于注意力的序列转导模型，用多头注意力取代了编码器-解码器架构中最常用的循环层。</p>
<p>对于翻译任务，Transformer可以比基于循环或卷积层的体系结构训练更快。 在WMT 2014英-德和WMT 2014英-法翻译任务中，我们取得了最好的结果。 在前面的任务中，我们最好的模型甚至超过以前报道过的所有的集成模型。</p>
<p>我们对基于注意力的模型的未来感到兴奋。我们计划将Transformer扩展到除文本之外的涉及输入和输出模式的问题，并研究局部的、受限的注意力机制以有效处理大规模的输入和输出，如图像、音频和视频。 让生成具有更少的顺序性是我们的另一个研究目标。</p>
<p>我们用于训练和评估模型的代码可以在<a target="_blank" rel="noopener" href="https://links.jianshu.com/go?to=https://github.com/tensorflow/tensor2tensor">https://github.com/tensorflow/tensor2tensor</a>上找到。</p>
<h3 id="transfromer实践"><a href="#transfromer实践" class="headerlink" title="transfromer实践"></a>transfromer实践</h3><h4 id="1-add-amp-norm"><a href="#1-add-amp-norm" class="headerlink" title="1 add &amp; norm"></a>1 add &amp; norm</h4><ul>
<li>定义子层</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">SublayerConnection</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    子层的连接: layer_norm(x + sublayer(x))</span><br><span class="hljs-string">    上述可以理解为一个残差网络加上一个LayerNorm归一化</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, size, dropout=<span class="hljs-number">0.1</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        :param size: d_model</span><br><span class="hljs-string">        :param dropout: drop比率</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>(SublayerConnection, self).__init__()<br>        self.layer_norm = LayerNorm(size)<br>        <span class="hljs-comment"># TODO：在SublayerConnection中LayerNorm可以换成nn.BatchNorm2d</span><br>        <span class="hljs-comment"># self.layer_norm = nn.BatchNorm2d()</span><br>        self.dropout = nn.Dropout(p=dropout)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, sublayer</span>):<br>        <span class="hljs-keyword">return</span> self.dropout(self.layer_norm(x + sublayer(x)))<br><br></code></pre></td></tr></table></figure>

<ul>
<li>定义<code>LayerNorm</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">LayerNorm</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    构建一个LayerNorm Module</span><br><span class="hljs-string">    LayerNorm的作用：对x归一化，使x的均值为0，方差为1</span><br><span class="hljs-string">    LayerNorm计算公式：x-mean(x)/\sqrt&#123;var(x)+\epsilon&#125; = x-mean(x)/std(x)+\epsilon</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, x_size, eps=<span class="hljs-number">1e-6</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        :param x_size: 特征的维度</span><br><span class="hljs-string">        :param eps: eps是一个平滑的过程，取值通常在（10^-4~10^-8 之间）</span><br><span class="hljs-string">        其含义是，对于每个参数，随着其更新的总距离增多，其学习速率也随之变慢。</span><br><span class="hljs-string">        防止出现除以0的情况。</span><br><span class="hljs-string"></span><br><span class="hljs-string">        nn.Parameter将一个不可训练的类型Tensor转换成可以训练的类型parameter，</span><br><span class="hljs-string">        并将这个parameter绑定到这个module里面。</span><br><span class="hljs-string">        使用这个函数的目的也是想让某些变量在学习的过程中不断的修改其值以达到最优化。</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>(LayerNorm, self).__init__()<br>        self.ones_tensor = nn.Parameter(torch.ones(x_size))  <span class="hljs-comment"># 按照特征向量大小返回一个全1的张量，并且转换成可训练的parameter类型</span><br>        self.zeros_tensor = nn.Parameter(torch.zeros(x_size))<br>        self.eps = eps<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        mean = x.mean(-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>        std = x.std(-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)  <span class="hljs-comment"># 求标准差</span><br>    <br></code></pre></td></tr></table></figure>

<h4 id="2-多头注意力-Multi-Head-Attention"><a href="#2-多头注意力-Multi-Head-Attention" class="headerlink" title="2 多头注意力 Multi-Head Attention"></a>2 多头注意力 Multi-Head Attention</h4><ul>
<li>定义自注意力层</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">self_attention</span>(<span class="hljs-params">query, key, value, dropout=<span class="hljs-literal">None</span>, mask=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    自注意力计算</span><br><span class="hljs-string">    :param query: Q</span><br><span class="hljs-string">    :param key: K</span><br><span class="hljs-string">    :param value: V</span><br><span class="hljs-string">    :param dropout: drop比率</span><br><span class="hljs-string">    :param mask: 是否mask</span><br><span class="hljs-string">    :return: 经自注意力机制计算后的值</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    d_k = query.size(-<span class="hljs-number">1</span>)  <span class="hljs-comment"># 防止softmax未来求梯度消失时的d_k</span><br>    <span class="hljs-comment"># Q,K相似度计算公式：\frac&#123;Q^TK&#125;&#123;\sqrt&#123;d_k&#125;&#125;</span><br>    scores = torch.matmul(query, key.transpose(-<span class="hljs-number">2</span>, -<span class="hljs-number">1</span>)) / math.sqrt(d_k)  <span class="hljs-comment"># Q,K相似度计算</span><br>    <span class="hljs-comment"># 判断是否要mask，注：mask的操作在QK之后，softmax之前</span><br>    <span class="hljs-keyword">if</span> mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        scores.masked_fill默认是按照传入的mask中为1的元素所在的索引，</span><br><span class="hljs-string">        在scores中相同的的索引处替换为value，替换值为-1e9，即-(10^9)</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># mask.cuda()</span><br>        <span class="hljs-comment"># 进行mask操作，由于参数mask==0，因此替换上述mask中为0的元素所在的索引</span><br><br>    scores = scores.masked_fill(mask == <span class="hljs-number">0</span>, -<span class="hljs-number">1e9</span>)<br><br>    self_attn_softmax = F.softmax(scores, dim=-<span class="hljs-number">1</span>)  <span class="hljs-comment"># 进行softmax</span><br>    <span class="hljs-comment"># 判断是否要对相似概率分布进行dropout操作</span><br>    <span class="hljs-keyword">if</span> dropout <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        self_attn_softmax = dropout(self_attn_softmax)<br><br>    <span class="hljs-comment"># 注意：返回经自注意力计算后的值，以及进行softmax后的相似度（即相似概率分布）</span><br>    <span class="hljs-keyword">return</span> torch.matmul(self_attn_softmax, value), self_attn_softmax<br></code></pre></td></tr></table></figure>

<ul>
<li>定义多头自注意力层</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MultiHeadAttention</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    多头注意力计算</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, head, d_model, dropout=<span class="hljs-number">0.1</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        :param head: 头数</span><br><span class="hljs-string">        :param d_model: 词向量的维度，必须是head的整数倍</span><br><span class="hljs-string">        :param dropout: drop比率</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>(MultiHeadAttention, self).__init__()<br>        <span class="hljs-keyword">assert</span> (d_model % head == <span class="hljs-number">0</span>)  <span class="hljs-comment"># 确保词向量维度是头数的整数倍</span><br>        self.d_k = d_model // head  <span class="hljs-comment"># 被拆分为多头后的某一头词向量的维度</span><br>        self.head = head<br>        self.d_model = d_model<br><br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        由于多头注意力机制是针对多组Q、K、V，因此有了下面这四行代码，具体作用是，</span><br><span class="hljs-string">        针对未来每一次输入的Q、K、V，都给予参数进行构建</span><br><span class="hljs-string">        其中linear_out是针对多头汇总时给予的参数</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        self.linear_query = nn.Linear(d_model, d_model)  <span class="hljs-comment"># 进行一个普通的全连接层变化，但不修改维度</span><br>        self.linear_key = nn.Linear(d_model, d_model)<br>        self.linear_value = nn.Linear(d_model, d_model)<br>        self.linear_out = nn.Linear(d_model, d_model)<br><br>        self.dropout = nn.Dropout(p=dropout)<br>        self.attn_softmax = <span class="hljs-literal">None</span>  <span class="hljs-comment"># attn_softmax是能量分数, 即句子中某一个词与所有词的相关性分数， softmax(QK^T)</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, query, key, value, mask=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-keyword">if</span> mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">            多头注意力机制的线性变换层是4维，是把query[batch, frame_num, d_model]变成[batch, -1, head, d_k]</span><br><span class="hljs-string">            再1，2维交换变成[batch, head, -1, d_k], 所以mask要在第二维（head维）添加一维，与后面的self_attention计算维度一样</span><br><span class="hljs-string">            具体点将，就是：</span><br><span class="hljs-string">            因为mask的作用是未来传入self_attention这个函数的时候，作为masked_fill需要mask哪些信息的依据</span><br><span class="hljs-string">            针对多head的数据，Q、K、V的形状维度中，只有head是通过view计算出来的，是多余的，为了保证mask和</span><br><span class="hljs-string">            view变换之后的Q、K、V的形状一直，mask就得在head这个维度添加一个维度出来，进而做到对正确信息的mask</span><br><span class="hljs-string">            &quot;&quot;&quot;</span><br>            mask = mask.unsqueeze(<span class="hljs-number">1</span>)<br><br>        n_batch = query.size(<span class="hljs-number">0</span>)  <span class="hljs-comment"># batch_size大小，假设query的维度是：[10, 32, 512]，其中10是batch_size的大小</span><br><br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        下列三行代码都在做类似的事情，对Q、K、V三个矩阵做处理</span><br><span class="hljs-string">        其中view函数是对Linear层的输出做一个形状的重构，其中-1是自适应（自主计算）</span><br><span class="hljs-string">        从这种重构中，可以看出，虽然增加了头数，但是数据的总维度是没有变化的，也就是说多头是对数据内部进行了一次拆分</span><br><span class="hljs-string">        transopose(1,2)是对前形状的两个维度(索引从0开始)做一个交换，例如(2,3,4,5)会变成(2,4,3,5)</span><br><span class="hljs-string">        因此通过transpose可以让view的第二维度参数变成n_head</span><br><span class="hljs-string">        假设Linear成的输出维度是：[10, 32, 512]，其中10是batch_size的大小</span><br><span class="hljs-string">        注：这里解释了为什么d_model // head == d_k，如若不是，则view函数做形状重构的时候会出现异常</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        query = self.linear_query(query).view(n_batch, -<span class="hljs-number">1</span>, self.head, self.d_k).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)  <span class="hljs-comment"># [b, 8, 32, 64]，head=8</span><br>        key = self.linear_key(key).view(n_batch, -<span class="hljs-number">1</span>, self.head, self.d_k).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)  <span class="hljs-comment"># [b, 8, 28, 64]</span><br>        value = self.linear_value(value).view(n_batch, -<span class="hljs-number">1</span>, self.head, self.d_k).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)  <span class="hljs-comment"># [b, 8, 28, 64]</span><br><br>        <span class="hljs-comment"># x是通过自注意力机制计算出来的值， self.attn_softmax是相似概率分布</span><br>        x, self.attn_softmax = self_attention(query, key, value, dropout=self.dropout, mask=mask)<br><br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        下面的代码是汇总各个头的信息，拼接后形成一个新的x</span><br><span class="hljs-string">        其中self.head * self.d_k，可以看出x的形状是按照head数拼接成了一个大矩阵，然后输入到linear_out层添加参数</span><br><span class="hljs-string">        contiguous()是重新开辟一块内存后存储x，然后才可以使用.view方法，否则直接使用.view方法会报错</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        x = x.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>).contiguous().view(n_batch, -<span class="hljs-number">1</span>, self.head * self.d_k)<br>        <span class="hljs-keyword">return</span> self.linear_out(x)<br></code></pre></td></tr></table></figure>

<h4 id="3-位置编码"><a href="#3-位置编码" class="headerlink" title="3 位置编码"></a>3 位置编码</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">PositionalEncoding</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    正弦位置编码，即通过三角函数构建位置编码</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Implementation based on &quot;Attention Is All You Need&quot;</span><br><span class="hljs-string">    :cite:`DBLP:journals/corr/VaswaniSPUJGKP17`</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, dim: <span class="hljs-built_in">int</span>, dropout: <span class="hljs-built_in">float</span>, max_len=<span class="hljs-number">5000</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        :param dim: 位置向量的向量维度，一般与词向量维度相同，即d_model</span><br><span class="hljs-string">        :param dropout: Dropout层的比率</span><br><span class="hljs-string">        :param max_len: 句子的最大长度</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 判断能够构建位置向量</span><br>        <span class="hljs-keyword">if</span> dim % <span class="hljs-number">2</span> != <span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">f&quot;不能使用 sin/cos 位置编码，得到了奇数的维度<span class="hljs-subst">&#123;dim:d&#125;</span>，应该使用偶数维度&quot;</span>)<br><br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        构建位置编码pe</span><br><span class="hljs-string">        pe公式为：</span><br><span class="hljs-string">        PE(pos,2i/2i+1) = sin/cos(pos/10000^&#123;2i/d_&#123;model&#125;&#125;)</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        pe = torch.zeros(max_len, dim)  <span class="hljs-comment"># 初始化pe</span><br>        position = torch.arange(<span class="hljs-number">0</span>, max_len).unsqueeze(<span class="hljs-number">1</span>)  <span class="hljs-comment"># 构建pos，为句子的长度，相当于pos</span><br>        div_term = torch.exp((torch.arange(<span class="hljs-number">0</span>, dim, <span class="hljs-number">2</span>, dtype=torch.<span class="hljs-built_in">float</span>) * torch.tensor(<br>            -(math.log(<span class="hljs-number">10000.0</span>) / dim))))  <span class="hljs-comment"># 复现位置编码sin/cos中的公式</span><br>        pe[:, <span class="hljs-number">0</span>::<span class="hljs-number">2</span>] = torch.sin(position.<span class="hljs-built_in">float</span>() * div_term)  <span class="hljs-comment"># 偶数使用sin函数</span><br>        pe[:, <span class="hljs-number">1</span>::<span class="hljs-number">2</span>] = torch.cos(position.<span class="hljs-built_in">float</span>() * div_term)  <span class="hljs-comment"># 奇数使用cos函数</span><br>        pe = pe.unsqueeze(<span class="hljs-number">1</span>)  <span class="hljs-comment"># 扁平化成一维向量</span><br><br>        <span class="hljs-built_in">super</span>(PositionalEncoding, self).__init__()<br>        self.register_buffer(<span class="hljs-string">&#x27;pe&#x27;</span>, pe)  <span class="hljs-comment"># pe不是模型的一个参数，通过register_buffer把pe写入内存缓冲区，当做一个内存中的常量</span><br>        self.drop_out = nn.Dropout(p=dropout)<br>        self.dim = dim<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, emb, step=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        词向量和位置编码拼接并输出</span><br><span class="hljs-string">        :param emb: 词向量序列（FloatTensor），``(seq_len, batch_size, self.dim)``</span><br><span class="hljs-string">        :param step: 如果 stepwise(&quot;seq_len=1&quot;)，则用此位置的编码</span><br><span class="hljs-string">        :return: 词向量和位置编码的拼接</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        emb = emb * math.sqrt(self.dim)<br>        <span class="hljs-keyword">if</span> step <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            emb = emb + self.pe[:emb.size(<span class="hljs-number">0</span>)]  <span class="hljs-comment"># 拼接词向量和位置编码</span><br>        <span class="hljs-keyword">else</span>:<br>            emb = emb + self.pe[step]<br>        emb = self.drop_out(emb)<br>        <span class="hljs-keyword">return</span> emb<br></code></pre></td></tr></table></figure>

<h4 id="4-前馈神经网络层-FFN"><a href="#4-前馈神经网络层-FFN" class="headerlink" title="4 前馈神经网络层 FFN"></a>4 前馈神经网络层 FFN</h4><p>两次线性变换穿插一次非线性变换：$w2(relu(w1*x + b1)) + b2$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">FeedForward</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    两层具有残差网络的前馈神经网络，FNN网络</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model: <span class="hljs-built_in">int</span>, d_ff: <span class="hljs-built_in">int</span>, dropout=<span class="hljs-number">0.1</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        :param d_model: FFN第一层输入的维度</span><br><span class="hljs-string">        :param d_ff: FNN第二层隐藏层输入的维度</span><br><span class="hljs-string">        :param dropout: drop比率</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>(FeedForward, self).__init__()<br>        self.w_1 = nn.Linear(d_model, d_ff)<br>        self.w_2 = nn.Linear(d_ff, d_model)<br>        self.layer_norm = nn.LayerNorm(d_model, eps=<span class="hljs-number">1e-6</span>)<br>        self.dropout_1 = nn.Dropout(dropout)<br>        self.relu = nn.ReLU()<br>        self.dropout_2 = nn.Dropout(dropout)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        :param x: 输入数据，形状为(batch_size, input_len, model_dim)</span><br><span class="hljs-string">        :return: 输出数据（FloatTensor），形状为(batch_size, input_len, model_dim)</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        inter = self.dropout_1(self.relu(self.w_1(self.layer_norm(x))))<br>        output = self.dropout_2(self.w_2(inter))<br>        <span class="hljs-comment"># return output + x，即为残差网络</span><br>        <span class="hljs-keyword">return</span> output  <span class="hljs-comment"># + x</span><br><br></code></pre></td></tr></table></figure>

<h4 id="5-Linear-amp-Softmax"><a href="#5-Linear-amp-Softmax" class="headerlink" title="5 Linear &amp; Softmax"></a>5 Linear &amp; Softmax</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">WordProbGenerator</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    文本生成器，即把Decoder层的输出通过最后一层softmax层变化为词概率</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model, vocab_size</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        :param d_model: 词向量维度</span><br><span class="hljs-string">        :param vocab_size: 词典大小</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>(WordProbGenerator, self).__init__()<br>        <span class="hljs-comment"># 通过线性层的映射，映射成词典大小的维度</span><br>        self.linear = nn.Linear(d_model, vocab_size)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># 通过softmax函数对词概率做出估计</span><br>        <span class="hljs-keyword">return</span> F.log_softmax(self.linear(x), dim=-<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure>

<h4 id="6-掩码多头注意力"><a href="#6-掩码多头注意力" class="headerlink" title="6 掩码多头注意力"></a>6 掩码多头注意力</h4><p>只是对输入做掩码，对模型改变不大</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">src_trg_mask</span>(<span class="hljs-params">src, r2l_trg, trg, pad_idx</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    :param src: 编码器的输入</span><br><span class="hljs-string">    :param r2l_trg: r2l方向解码器的输入</span><br><span class="hljs-string">    :param trg: l2r方向解码器的输入</span><br><span class="hljs-string">    :param pad_idx: pad的索引</span><br><span class="hljs-string">    :return: trg为None，返回编码器输入的掩码；trg存在，返回编码器和解码器输入的掩码</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> enc_src_mask是元组，是否可以改成list，然后修改这种冗余代码</span><br>    <span class="hljs-comment"># 通过src的长短，即视频特征向量提取的模式，判断有多少种特征向量需要进行mask</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(src, <span class="hljs-built_in">tuple</span>) <span class="hljs-keyword">and</span> <span class="hljs-built_in">len</span>(src) == <span class="hljs-number">4</span>:<br>        <span class="hljs-comment"># 不同模式的视频特征向量的mask</span><br>        src_image_mask = (src[<span class="hljs-number">0</span>][:, :, <span class="hljs-number">0</span>] != pad_idx).unsqueeze(<span class="hljs-number">1</span>)  <span class="hljs-comment"># 二维特征向量</span><br>        src_motion_mask = (src[<span class="hljs-number">1</span>][:, :, <span class="hljs-number">0</span>] != pad_idx).unsqueeze(<span class="hljs-number">1</span>)  <span class="hljs-comment"># 三维特征向量</span><br>        src_object_mask = (src[<span class="hljs-number">2</span>][:, :, <span class="hljs-number">0</span>] != pad_idx).unsqueeze(<span class="hljs-number">1</span>)  <span class="hljs-comment"># 目标检测特征向量</span><br>        src_rel_mask = (src[<span class="hljs-number">3</span>][:, :, <span class="hljs-number">0</span>] != pad_idx).unsqueeze(<span class="hljs-number">1</span>)  <span class="hljs-comment"># 目标关系特征向量</span><br><br>        <span class="hljs-comment"># 视频所有特征向量mask的拼接</span><br>        enc_src_mask = (src_image_mask, src_motion_mask, src_object_mask, src_rel_mask)<br>        dec_src_mask = src_image_mask &amp; src_motion_mask  <span class="hljs-comment"># 视频二维和三维特征向量mask的拼接</span><br>        src_mask = (enc_src_mask, dec_src_mask)  <span class="hljs-comment"># 视频最终的mask</span><br>    <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(src, <span class="hljs-built_in">tuple</span>) <span class="hljs-keyword">and</span> <span class="hljs-built_in">len</span>(src) == <span class="hljs-number">3</span>:<br>        src_image_mask = (src[<span class="hljs-number">0</span>][:, :, <span class="hljs-number">0</span>] != pad_idx).unsqueeze(<span class="hljs-number">1</span>)<br>        src_motion_mask = (src[<span class="hljs-number">1</span>][:, :, <span class="hljs-number">0</span>] != pad_idx).unsqueeze(<span class="hljs-number">1</span>)<br>        src_object_mask = (src[<span class="hljs-number">2</span>][:, :, <span class="hljs-number">0</span>] != pad_idx).unsqueeze(<span class="hljs-number">1</span>)<br><br>        enc_src_mask = (src_image_mask, src_motion_mask, src_object_mask)<br>        dec_src_mask = src_image_mask &amp; src_motion_mask<br>        src_mask = (enc_src_mask, dec_src_mask)<br>    <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(src, <span class="hljs-built_in">tuple</span>) <span class="hljs-keyword">and</span> <span class="hljs-built_in">len</span>(src) == <span class="hljs-number">2</span>:<br>        src_image_mask = (src[<span class="hljs-number">0</span>][:, :, <span class="hljs-number">0</span>] != pad_idx).unsqueeze(<span class="hljs-number">1</span>)<br>        src_motion_mask = (src[<span class="hljs-number">1</span>][:, :, <span class="hljs-number">0</span>] != pad_idx).unsqueeze(<span class="hljs-number">1</span>)<br><br>        enc_src_mask = (src_image_mask, src_motion_mask)<br>        dec_src_mask = src_image_mask &amp; src_motion_mask<br>        src_mask = (enc_src_mask, dec_src_mask)<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-comment"># 即只有src_image_mask，即二维特征的mask</span><br>        src_mask = src_image_mask = (src[:, :, <span class="hljs-number">0</span>] != pad_idx).unsqueeze(<span class="hljs-number">1</span>)<br><br>    <span class="hljs-comment"># 判断是否需要对trg，也就是解码器的输入进行掩码</span><br>    <span class="hljs-keyword">if</span> trg <span class="hljs-keyword">and</span> r2l_trg:<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        trg_mask是填充掩码和序列掩码，&amp;前是填充掩码，&amp;后是通过subsequent_mask函数得到的序列掩码</span><br><span class="hljs-string">        其中type_as，是为了让序列掩码和填充掩码的维度一致</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        trg_mask = (trg != pad_idx).unsqueeze(<span class="hljs-number">1</span>) &amp; sequence_mask(trg.size(<span class="hljs-number">1</span>)).type_as(src_image_mask.data)<br>        <span class="hljs-comment"># r2l_trg的填充掩码</span><br>        r2l_pad_mask = (r2l_trg != pad_idx).unsqueeze(<span class="hljs-number">1</span>).type_as(src_image_mask.data)<br>        <span class="hljs-comment"># r2l_trg的填充掩码和序列掩码</span><br>        r2l_trg_mask = r2l_pad_mask &amp; sequence_mask(r2l_trg.size(<span class="hljs-number">1</span>)).type_as(src_image_mask.data)<br>        <span class="hljs-comment"># src_mask[batch, 1, lens]  trg_mask[batch, 1, lens]</span><br>        <span class="hljs-keyword">return</span> src_mask, r2l_pad_mask, r2l_trg_mask, trg_mask<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> src_mask<br><br></code></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">sequence_mask</span>(<span class="hljs-params">size</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    序列掩码，解码器输入数据时掩盖后续词的位置</span><br><span class="hljs-string">    :param size: 生成词个数</span><br><span class="hljs-string">    :return: 右上角为False，主对角线及左下角为True的bool矩阵</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    attn_shape = (<span class="hljs-number">1</span>, size, size)<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    np.triu：返回函数的上三角矩阵A，k=1得到主对角线向上平移一个距离的对角线，</span><br><span class="hljs-string">    即保留右上对角线及其以上的数据，其余置为0，即a_11=0</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    mask = np.triu(np.ones(attn_shape), k=<span class="hljs-number">1</span>).astype(<span class="hljs-string">&#x27;uint8&#x27;</span>)<br>    <span class="hljs-keyword">return</span> (torch.from_numpy(mask) == <span class="hljs-number">0</span>).cuda()  <span class="hljs-comment"># 通过==0返回的是bool矩阵，即矩阵元素为bool值</span><br><br></code></pre></td></tr></table></figure>

<h4 id="7-Encoder"><a href="#7-Encoder" class="headerlink" title="7 Encoder"></a>7 Encoder</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">EncoderLayer</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    一层编码Encoder层</span><br><span class="hljs-string">    MultiHeadAttention -&gt; Add &amp; Norm -&gt; Feed Forward -&gt; Add &amp; Norm</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, size, attn, feed_forward, dropout=<span class="hljs-number">0.1</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        :param size: d_model</span><br><span class="hljs-string">        :param attn: 已经初始化的Multi-Head Attention层</span><br><span class="hljs-string">        :param feed_forward: 已经初始化的Feed Forward层</span><br><span class="hljs-string">        :param dropout: drop比率</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>(EncoderLayer, self).__init__()<br>        self.attn = attn<br>        self.feed_forward = feed_forward<br><br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        下面一行的作用是因为一个Encoder层具有两个残差结构的网络</span><br><span class="hljs-string">        因此构建一个ModuleList存储两个SublayerConnection，以便未来对数据进行残差处理</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        self.sublayer_connection_list = clone_module_to_modulelist(SublayerConnection(size, dropout), <span class="hljs-number">2</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, mask</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        :param x: Encoder层的输入</span><br><span class="hljs-string">        :param mask: mask标志</span><br><span class="hljs-string">        :return: 经过一个Encoder层处理后的输出</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        编码层第一层子层</span><br><span class="hljs-string">        self.attn 应该是一个已经初始化的Multi-Head Attention层</span><br><span class="hljs-string">        把Encoder的输入数据x和经过一个Multi-Head Attention处理后的x_attn送入第一个残差网络进行处理得到first_x</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        first_x = self.sublayer_connection_list[<span class="hljs-number">0</span>](x, <span class="hljs-keyword">lambda</span> x_attn: self.attn(x, x, x, mask))<br><br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        编码层第二层子层</span><br><span class="hljs-string">        把经过第一层子层处理后的数据first_x与前馈神经网络送入第二个残差网络进行处理得到Encoder层的输出</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> self.sublayer_connection_list[<span class="hljs-number">1</span>](first_x, self.feed_forward)<br><br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Encoder</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    构建n层编码层</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, n, encoder_layer</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        :param n: Encoder层的层数</span><br><span class="hljs-string">        :param encoder_layer: 初始化的Encoder层</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>(Encoder, self).__init__()<br>        self.encoder_layer_list = clone_module_to_modulelist(encoder_layer, n)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, src_mask</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        :param x: 输入数据</span><br><span class="hljs-string">        :param src_mask: mask标志</span><br><span class="hljs-string">        :return: 经过n层Encoder处理后的数据</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">for</span> encoder_layer <span class="hljs-keyword">in</span> self.encoder_layer_list:<br>            x = encoder_layer(x, src_mask)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure>

<h4 id="8-Decoder"><a href="#8-Decoder" class="headerlink" title="8 Decoder"></a>8 Decoder</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">DecoderLayer</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    一层解码Decoder层</span><br><span class="hljs-string">    Mask MultiHeadAttention -&gt; Add &amp; Norm -&gt; Multi-Head Attention -&gt; Add &amp; Norm</span><br><span class="hljs-string">    -&gt; Feed Forward -&gt; Add &amp; Norm</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model, attn, feed_forward, sublayer_num, dropout=<span class="hljs-number">0.1</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        :param d_model: d_model</span><br><span class="hljs-string">        :param attn: 已经初始化的Multi-Head Attention层</span><br><span class="hljs-string">        :param feed_forward: 已经初始化的Feed Forward层</span><br><span class="hljs-string">        :param sublayer_num: 解码器内部子层数，如果未来r2l_memory传入有值，则为4层，否则为普通的3层</span><br><span class="hljs-string">        :param dropout: drop比率</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>(DecoderLayer, self).__init__()<br>        self.attn = attn<br>        self.feed_forward = feed_forward<br>        self.sublayer_connection_list = clone_module_to_modulelist(SublayerConnection(d_model, dropout), sublayer_num)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, l2r_memory, src_mask, trg_mask, r2l_memory=<span class="hljs-literal">None</span>, r2l_trg_mask=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        :param x: Decoder的输入(captioning)</span><br><span class="hljs-string">        :param l2r_memory: Encoder的输出，作为Multi-Head Attention的K，V值，为从左到右的Encoder的输出</span><br><span class="hljs-string">        :param src_mask: 编码器输入的填充掩码</span><br><span class="hljs-string">        :param trg_mask: 解码器输入的填充掩码和序列掩码，即对后面单词的掩码</span><br><span class="hljs-string">        :param r2l_memory: 从右到左解码器的输出</span><br><span class="hljs-string">        :param r2l_trg_mask: 从右到左解码器的输出的填充掩码和序列掩码</span><br><span class="hljs-string">        :return: Encoder的输出</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        解码器第一层子层</span><br><span class="hljs-string">        把Decoder的输入数据x和经过一个Masked Multi-Head Attention处理后的first_x_attn送入第一个残差网络进行处理得到first_x</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        first_x = self.sublayer_connection_list[<span class="hljs-number">0</span>](x, <span class="hljs-keyword">lambda</span> first_x_attn: self.attn(x, x, x, trg_mask))<br><br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        解码器第二层子层</span><br><span class="hljs-string">        把第一层子层得到的first_x和</span><br><span class="hljs-string">        经过一个Multi-Head Attention处理后的second_x_attn（由first_x和Encoder的输出进行自注意力计算）</span><br><span class="hljs-string">        送入第二个残差网络进行处理</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        second_x = self.sublayer_connection_list[<span class="hljs-number">1</span>](first_x,<br>                                                    <span class="hljs-keyword">lambda</span> second_x_attn: self.attn(first_x, l2r_memory, l2r_memory,<br>                                                                                    src_mask))<br><br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        解码器第三层子层</span><br><span class="hljs-string">        把经过第二层子层处理后的数据second_x与前馈神经网络送入第三个残差网络进行处理得到Decoder层的输出</span><br><span class="hljs-string">        </span><br><span class="hljs-string">        如果有r2l_memory数据，则还需要经过一层多头注意力计算，也就是说会有四个残差网络</span><br><span class="hljs-string">        r2l_memory是让Decoder层多了一层双向编码中从右到左的编码层</span><br><span class="hljs-string">        而只要三个残差网络的Decoder层只有从左到右的编码</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> r2l_memory:<br>            <span class="hljs-comment"># 进行从右到左的编码，增加语义信息</span><br>            third_x = self.sublayer_connection_list[-<span class="hljs-number">2</span>](second_x,<br>                                                        <span class="hljs-keyword">lambda</span> third_x_attn: self.attn(second_x, r2l_memory, r2l_memory,<br>                                                                                       r2l_trg_mask))<br>            <span class="hljs-keyword">return</span> self.sublayer_connection_list[-<span class="hljs-number">1</span>](third_x, self.feed_forward)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span> self.sublayer_connection_list[-<span class="hljs-number">1</span>](second_x, self.feed_forward)<br><br><br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Encoder</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    构建n层编码层</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, n, encoder_layer</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        :param n: Encoder层的层数</span><br><span class="hljs-string">        :param encoder_layer: 初始化的Encoder层</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>(Encoder, self).__init__()<br>        self.encoder_layer_list = clone_module_to_modulelist(encoder_layer, n)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, src_mask</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        :param x: 输入数据</span><br><span class="hljs-string">        :param src_mask: mask标志</span><br><span class="hljs-string">        :return: 经过n层Encoder处理后的数据</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">for</span> encoder_layer <span class="hljs-keyword">in</span> self.encoder_layer_list:<br>            x = encoder_layer(x, src_mask)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">R2LDecoder</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    n个含有R2L自注意计算的解码层，该解码层只有3个残差网络</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, n_layers, decoder_layer</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        :param n_layers: Decoder层的层数</span><br><span class="hljs-string">        :param decoder_layer: 初始化的Decoder层</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>(R2LDecoder, self).__init__()<br>        self.decoder_layer_list = clone_module_to_modulelist(decoder_layer, n_layers)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, memory, src_mask, trg_mask</span>):<br>        <span class="hljs-keyword">for</span> decoder_layer <span class="hljs-keyword">in</span> self.decoder_layer_list:<br>            <span class="hljs-comment"># 没有传入r2l_memory和r2l_trg_mask，默认值为None，即该Decoder只有3个残差网络</span><br>            x = decoder_layer(x, memory, src_mask, trg_mask)<br>        <span class="hljs-keyword">return</span> x<br><br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">L2RDecoder</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    n个含有L2R自注意计算的解码层，该解码层有4个残差网络</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, n_layers, decoder_layer</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        :param n_layers: Decoder层的层数</span><br><span class="hljs-string">        :param decoder_layer: 初始化的Decoder层</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>(L2RDecoder, self).__init__()<br>        self.decoder_layer_list = clone_module_to_modulelist(decoder_layer, n_layers)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, memory, src_mask, trg_mask, r2l_memory, r2l_trg_mask</span>):<br>        <span class="hljs-keyword">for</span> decoder_layer <span class="hljs-keyword">in</span> self.decoder_layer_list:<br>            <span class="hljs-comment"># 传入r2l_memory和r2l_trg_mask，即修改默认值，Decoder将具有4个残差网络</span><br>            x = decoder_layer(x, memory, src_mask, trg_mask, r2l_memory, r2l_trg_mask)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure>

<h4 id="9-Transformer汇总"><a href="#9-Transformer汇总" class="headerlink" title="9 Transformer汇总"></a>9 Transformer汇总</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ABDTransformer</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    拼凑出Transformer</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab, d_feat, d_model, d_ff, n_heads, n_layers, dropout, feature_mode, device=<span class="hljs-string">&#x27;cuda&#x27;</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        :param vocab: 字典长度</span><br><span class="hljs-string">        :param d_feat: per frame dimension（每帧的维度）</span><br><span class="hljs-string">        :param d_model: 词向量的长度</span><br><span class="hljs-string">        :param d_ff: FNN（FeedForward）第二层隐藏层输入的维度</span><br><span class="hljs-string">        :param n_heads: 多头注意力时的头数</span><br><span class="hljs-string">        :param n_layers: 编码器和解码器的层数</span><br><span class="hljs-string">        :param dropout: drop的比率</span><br><span class="hljs-string">        :param feature_mode: 提取视频特征的模式</span><br><span class="hljs-string">        :param device: 是否使用gpu</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>(ABDTransformer, self).__init__()<br>        self.vocab = vocab<br>        self.device = device<br>        self.feature_mode = feature_mode<br>        attn = MultiHeadAttention(n_heads, d_model, dropout)  <span class="hljs-comment"># 多头注意力计算</span><br>        feed_forward = FeedForward(d_model, d_ff)  <span class="hljs-comment"># 前馈神经网络</span><br><br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        提取视频特征向量</span><br><span class="hljs-string">        通过feature_mode判断d_feat提取出的维度，也就是提取了多少种信息</span><br><span class="hljs-string">        共有四种特征向量信息，四种特征向量依次为：</span><br><span class="hljs-string">        image_mask：二维特征向量</span><br><span class="hljs-string">        motion_mask：三维特征向量</span><br><span class="hljs-string">        object_mask：目标检测，分为两部分，第一部分是目标检测框的坐标，第二部分是被检测目标的特征向量</span><br><span class="hljs-string">        rel_mask：是目标之间的关系特征向量</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> feature_mode == <span class="hljs-string">&#x27;one&#x27;</span>:<br>            <span class="hljs-comment"># 使用unknown_src_embed命名的目的：提取视频一个特征向量的时候，不一定会提取什么种类的特征向量</span><br>            self.unknown_src_embed = FeatEmbedding(d_feat, d_model, dropout)<br>        <span class="hljs-keyword">elif</span> feature_mode == <span class="hljs-string">&#x27;two&#x27;</span>:<br>            self.image_src_embed = FeatEmbedding(d_feat[<span class="hljs-number">0</span>], d_model, dropout)<br>            self.motion_src_embed = FeatEmbedding(d_feat[<span class="hljs-number">1</span>], d_model, dropout)<br>        <span class="hljs-keyword">elif</span> feature_mode == <span class="hljs-string">&#x27;three&#x27;</span>:<br>            self.image_src_embed = FeatEmbedding(d_feat[<span class="hljs-number">0</span>], d_model, dropout)<br>            self.motion_src_embed = FeatEmbedding(d_feat[<span class="hljs-number">1</span>], d_model, dropout)<br>            self.object_src_embed = FeatEmbedding(d_feat[<span class="hljs-number">2</span>], d_model, dropout)<br>        <span class="hljs-keyword">elif</span> feature_mode == <span class="hljs-string">&#x27;four&#x27;</span>:<br>            self.image_src_embed = FeatEmbedding(d_feat[<span class="hljs-number">0</span>], d_model, dropout)<br>            self.motion_src_embed = FeatEmbedding(d_feat[<span class="hljs-number">1</span>], d_model, dropout)<br>            self.object_src_embed = FeatEmbedding(d_feat[<span class="hljs-number">2</span>], d_model, dropout)<br>            self.rel_src_embed = FeatEmbedding(d_feat[<span class="hljs-number">3</span>], d_model, dropout)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">raise</span> <span class="hljs-string">&quot;feature_mode没有该模式，只有[&#x27;one&#x27;,&#x27;two&#x27;,&#x27;three&#x27;,&#x27;four&#x27;]四种模式&quot;</span><br><br>        <span class="hljs-comment"># 把特征向量提取成d_model维度的词向量</span><br>        self.trg_embed = WordEmbedding(vocab.n_vocabs, d_model)<br>        <span class="hljs-comment"># 提取位置向量</span><br>        self.pos_embed = PositionalEncoding(d_model, dropout)<br>        <span class="hljs-comment"># 编码层</span><br>        self.encoder = Encoder(n_layers, EncoderLayer(d_model, deepcopy(attn), deepcopy(feed_forward), dropout))<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        单向解码层</span><br><span class="hljs-string">        使用deepcopy的原因：因为每个层的参数是不同的，因此通过deepcopy拷贝一份到新的内存里，避免共享参数</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        self.r2l_decoder = R2LDecoder(n_layers, DecoderLayer(d_model, deepcopy(attn), deepcopy(feed_forward),<br>                                                             sublayer_num=<span class="hljs-number">3</span>, dropout=dropout))<br>        <span class="hljs-comment"># 双向解码层</span><br>        self.l2r_decoder = L2RDecoder(n_layers, DecoderLayer(d_model, deepcopy(attn), deepcopy(feed_forward),<br>                                                             sublayer_num=<span class="hljs-number">4</span>, dropout=dropout))<br>        <span class="hljs-comment"># 生成单词概率分布</span><br>        self.word_prob_generator = WordProbGenerator(d_model, vocab.n_vocabs)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_encoder_feature_concat</span>(<span class="hljs-params">self, src, feature_type, src_mask</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        为接下来的encoder函数做准备，主要目的是对视频的特征向量做处理</span><br><span class="hljs-string">        :param src: 特征向量</span><br><span class="hljs-string">        :param feature_type: 根据视频的类别不同，使用不同的特征向量生成函数， [&#x27;image&#x27;, &#x27;motion&#x27;, &#x27;object&#x27;, &#x27;rel&#x27;]</span><br><span class="hljs-string">        :param src_mask: 特征向量掩码的标志</span><br><span class="hljs-string">        :return: 经过处理后的视频特征向量</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> feature_type == <span class="hljs-string">&#x27;rel&#x27;</span>:<br>            <span class="hljs-comment"># 视频的关系特征向量不需要进行位置向量</span><br>            x = self.rel_src_embed(src)  <span class="hljs-comment"># 提取目标关系特征向量</span><br>            <span class="hljs-keyword">return</span> self.encoder(x, src_mask)  <span class="hljs-comment"># 送入编码器进行编码</span><br><br>        <span class="hljs-comment"># 例：对于&#x27;image&#x27;，下面的调用为 self.image_src_embed(src)</span><br>        x = self.__getattribute__(<span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;feature_type&#125;</span>_src_embed&#x27;</span>)(src)<br>        x = self.pos_embed(x)  <span class="hljs-comment"># 提取视频位置特征向量</span><br>        <span class="hljs-keyword">return</span> self.encoder(x, src_mask)  <span class="hljs-comment"># 送入编码器进行编码</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">self, src, src_mask</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        对数据进行编码，此处主要目的是对不同类型的视频特征向量进行编码</span><br><span class="hljs-string">        :param src: 视频的特征向量</span><br><span class="hljs-string">        :param src_mask: 视频特征向量的掩码标志</span><br><span class="hljs-string">        :return: 成功被编码器编码的视频特征向量</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        x_list = []  <span class="hljs-comment"># 存储不同类型的视频特征向量被编码后的向量</span><br>        feature_type_list = [<span class="hljs-string">&#x27;image&#x27;</span>, <span class="hljs-string">&#x27;motion&#x27;</span>, <span class="hljs-string">&#x27;object&#x27;</span>, <span class="hljs-string">&#x27;rel&#x27;</span>]  <span class="hljs-comment"># 视频特征向量的类型</span><br>        feature_mode_dict = &#123;<span class="hljs-string">&#x27;two&#x27;</span>: <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;three&#x27;</span>: <span class="hljs-number">3</span>, <span class="hljs-string">&#x27;four&#x27;</span>: <span class="hljs-number">4</span>&#125;  <span class="hljs-comment"># 输入视频特征向量的种类</span><br><br>        <span class="hljs-keyword">if</span> self.feature_mode == <span class="hljs-string">&#x27;one&#x27;</span>:<br>            <span class="hljs-keyword">return</span> self._encoder_feature_concat[src, <span class="hljs-string">&#x27;unknown&#x27;</span>, src_mask]<br><br>        <span class="hljs-keyword">for</span> i, feature_type <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(feature_type_list):<br>            <span class="hljs-comment"># 对于不同的feature_mode，拥有的encode的种类也不同</span><br>            <span class="hljs-keyword">if</span> i == feature_mode_dict[self.feature_mode]:<br>                <span class="hljs-keyword">break</span><br>            x_list.append(self._encoder_feature_concat[src[i], feature_type, src_mask[i]])<br><br>        <span class="hljs-comment"># TODO（灵感）：这里是否能添加一个线性变化，找出对于视频词向量更为有作用的模式和权重，这样也具有一定的解释性</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">sum</span>(x_list)  <span class="hljs-comment"># 对于不同feature_type提取的向量进行叠加</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">r2l_decode</span>(<span class="hljs-params">self, trg, memory, src_mask, trg_mask</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        对于单向编码，把视频向量转为文本向量，并且添加位置向量</span><br><span class="hljs-string">        :param trg: 解码器的输入</span><br><span class="hljs-string">        :param memory: 编码器的输出，也就是传给解码器的K、V</span><br><span class="hljs-string">        :param src_mask: 编码器输出的掩码标志</span><br><span class="hljs-string">        :param trg_mask: 解码器的掩码和单词掩码序列（看不见后面的词）</span><br><span class="hljs-string">        :return:</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        x = self.trg_embed(trg)  <span class="hljs-comment"># 把视频向量转为单向编码的词向量</span><br>        x = self.pos_embed(x)<br>        <span class="hljs-keyword">return</span> self.r2l_decoder(x, memory, src_mask, trg_mask)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">l2r_decode</span>(<span class="hljs-params">self, trg, memory, src_mask, trg_mask, r2l_memory, r2l_trg_mask</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        对于双向编码，把视频向量转为文本向量，并且添加位置向量</span><br><span class="hljs-string">        :param trg: 解码器的输入</span><br><span class="hljs-string">        :param memory: 编码器的输出，也就是传给解码器的K、V</span><br><span class="hljs-string">        :param src_mask: 编码器输出的掩码标志</span><br><span class="hljs-string">        :param trg_mask: 解码器的掩码和单词掩码序列（看不见后面的词）</span><br><span class="hljs-string">        :param r2l_memory: 从右到左解码器的输出</span><br><span class="hljs-string">        :param r2l_trg_mask: 从右到左解码器的输出的填充掩码和序列掩码</span><br><span class="hljs-string">        :return:</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        x = self.trg_embed(trg)  <span class="hljs-comment"># 把视频向量转为双向编码的词向量</span><br>        x = self.pos_embed(x)<br>        <span class="hljs-keyword">return</span> self.l2r_decoder(x, memory, src_mask, trg_mask, r2l_memory, r2l_trg_mask)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, src, r2l_trg, trg, mask</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        :param src: 编码器的输入</span><br><span class="hljs-string">        :param r2l_trg: 从右到左解码器的输入</span><br><span class="hljs-string">        :param trg: 从左到右解码器的输入</span><br><span class="hljs-string">        :param mask: mask标志</span><br><span class="hljs-string">        :return: 从右到左解码器和从左到右解码器的输出词概率分布</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># mask应该是个元组，其中src_mask是包括了不同特征模式的mask的元组</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(mask) == <span class="hljs-number">4</span>:<br>            src_mask, r2l_pad_mask, r2l_trg_mask, trg_mask = mask<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">raise</span> <span class="hljs-string">&quot;mask返回的是不带有解码器输入掩码的掩码元组，确认src_trg_mask()函数的参数&quot;</span><br><br>        <span class="hljs-keyword">if</span> self.feature_mode == <span class="hljs-string">&#x27;one&#x27;</span>:<br>            <span class="hljs-comment"># 得到视频encode后的输出</span><br>            encoding_output = self.encode(src, src_mask)<br>            <span class="hljs-comment"># 视频特征单向编码后送入三层残差网络的解码器后得到的输出</span><br>            r2l_output = self.r2l_decode(r2l_trg, encoding_output, src_mask, r2l_trg_mask)<br>            <span class="hljs-comment"># 视频特征双向编码后送入四层残差网络的解码器后得到的输出</span><br>            l2r_output = self.l2r_decode(trg, encoding_output, src_mask, trg_mask, r2l_output, r2l_pad_mask)<br>        <span class="hljs-keyword">elif</span> self.feature_mode == <span class="hljs-string">&#x27;two&#x27;</span> <span class="hljs-keyword">or</span> <span class="hljs-string">&#x27;three&#x27;</span> <span class="hljs-keyword">or</span> <span class="hljs-string">&#x27;four&#x27;</span>:<br>            <span class="hljs-comment"># enc_src_mask是视频所有类型的特征掩码；dec_src_mask是二维和三维类型的特征的掩码</span><br>            enc_src_mask, dec_src_mask = src_mask<br>            <span class="hljs-comment"># 视频特征向量模式的不同，对应不同的掩码方式</span><br>            encoding_output = self.encode(src, enc_src_mask)<br>            r2l_output = self.r2l_decode(r2l_trg, encoding_output, dec_src_mask, r2l_trg_mask)<br>            l2r_output = self.l2r_decode(trg, encoding_output, dec_src_mask, trg_mask, r2l_output, r2l_pad_mask)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">raise</span> <span class="hljs-string">&quot;没有这种feature_mode，只有[&#x27;one&#x27;,&#x27;two&#x27;,&#x27;three&#x27;,&#x27;four&#x27;]&quot;</span><br><br>        <span class="hljs-comment"># 预测解码词概率分布</span><br>        r2l_pred = self.word_prob_generator(r2l_output)<br>        l2r_pred = self.word_prob_generator(l2r_output)<br><br>        <span class="hljs-keyword">return</span> r2l_pred, l2r_pred<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">greedy_decode</span>(<span class="hljs-params">self, batch_size, src_mask, memory, max_len</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        针对r2l的解码单词生成，贪婪解码，每次按照最大概率的词作为候选词</span><br><span class="hljs-string">        :param batch_size: 每次送入的数据的数量</span><br><span class="hljs-string">        :param src_mask: 编码器输入数据的掩码标志</span><br><span class="hljs-string">        :param memory: 编码器的输出</span><br><span class="hljs-string">        :param max_len: 最大的迭代次数，即生成单词数</span><br><span class="hljs-string">        :return: 返回的r2l_hidden，即未来送入l2r中的r2l_memory；output是r2l层的预测输出</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        eos_idx = self.vocab.word2idx[<span class="hljs-string">&#x27;&lt;S&gt;&#x27;</span>]  <span class="hljs-comment"># &lt;S&gt;符号，表示结束输出的标志</span><br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            <span class="hljs-comment"># 构建一个batch_size大小的向量存储eos标志，作为初始化的output</span><br>            output = torch.ones(batch_size, <span class="hljs-number">1</span>).fill_(eos_idx).long().cuda()<br>            <span class="hljs-comment"># 迭代生成最终输出</span><br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_len + <span class="hljs-number">2</span> - <span class="hljs-number">1</span>):<br>                <span class="hljs-comment"># 构建解码器输入的序列掩码，掩盖后续的词</span><br>                trg_mask = sequence_mask(output.size(<span class="hljs-number">1</span>))<br>                <span class="hljs-comment"># 把初始化的输出和编码器的输出进行解码输出</span><br>                dec_out = self.r2l_decode(output, memory, src_mask, trg_mask)  <span class="hljs-comment"># batch, len, d_model</span><br>                r2l_hidden = dec_out<br>                <span class="hljs-comment"># 按照最大概率的词作为候选词</span><br>                pred = self.word_prob_generator(dec_out)  <span class="hljs-comment"># batch, len, n_vocabs</span><br>                next_word = pred[:, -<span class="hljs-number">1</span>].<span class="hljs-built_in">max</span>(dim=-<span class="hljs-number">1</span>)[<span class="hljs-number">1</span>].unsqueeze(<span class="hljs-number">1</span>)  <span class="hljs-comment"># pred[:, -1]([batch, n_vocabs])</span><br>                output = torch.cat([output, next_word], dim=-<span class="hljs-number">1</span>)  <span class="hljs-comment"># 拼接预测单词送入解码器解码</span><br><br>        <span class="hljs-comment"># 返回的r2l_hidden，即未来送入l2r中的r2l_memory；output是r2l层的预测输出</span><br>        <span class="hljs-keyword">return</span> r2l_hidden, output<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">r2l_beam_search_decode</span>(<span class="hljs-params">self, batch_size, src, src_mask, model_encodings, beam_size, max_len</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Beam Search算法可以参考：https://www.cnblogs.com/nickchen121/p/15499576.html</span><br><span class="hljs-string">        在每生成一个单词的时间步上，不是只保留当前分数最高的1个输出，而是保留num_beams个。</span><br><span class="hljs-string">        当num_beams=1时集束搜索就退化成了贪心搜索，也就是上述的greedy_decode。</span><br><span class="hljs-string">        :param batch_size: 一次送入数据的大小</span><br><span class="hljs-string">        :param src: 编码器的输入</span><br><span class="hljs-string">        :param src_mask: 编码器输入的掩码</span><br><span class="hljs-string">        :param model_encodings:</span><br><span class="hljs-string">        :param beam_size:</span><br><span class="hljs-string">        :param max_len: 最大迭代数，即生成单词数</span><br><span class="hljs-string">        :return:</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># batch_size = src.shape[0]</span><br>        end_symbol = self.vocab.word2idx[<span class="hljs-string">&#x27;&lt;S&gt;&#x27;</span>]  <span class="hljs-comment"># 结束符号</span><br>        start_symbol = self.vocab.word2idx[<span class="hljs-string">&#x27;&lt;S&gt;&#x27;</span>]  <span class="hljs-comment"># 开始符号</span><br>        r2l_output = <span class="hljs-literal">None</span>  <span class="hljs-comment"># r2l解码器的输出</span><br><br>        r2l_outputs = <span class="hljs-literal">None</span><br><br>        <span class="hljs-comment"># 1.1 Setup Src</span><br>        <span class="hljs-comment"># src has shape (batch_size, sent_len)</span><br>        <span class="hljs-comment"># src_mask has shape (batch_size, 1, sent_len)</span><br>        <span class="hljs-comment"># src_mask = (src[:, :, 0] != self.vocab.word2idx[&#x27;&lt;PAD&gt;&#x27;]).unsqueeze(-2)  # TODO Untested</span><br>        <span class="hljs-comment"># model_encodings has shape (batch_size, sentence_len, d_model)</span><br>        <span class="hljs-comment"># model_encodings = self.encode(src, src_mask)</span><br><br>        <span class="hljs-comment"># 1.2 Setup Tgt Hypothesis Tracking</span><br>        <span class="hljs-comment"># hypothesis is List(4 bt)[(cur beam_sz, dec_sent_len)], init: List(4 bt)[(1 init_beam_sz, dec_sent_len)]</span><br>        <span class="hljs-comment"># hypotheses[i] is shape (cur beam_sz, dec_sent_len)</span><br>        hypotheses = [copy.deepcopy(torch.full((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), start_symbol, dtype=torch.long,<br>                                               device=self.device)) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(batch_size)]<br>        <span class="hljs-comment"># List after init: List 4 bt of List of len max_len_completed, init: List of len 4 bt of []</span><br>        completed_hypotheses = [copy.deepcopy([]) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(batch_size)]<br>        <span class="hljs-comment"># List len batch_sz of shape (cur beam_sz), init: List(4 bt)[(1 init_beam_sz)]</span><br>        <span class="hljs-comment"># hyp_scores[i] is shape (cur beam_sz)</span><br>        hyp_scores = [copy.deepcopy(torch.full((<span class="hljs-number">1</span>,), <span class="hljs-number">0</span>, dtype=torch.<span class="hljs-built_in">float</span>, device=self.device))<br>                      <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(batch_size)]  <span class="hljs-comment"># probs are log_probs must be init at 0.</span><br><br>        <span class="hljs-comment"># 2. Iterate: Generate one char at a time until maxlen</span><br>        <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_len + <span class="hljs-number">1</span>):<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">all</span>([<span class="hljs-built_in">len</span>(completed_hypotheses[i]) == beam_size <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(batch_size)]):<br>                <span class="hljs-keyword">break</span><br><br>            <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">            2.1 Setup the batch. Since we use beam search, each batch has a variable number (called cur_beam_size)</span><br><span class="hljs-string">            between 0 and beam_size of hypotheses live at any moment. We decode all hypotheses for all batches at</span><br><span class="hljs-string">            the same time, so we must copy the src_encodings, src_mask, etc the appropriate number fo times for</span><br><span class="hljs-string">            the number of hypotheses for each example. We keep track of the number of live hypotheses for each example.</span><br><span class="hljs-string">            We run all hypotheses for all examples together through the decoder and log-softmax,</span><br><span class="hljs-string">            and then use `torch.split` to get the appropriate number of hypotheses for each example in the end.</span><br><span class="hljs-string">            &quot;&quot;&quot;</span><br>            cur_beam_sizes, last_tokens, model_encodings_l, src_mask_l = [], [], [], []<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(batch_size):<br>                <span class="hljs-keyword">if</span> hypotheses[i] <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                    cur_beam_sizes += [<span class="hljs-number">0</span>]<br>                    <span class="hljs-keyword">continue</span><br>                cur_beam_size, decoded_len = hypotheses[i].shape<br>                cur_beam_sizes += [cur_beam_size]<br>                last_tokens += [hypotheses[i]]<br>                model_encodings_l += [model_encodings[i:i + <span class="hljs-number">1</span>]] * cur_beam_size<br>                src_mask_l += [src_mask[i:i + <span class="hljs-number">1</span>]] * cur_beam_size<br>            <span class="hljs-comment"># shape (sum(4 bt * cur_beam_sz_i), 1 dec_sent_len, 128 d_model)</span><br>            model_encodings_cur = torch.cat(model_encodings_l, dim=<span class="hljs-number">0</span>)<br>            src_mask_cur = torch.cat(src_mask_l, dim=<span class="hljs-number">0</span>)<br>            y_tm1 = torch.cat(last_tokens, dim=<span class="hljs-number">0</span>)<br>            <span class="hljs-comment"># shape (sum(4 bt * cur_beam_sz_i), 1 dec_sent_len, 128 d_model)</span><br>            <span class="hljs-keyword">if</span> self.feature_mode == <span class="hljs-string">&#x27;one&#x27;</span>:<br>                out = self.r2l_decode(Variable(y_tm1).to(self.device), model_encodings_cur, src_mask_cur,<br>                                      Variable(sequence_mask(y_tm1.size(-<span class="hljs-number">1</span>)).type_as(src.data)).to(self.device))<br>            <span class="hljs-keyword">elif</span> self.feature_mode == <span class="hljs-string">&#x27;two&#x27;</span> <span class="hljs-keyword">or</span> <span class="hljs-string">&#x27;three&#x27;</span> <span class="hljs-keyword">or</span> <span class="hljs-string">&#x27;four&#x27;</span>:<br>                out = self.r2l_decode(Variable(y_tm1).to(self.device), model_encodings_cur, src_mask_cur,<br>                                      Variable(sequence_mask(y_tm1.size(-<span class="hljs-number">1</span>)).type_as(src[<span class="hljs-number">0</span>].data)).to(self.device))<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-keyword">raise</span> <span class="hljs-string">&quot;out为None&quot;</span><br><br>            r2l_output = out<br><br>            <span class="hljs-comment"># shape (sum(4 bt * cur_beam_sz_i), 1 dec_sent_len, 50002 vocab_sz)</span><br>            log_prob = self.word_prob_generator(out[:, -<span class="hljs-number">1</span>, :]).unsqueeze(<span class="hljs-number">1</span>)<br>            <span class="hljs-comment"># shape (sum(4 bt * cur_beam_sz_i), 1 dec_sent_len, 50002 vocab_sz)</span><br>            _, decoded_len, vocab_sz = log_prob.shape<br>            <span class="hljs-comment"># log_prob = log_prob.reshape(batch_size, cur_beam_size, decoded_len, vocab_sz)</span><br>            <span class="hljs-comment"># shape List(4 bt)[(cur_beam_sz_i, dec_sent_len, 50002 vocab_sz)]</span><br>            <span class="hljs-comment"># log_prob[i] is (cur_beam_sz_i, dec_sent_len, 50002 vocab_sz)</span><br>            log_prob = torch.split(log_prob, cur_beam_sizes, dim=<span class="hljs-number">0</span>)<br><br>            <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">            2.2 Now we process each example in the batch. </span><br><span class="hljs-string">            Note that the example may have already finished processing before</span><br><span class="hljs-string">            other examples (no more hypotheses to try), in which case we continue</span><br><span class="hljs-string">            &quot;&quot;&quot;</span><br>            new_hypotheses, new_hyp_scores = [], []<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(batch_size):<br>                <span class="hljs-keyword">if</span> hypotheses[i] <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">or</span> <span class="hljs-built_in">len</span>(completed_hypotheses[i]) &gt;= beam_size:<br>                    new_hypotheses += [<span class="hljs-literal">None</span>]<br>                    new_hyp_scores += [<span class="hljs-literal">None</span>]<br>                    <span class="hljs-keyword">continue</span><br><br>                <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">                2.2.1 We compute the cumulative scores for each live hypotheses for the example</span><br><span class="hljs-string">                hyp_scores is the old scores for the previous stage, and `log_prob` are the new probs for</span><br><span class="hljs-string">                this stage. Since they are log probs, we sum them instead of multiplying them.</span><br><span class="hljs-string">                The .view(-1) forces all the hypotheses into one dimension. The shape of this dimension is</span><br><span class="hljs-string">                cur_beam_sz * vocab_sz (ex: 5 * 50002).</span><br><span class="hljs-string">                So after getting the topk from it, </span><br><span class="hljs-string">                we can recover the generating sentence and the next word using: ix // vocab_sz, ix % vocab_sz.</span><br><span class="hljs-string">                &quot;&quot;&quot;</span><br>                cur_beam_sz_i, dec_sent_len, vocab_sz = log_prob[i].shape<br>                <span class="hljs-comment"># shape (vocab_sz,)</span><br>                cumulative_hyp_scores_i = (hyp_scores[i].unsqueeze(-<span class="hljs-number">1</span>).unsqueeze(-<span class="hljs-number">1</span>)<br>                                           .expand((cur_beam_sz_i, <span class="hljs-number">1</span>, vocab_sz)) + log_prob[i]).view(-<span class="hljs-number">1</span>)<br><br>                <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">                2.2.2 We get the topk values in cumulative_hyp_scores_i and compute the current (generating) sentence</span><br><span class="hljs-string">                and the next word using: ix // vocab_sz, ix % vocab_sz.</span><br><span class="hljs-string">                &quot;&quot;&quot;</span><br><br>                <span class="hljs-comment"># shape (cur_beam_sz,)</span><br>                live_hyp_num_i = beam_size - <span class="hljs-built_in">len</span>(completed_hypotheses[i])<br>                <span class="hljs-comment"># shape (cur_beam_sz,). Vals are between 0 and 50002 vocab_sz</span><br>                top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(cumulative_hyp_scores_i, k=live_hyp_num_i)<br>                <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">                shape (cur_beam_sz,). prev_hyp_ids vals are 0 &lt;= val &lt; cur_beam_sz. </span><br><span class="hljs-string">                hyp_word_ids vals are 0 &lt;= val &lt; vocab_len</span><br><span class="hljs-string">                &quot;&quot;&quot;</span><br>                prev_hyp_ids = top_cand_hyp_pos // self.vocab.n_vocabs<br>                hyp_word_ids = top_cand_hyp_pos % self.vocab.n_vocabs<br><br>                <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">                2.2.3 For each of the topk words, we append the new word to the current (generating) sentence</span><br><span class="hljs-string">                We add this to new_hypotheses_i and add its corresponding total score to new_hyp_scores_i</span><br><span class="hljs-string">                &quot;&quot;&quot;</span><br><br>                <span class="hljs-comment"># Removed live_hyp_ids_i, which is used in the LSTM decoder to track live hypothesis ids</span><br>                new_hypotheses_i, new_hyp_scores_i = [], []<br>                <span class="hljs-keyword">for</span> prev_hyp_id, hyp_word_id, cand_new_hyp_score <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(prev_hyp_ids, hyp_word_ids,<br>                                                                        top_cand_hyp_scores):<br>                    prev_hyp_id, hyp_word_id, cand_new_hyp_score = \<br>                        prev_hyp_id.item(), hyp_word_id.item(), cand_new_hyp_score.item()<br><br>                    new_hyp_sent = torch.cat(<br>                        (hypotheses[i][prev_hyp_id], torch.tensor([hyp_word_id], device=self.device)))<br>                    <span class="hljs-keyword">if</span> hyp_word_id == end_symbol:<br>                        completed_hypotheses[i].append(Hypothesis(<br>                            value=[self.vocab.idx2word[a.item()] <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> new_hyp_sent[<span class="hljs-number">1</span>:-<span class="hljs-number">1</span>]],<br>                            score=cand_new_hyp_score))<br>                    <span class="hljs-keyword">else</span>:<br>                        new_hypotheses_i.append(new_hyp_sent.unsqueeze(-<span class="hljs-number">1</span>))<br>                        new_hyp_scores_i.append(cand_new_hyp_score)<br><br>                <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">                2.2.4 We may find that the hypotheses_i for some example in the batch</span><br><span class="hljs-string">                is empty - we have fully processed that example. We use None as a sentinel in this case.</span><br><span class="hljs-string">                Above, the loops gracefully handle None examples.</span><br><span class="hljs-string">                &quot;&quot;&quot;</span><br>                <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(new_hypotheses_i) &gt; <span class="hljs-number">0</span>:<br>                    hypotheses_i = torch.cat(new_hypotheses_i, dim=-<span class="hljs-number">1</span>).transpose(<span class="hljs-number">0</span>, -<span class="hljs-number">1</span>).to(self.device)<br>                    hyp_scores_i = torch.tensor(new_hyp_scores_i, dtype=torch.<span class="hljs-built_in">float</span>, device=self.device)<br>                <span class="hljs-keyword">else</span>:<br>                    hypotheses_i, hyp_scores_i = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br>                new_hypotheses += [hypotheses_i]<br>                new_hyp_scores += [hyp_scores_i]<br>            <span class="hljs-comment"># print(new_hypotheses, new_hyp_scores)</span><br>            hypotheses, hyp_scores = new_hypotheses, new_hyp_scores<br><br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        2.3 Finally, we do some postprocessing to get our final generated candidate sentences.</span><br><span class="hljs-string">        Sometimes, we may get to max_len of a sentence and still not generate the &lt;/s&gt; end token.</span><br><span class="hljs-string">        In this case, the partial sentence we have generated will not be added to the completed_hypotheses</span><br><span class="hljs-string">        automatically, and we have to manually add it in. We add in as many as necessary so that there are</span><br><span class="hljs-string">        `beam_size` completed hypotheses for each example.</span><br><span class="hljs-string">        Finally, we sort each completed hypothesis by score.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(batch_size):<br>            hyps_to_add = beam_size - <span class="hljs-built_in">len</span>(completed_hypotheses[i])<br>            <span class="hljs-keyword">if</span> hyps_to_add &gt; <span class="hljs-number">0</span>:<br>                scores, ix = torch.topk(hyp_scores[i], k=hyps_to_add)<br>                <span class="hljs-keyword">for</span> score, id_ <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(scores, ix):<br>                    completed_hypotheses[i].append(Hypothesis(<br>                        value=[self.vocab.idx2word[a.item()] <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> hypotheses[i][id_][<span class="hljs-number">1</span>:]],<br>                        score=score))<br>            completed_hypotheses[i].sort(key=<span class="hljs-keyword">lambda</span> hyp: hyp.score, reverse=<span class="hljs-literal">True</span>)<br>        <span class="hljs-keyword">return</span> r2l_output, completed_hypotheses<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">beam_search_decode</span>(<span class="hljs-params">self, src, beam_size, max_len</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        An Implementation of Beam Search for the Transformer Model.</span><br><span class="hljs-string">        Beam search is performed in a batched manner. Each example in a batch generates `beam_size` hypotheses.</span><br><span class="hljs-string">        We return a list (len: batch_size) of list (len: beam_size) of Hypothesis,</span><br><span class="hljs-string">        which contain our output decoded sentence sand their scores.</span><br><span class="hljs-string">        :param src: shape (sent_len, batch_size). Each val is 0 &lt; val &lt; len(vocab_dec). The input tokens to the decoder.</span><br><span class="hljs-string">        :param max_len: the maximum length to decode</span><br><span class="hljs-string">        :param beam_size: the beam size to use</span><br><span class="hljs-string">        :return completed_hypotheses: A List of length batch_size,</span><br><span class="hljs-string">        each containing a List of beam_size Hypothesis objects.Hypothesis is a named Tuple,</span><br><span class="hljs-string">        its first entry is &quot;value&quot; and is a List of strings which contains the translated word</span><br><span class="hljs-string">        (one string is one word token).</span><br><span class="hljs-string">        The second entry is &quot;score&quot; and it is the log-prob score for this translated sentence.</span><br><span class="hljs-string">        Note: Below I note &quot;4 bt&quot;, &quot;5 beam_size&quot; as the shapes of objects. 4, 5 are default values.</span><br><span class="hljs-string">         Actual values may differ.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br><br>        <span class="hljs-comment"># 1. Setup</span><br>        start_symbol = self.vocab.word2idx[<span class="hljs-string">&#x27;&lt;S&gt;&#x27;</span>]<br>        end_symbol = self.vocab.word2idx[<span class="hljs-string">&#x27;&lt;S&gt;&#x27;</span>]<br><br>        <span class="hljs-comment"># 1.1 Setup Src</span><br>        <span class="hljs-comment"># src has shape (batch_size, sent_len)</span><br>        <span class="hljs-comment"># src_mask has shape (batch_size, 1, sent_len)</span><br>        <span class="hljs-comment"># src_mask = (src[:, :, 0] != self.vocab.word2idx[&#x27;&lt;PAD&gt;&#x27;]).unsqueeze(-2)  # TODO Untested</span><br>        src_mask = src_trg_mask(src, r2l_trg=<span class="hljs-literal">None</span>, trg=<span class="hljs-literal">None</span>, pad_idx=self.vocab.word2idx[<span class="hljs-string">&#x27;&lt;PAD&gt;&#x27;</span>])<br>        <span class="hljs-comment"># model_encodings has shape (batch_size, sentence_len, d_model)</span><br>        <span class="hljs-keyword">if</span> self.feature_mode == <span class="hljs-string">&#x27;one&#x27;</span>:<br>            batch_size = src.shape[<span class="hljs-number">0</span>]<br>            dec_src_mask = <span class="hljs-literal">None</span><br>            model_encodings = self.encode(src, src_mask)<br>            r2l_memory, r2l_completed_hypotheses = self.r2l_beam_search_decode(batch_size, src, src_mask,<br>                                                                               model_encodings=model_encodings,<br>                                                                               beam_size=<span class="hljs-number">1</span>, max_len=max_len)<br>        <span class="hljs-keyword">elif</span> self.feature_mode == <span class="hljs-string">&#x27;two&#x27;</span> <span class="hljs-keyword">or</span> <span class="hljs-string">&#x27;three&#x27;</span> <span class="hljs-keyword">or</span> <span class="hljs-string">&#x27;four&#x27;</span>:<br>            batch_size = src[<span class="hljs-number">0</span>].shape[<span class="hljs-number">0</span>]<br>            enc_src_mask = src_mask[<span class="hljs-number">0</span>]<br>            dec_src_mask = src_mask[<span class="hljs-number">1</span>]<br>            model_encodings = self.encode(src, enc_src_mask)<br>            r2l_memory, r2l_completed_hypotheses = self.r2l_beam_search_decode(batch_size, src, dec_src_mask,<br>                                                                               model_encodings=model_encodings,<br>                                                                               beam_size=<span class="hljs-number">1</span>, max_len=max_len)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">raise</span> <span class="hljs-string">&quot;batch_size为None&quot;</span><br><br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        1.2 Setup r2l target output</span><br><span class="hljs-string">        r2l_memory, r2l_completed_hypotheses = self.r2l_beam_search_decode(batch_size, src, src_mask,</span><br><span class="hljs-string">                                                                           model_encodings=model_encodings,</span><br><span class="hljs-string">                                                                           beam_size=1, max_len=max_len)</span><br><span class="hljs-string">        r2l_memory, r2l_completed_hypotheses = self.greedy_decode(batch_size, src_mask, model_encodings, max_len)</span><br><span class="hljs-string">        beam_r2l_memory = [copy.deepcopy(r2l_memory) for _ in range(beam_size)]</span><br><span class="hljs-string">        </span><br><span class="hljs-string">        1.3 Setup Tgt Hypothesis Tracking</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br><br>        <span class="hljs-comment"># hypothesis is List(4 bt)[(cur beam_sz, dec_sent_len)], init: List(4 bt)[(1 init_beam_sz, dec_sent_len)]</span><br>        <span class="hljs-comment"># hypotheses[i] is shape (cur beam_sz, dec_sent_len)</span><br>        hypotheses = [copy.deepcopy(torch.full((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), start_symbol, dtype=torch.long,<br>                                               device=self.device)) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(batch_size)]<br>        <span class="hljs-comment"># List after init: List 4 bt of List of len max_len_completed, init: List of len 4 bt of []</span><br>        completed_hypotheses = [copy.deepcopy([]) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(batch_size)]<br>        <span class="hljs-comment"># List len batch_sz of shape (cur beam_sz), init: List(4 bt)[(1 init_beam_sz)]</span><br>        <span class="hljs-comment"># hyp_scores[i] is shape (cur beam_sz)</span><br>        hyp_scores = [copy.deepcopy(torch.full((<span class="hljs-number">1</span>,), <span class="hljs-number">0</span>, dtype=torch.<span class="hljs-built_in">float</span>, device=self.device))<br>                      <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(batch_size)]  <span class="hljs-comment"># probs are log_probs must be init at 0.</span><br><br>        <span class="hljs-comment"># 2. Iterate: Generate one char at a time until maxlen</span><br>        <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_len + <span class="hljs-number">1</span>):<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">all</span>([<span class="hljs-built_in">len</span>(completed_hypotheses[i]) == beam_size <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(batch_size)]):<br>                <span class="hljs-keyword">break</span><br><br>            <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">            2.1 Setup the batch. Since we use beam search, each batch has a variable number (called cur_beam_size)</span><br><span class="hljs-string">            between 0 and beam_size of hypotheses live at any moment. We decode all hypotheses for all batches at</span><br><span class="hljs-string">            the same time, so we must copy the src_encodings, src_mask, etc the appropriate number fo times for</span><br><span class="hljs-string">            the number of hypotheses for each example. We keep track of the number of live hypotheses for each example.</span><br><span class="hljs-string">            We run all hypotheses for all examples together through the decoder and log-softmax,</span><br><span class="hljs-string">            and then use `torch.split` to get the appropriate number of hypotheses for each example in the end.</span><br><span class="hljs-string">            &quot;&quot;&quot;</span><br><br>            cur_beam_sizes, last_tokens, model_encodings_l, src_mask_l, r2l_memory_l = [], [], [], [], []<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(batch_size):<br>                <span class="hljs-keyword">if</span> hypotheses[i] <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                    cur_beam_sizes += [<span class="hljs-number">0</span>]<br>                    <span class="hljs-keyword">continue</span><br>                cur_beam_size, decoded_len = hypotheses[i].shape<br>                cur_beam_sizes += [cur_beam_size]<br>                last_tokens += [hypotheses[i]]<br>                model_encodings_l += [model_encodings[i:i + <span class="hljs-number">1</span>]] * cur_beam_size<br>                <span class="hljs-keyword">if</span> self.feature_mode == <span class="hljs-string">&#x27;one&#x27;</span>:<br>                    src_mask_l += [src_mask[i:i + <span class="hljs-number">1</span>]] * cur_beam_size<br>                <span class="hljs-keyword">elif</span> dec_src_mask <span class="hljs-keyword">and</span> (self.feature_mode == <span class="hljs-string">&#x27;two&#x27;</span> <span class="hljs-keyword">or</span> <span class="hljs-string">&#x27;three&#x27;</span> <span class="hljs-keyword">or</span> <span class="hljs-string">&#x27;four&#x27;</span>):<br>                    src_mask_l += [dec_src_mask[i:i + <span class="hljs-number">1</span>]] * cur_beam_size<br>                r2l_memory_l += [r2l_memory[i: i + <span class="hljs-number">1</span>]] * cur_beam_size<br>            <span class="hljs-comment"># shape (sum(4 bt * cur_beam_sz_i), 1 dec_sent_len, 128 d_model)</span><br>            model_encodings_cur = torch.cat(model_encodings_l, dim=<span class="hljs-number">0</span>)<br>            src_mask_cur = torch.cat(src_mask_l, dim=<span class="hljs-number">0</span>)<br>            y_tm1 = torch.cat(last_tokens, dim=<span class="hljs-number">0</span>)<br>            r2l_memory_cur = torch.cat(r2l_memory_l, dim=<span class="hljs-number">0</span>)<br>            <span class="hljs-comment"># shape (sum(4 bt * cur_beam_sz_i), 1 dec_sent_len, 128 d_model)</span><br>            <span class="hljs-keyword">if</span> self.feature_mode == <span class="hljs-string">&#x27;one&#x27;</span>:<br>                out = self.l2r_decode(Variable(y_tm1).to(self.device), model_encodings_cur, src_mask_cur,<br>                                      Variable(sequence_mask(y_tm1.size(-<span class="hljs-number">1</span>)).type_as(src.data)).to(self.device),<br>                                      r2l_memory_cur, r2l_trg_mask=<span class="hljs-literal">None</span>)<br>            <span class="hljs-keyword">elif</span> self.feature_mode == <span class="hljs-string">&#x27;two&#x27;</span> <span class="hljs-keyword">or</span> <span class="hljs-string">&#x27;three&#x27;</span> <span class="hljs-keyword">or</span> <span class="hljs-string">&#x27;four&#x27;</span>:<br>                out = self.l2r_decode(Variable(y_tm1).to(self.device), model_encodings_cur, src_mask_cur,<br>                                      Variable(sequence_mask(y_tm1.size(-<span class="hljs-number">1</span>)).type_as(src[<span class="hljs-number">0</span>].data)).to(self.device),<br>                                      r2l_memory_cur, r2l_trg_mask=<span class="hljs-literal">None</span>)<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-keyword">raise</span> <span class="hljs-string">&quot;out为None&quot;</span><br><br>            <span class="hljs-comment"># shape (sum(4 bt * cur_beam_sz_i), 1 dec_sent_len, 50002 vocab_sz)</span><br>            log_prob = self.word_prob_generator(out[:, -<span class="hljs-number">1</span>, :]).unsqueeze(<span class="hljs-number">1</span>)<br>            <span class="hljs-comment"># shape (sum(4 bt * cur_beam_sz_i), 1 dec_sent_len, 50002 vocab_sz)</span><br>            _, decoded_len, vocab_sz = log_prob.shape<br>            <span class="hljs-comment"># log_prob = log_prob.reshape(batch_size, cur_beam_size, decoded_len, vocab_sz)</span><br>            <span class="hljs-comment"># shape List(4 bt)[(cur_beam_sz_i, dec_sent_len, 50002 vocab_sz)]</span><br>            <span class="hljs-comment"># log_prob[i] is (cur_beam_sz_i, dec_sent_len, 50002 vocab_sz)</span><br>            log_prob = torch.split(log_prob, cur_beam_sizes, dim=<span class="hljs-number">0</span>)<br><br>            <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">            2.2 Now we process each example in the batch.</span><br><span class="hljs-string">            Note that the example may have already finished processing before.</span><br><span class="hljs-string">            other examples (no more hypotheses to try), in which case we continue</span><br><span class="hljs-string">            &quot;&quot;&quot;</span><br>            new_hypotheses, new_hyp_scores = [], []<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(batch_size):<br>                <span class="hljs-keyword">if</span> hypotheses[i] <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">or</span> <span class="hljs-built_in">len</span>(completed_hypotheses[i]) &gt;= beam_size:<br>                    new_hypotheses += [<span class="hljs-literal">None</span>]<br>                    new_hyp_scores += [<span class="hljs-literal">None</span>]<br>                    <span class="hljs-keyword">continue</span><br><br>                <span class="hljs-comment"># 2.2.1 We compute the cumulative scores for each live hypotheses for the example</span><br>                <span class="hljs-comment"># hyp_scores is the old scores for the previous stage, and `log_prob` are the new probs for</span><br>                <span class="hljs-comment"># this stage. Since they are log probs, we sum them instead of multiplying them.</span><br>                <span class="hljs-comment"># The .view(-1) forces all the hypotheses into one dimension. The shape of this dimension is</span><br>                <span class="hljs-comment"># cur_beam_sz * vocab_sz (ex: 5 * 50002). So after getting the topk from it, we can recover the</span><br>                <span class="hljs-comment"># generating sentence and the next word using: ix // vocab_sz, ix % vocab_sz.</span><br>                cur_beam_sz_i, dec_sent_len, vocab_sz = log_prob[i].shape<br>                <span class="hljs-string">&quot;shape (vocab_sz,)&quot;</span><br>                cumulative_hyp_scores_i = (hyp_scores[i].unsqueeze(-<span class="hljs-number">1</span>).unsqueeze(-<span class="hljs-number">1</span>)<br>                                           .expand((cur_beam_sz_i, <span class="hljs-number">1</span>, vocab_sz)) + log_prob[i]).view(-<span class="hljs-number">1</span>)<br><br>                <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">                2.2.2 We get the topk values in cumulative_hyp_scores_i and compute the current (generating) sentence</span><br><span class="hljs-string">                and the next word using: ix // vocab_sz, ix % vocab_sz.</span><br><span class="hljs-string">                &quot;&quot;&quot;</span><br>                <span class="hljs-comment"># shape (cur_beam_sz,)</span><br>                live_hyp_num_i = beam_size - <span class="hljs-built_in">len</span>(completed_hypotheses[i])<br>                <span class="hljs-comment"># shape (cur_beam_sz,). Vals are between 0 and 50002 vocab_sz</span><br>                top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(cumulative_hyp_scores_i, k=live_hyp_num_i)<br>                <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">                shape (cur_beam_sz,). prev_hyp_ids vals are 0 &lt;= val &lt; cur_beam_sz. </span><br><span class="hljs-string">                hyp_word_ids vals are 0 &lt;= val &lt; vocab_len</span><br><span class="hljs-string">                &quot;&quot;&quot;</span><br>                prev_hyp_ids = top_cand_hyp_pos // self.vocab.n_vocabs<br>                hyp_word_ids = top_cand_hyp_pos % self.vocab.n_vocabs<br><br>                <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">                2.2.3 For each of the topk words, we append the new word to the current (generating) sentence</span><br><span class="hljs-string">                We add this to new_hypotheses_i and add its corresponding total score to new_hyp_scores_i</span><br><span class="hljs-string">                &quot;&quot;&quot;</span><br>                <span class="hljs-comment"># Removed live_hyp_ids_i, which is used in the LSTM decoder to track live hypothesis ids</span><br>                new_hypotheses_i, new_hyp_scores_i = [], []<br>                <span class="hljs-keyword">for</span> prev_hyp_id, hyp_word_id, cand_new_hyp_score <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(prev_hyp_ids, hyp_word_ids,<br>                                                                        top_cand_hyp_scores):<br>                    prev_hyp_id, hyp_word_id, cand_new_hyp_score = \<br>                        prev_hyp_id.item(), hyp_word_id.item(), cand_new_hyp_score.item()<br><br>                    new_hyp_sent = torch.cat(<br>                        (hypotheses[i][prev_hyp_id], torch.tensor([hyp_word_id], device=self.device)))<br>                    <span class="hljs-keyword">if</span> hyp_word_id == end_symbol:<br>                        completed_hypotheses[i].append(Hypothesis(<br>                            value=[self.vocab.idx2word[a.item()] <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> new_hyp_sent[<span class="hljs-number">1</span>:-<span class="hljs-number">1</span>]],<br>                            score=cand_new_hyp_score))<br>                    <span class="hljs-keyword">else</span>:<br>                        new_hypotheses_i.append(new_hyp_sent.unsqueeze(-<span class="hljs-number">1</span>))<br>                        new_hyp_scores_i.append(cand_new_hyp_score)<br><br>                <span class="hljs-comment"># 2.2.4 We may find that the hypotheses_i for some example in the batch</span><br>                <span class="hljs-comment"># is empty - we have fully processed that example. We use None as a sentinel in this case.</span><br>                <span class="hljs-comment"># Above, the loops gracefully handle None examples.</span><br>                <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(new_hypotheses_i) &gt; <span class="hljs-number">0</span>:<br>                    hypotheses_i = torch.cat(new_hypotheses_i, dim=-<span class="hljs-number">1</span>).transpose(<span class="hljs-number">0</span>, -<span class="hljs-number">1</span>).to(self.device)<br>                    hyp_scores_i = torch.tensor(new_hyp_scores_i, dtype=torch.<span class="hljs-built_in">float</span>, device=self.device)<br>                <span class="hljs-keyword">else</span>:<br>                    hypotheses_i, hyp_scores_i = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br>                new_hypotheses += [hypotheses_i]<br>                new_hyp_scores += [hyp_scores_i]<br>            <span class="hljs-comment"># print(new_hypotheses, new_hyp_scores)</span><br>            hypotheses, hyp_scores = new_hypotheses, new_hyp_scores<br><br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        2.3 Finally, we do some postprocessing to get our final generated candidate sentences.</span><br><span class="hljs-string">        Sometimes, we may get to max_len of a sentence and still not generate the &lt;/s&gt; end token.</span><br><span class="hljs-string">        In this case, the partial sentence we have generated will not be added to the completed_hypotheses</span><br><span class="hljs-string">        automatically, and we have to manually add it in. We add in as many as necessary so that there are</span><br><span class="hljs-string">        `beam_size` completed hypotheses for each example.</span><br><span class="hljs-string">        Finally, we sort each completed hypothesis by score.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(batch_size):<br>            hyps_to_add = beam_size - <span class="hljs-built_in">len</span>(completed_hypotheses[i])<br>            <span class="hljs-keyword">if</span> hyps_to_add &gt; <span class="hljs-number">0</span>:<br>                scores, ix = torch.topk(hyp_scores[i], k=hyps_to_add)<br>                <span class="hljs-keyword">for</span> score, id_ <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(scores, ix):<br>                    completed_hypotheses[i].append(Hypothesis(<br>                        value=[self.vocab.idx2word[a.item()] <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> hypotheses[i][id_][<span class="hljs-number">1</span>:]],<br>                        score=score))<br>            completed_hypotheses[i].sort(key=<span class="hljs-keyword">lambda</span> hyp: hyp.score, reverse=<span class="hljs-literal">True</span>)<br>        <span class="hljs-comment"># print(&#x27;completed_hypotheses&#x27;, completed_hypotheses)</span><br>        <span class="hljs-keyword">return</span> r2l_completed_hypotheses, completed_hypotheses<br></code></pre></td></tr></table></figure>

<h4 id="汇总"><a href="#汇总" class="headerlink" title="汇总"></a>汇总</h4><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/nickchen121/p/16518613.html">https://www.cnblogs.com/nickchen121/p/16518613.html</a></p>
<h2 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h2><p>在CMU 博士后研究员刘鹏飞的<a href="https://link.zhihu.com/?target=https://arxiv.org/pdf/2107.13586.pdf">一篇PROMPT METHOD的综述里</a>，他介绍了自然语言学界经历过的四种任务处理范式。他认为古早时期的<strong>第一种范式</strong>便是语言学家需要手工设计一系列特征模板，来输入模型。模型对任务的处理结果高度依赖于特征模板的设计，间接地便高度依赖于领域专家的知识。举个例子，如果有学过自然语言处理的经典算法课的可能对条件随机场CRF模型不陌生。业界甚至有一个专门的库CRF++帮助你自动生成大量的随机模板输入模型进行训练从而避免对领域专家的需要。可是当<strong>第二范式</strong>神经网络学派开始流行以后，用预训练后的词嵌入表征加上模型架构的调整，便取得了相似甚至远超过第一范式的效果后，需要大量人工介入的第一范式便渐渐式微了。在这个时期我们可以看到大量的工作在词嵌入上，比如NNLM，CBOW,SKIP-GRAM,GLOVE,ELMO等。也可以看到大量的工作在模型架构上，比如BI-LSTM, SEQ2SEQ架构在神经机器翻译领域NMT的应用等。而真正开启<strong>第三范式</strong>，在超大的文本数据集上预训练一个通用的模型，接着再对下游的特定任务微调的PRETRAIN-FINETUNE的范式，则是我们今天本文的主角，GPT1模型。相比于第二范式而言，第三范式的优点在于更进一步减少了人工的参与。不再需要对于每个任务采取不同的模型架构，而是用一个取得了优异泛化能力的模型，去针对性地对下游任务进行微调。</p>
<p>如上所述，预训练的想法早在第二范式便已普及开来。但是不同的是词嵌入捕捉的更多是词义维度上的信息，如何捕捉更多的语言学信息是个大难点。难点如论文所述主要有两点：<strong>如何为所谓的可迁移的语言学能力或信息制定目标函数？</strong>即我们的优化目标是什么(如语言模型好坏，机器翻译结果或辩论的一致性)？<strong>其次学习到的能力将以何种形式迁移？</strong>是否需要如ELMO论文里展现的将学到的上下文词嵌入(contextual embeddings) 和输入拼接，并且定制下游模型的架构？或者又如UMLFit那样需要非常细致复杂的训练调整？这两个问题该论文都提出了自己的解答，并在未来不断地被其他第三范式的论文扩展深化。</p>
<h3 id="相关论文链接："><a href="#相关论文链接：" class="headerlink" title="相关论文链接："></a>相关论文链接：</h3><p>GPT <strong>Improving Language Understanding by Generative Pre-Training</strong>. 2018. <a href="https://link.zhihu.com/?target=https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">Paper</a></p>
<p>GPT-2 <strong>Language Models are Unsupervised Multitask Learners</strong>. 2018. <a href="https://link.zhihu.com/?target=https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Paper</a></p>
<p>GPT-3 <strong>“Language Models are Few-Shot Learners”</strong>. NeurIPS 2020. <a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2005.14165">Paper</a></p>
<p>InstructGPT: <strong>Training language models to follow instructions with human feedback</strong>, Arxiv 2022 <a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2203.02155">Paper</a></p>
<p>GPT-4 <strong>“GPT-4 Technical Report”</strong>. 2023. <a href="https://link.zhihu.com/?target=http://arxiv.org/abs/2303.08774v2">Paper</a></p>
<p>GPT影响 [<a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2303.10130">2303.10130] GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models (arxiv.org)</a> 2023. </p>
<h3 id="GPT-1（GPT就是Generative-Pre-Training）："><a href="#GPT-1（GPT就是Generative-Pre-Training）：" class="headerlink" title="GPT-1（GPT就是Generative Pre-Training）："></a>GPT-1（GPT就是Generative Pre-Training）：</h3><p>《Improving Language Understanding by Generative Pre-Training》是2018年由OpenAI的研究团队发布的一篇论文。它介绍了一种名为“生成式预训练”（Generative Pre-Training，简称GPT）的新型语言模型，该模型通过在大规模语料库上进行训练，能够学习自然语言的模式和规律，从而实现更好的语言理解。</p>
<p><img src="https://pic3.zhimg.com/80/v2-e8db5a0379bcc7c38fbff118b18edc5e_1440w.jpg" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>GPT模型是一种基于神经网络的自回归语言模型。该模型使用了一个称为“Transformer”的架构，这是一种新型的序列到序列模型，能够在处理长序列数据时避免传统的循环神经网络（Recurrent Neural Network，RNN）中存在的梯度消失问题。Transformer架构中的关键组件包括多头注意力机制和残差连接等。GPT使用了Transformer的解码器部分。为了预训练GPT模型，研究团队使用了两个大规模的语料库：BooksCorpus和英文维基百科。</p>
<p>以下是GPT1的主要技术特点：</p>
<p>基于Transformer架构：GPT1采用了Transformer架构，其中包括多头自注意力机制和前向神经网络。这使得GPT1可以在处理自然语言时捕捉长距离依赖性，并且具有高效的并行性。</p>
<p><img src="https://pic4.zhimg.com/80/v2-690e96ca8c81aba927c709f6ca45b19b_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>预训练技术：GPT-1使用了一种称为“生成式预训练”（Generative Pre-Training，GPT）的技术。预训练分为两个阶段：预训练和微调（fine-tuning）。在预训练阶段，GPT-1使用了大量的无标注文本数据集，例如维基百科和网页文本等。通过最大化预训练数据集上的log-likelihood来训练模型参数。在微调阶段，GPT-1将预训练模型的参数用于特定的自然语言处理任务，如文本分类和问答系统等。</p>
<p>多层模型：GPT-1模型由多个堆叠的Transformer编码器组成，每个编码器包含多个注意力头和前向神经网络。这使得模型可以从多个抽象层次对文本进行建模，从而更好地捕捉文本的语义信息。</p>
<p>通过使用上述预训练任务，研究团队成功地训练出了一个大规模的语言模型GPT。该模型在多项语言理解任务上取得了显著的成果，包括阅读理解、情感分类和自然语言推理等任务。</p>
<p><img src="https://pic3.zhimg.com/80/v2-34d7a88c31462839d3563c30a294e246_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>通过微调GPT模型，可以针对特定的任务进行优化，例如文本生成、机器翻译和对话系统等。</p>
<p>总之，GPT1是一种基于Transformer架构的预训练语言模型，具有多层模型、生成式预训练技术和独特的解码技术等特点。它为后续的自然语言处理技术提供了一个新的标准，并为人工智能技术的发展提供了新的思路。</p>
<h3 id="GPT-2（模型不需要人来指导，要的就是Unsupervise）："><a href="#GPT-2（模型不需要人来指导，要的就是Unsupervise）：" class="headerlink" title="GPT-2（模型不需要人来指导，要的就是Unsupervise）："></a>GPT-2（模型不需要人来指导，要的就是Unsupervise）：</h3><p>《Language Models are Unsupervised Multitask Learners》是一篇介绍GPT-2（Generative Pre-trained Transformer 2）模型的论文，它是2019年发表在OpenAI的博客上。</p>
<p><img src="https://pic1.zhimg.com/80/v2-e61bbc26b8b1fd35bb7da668152837ec_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>GPT-2主要解决的问题是如何利用大规模未标注的自然语言文本来预训练一个通用的语言模型，从而提高自然语言处理的能力。与GPT-1模型不同之处在于，GPT-2模型使用了更大的模型规模和更多的数据进行预训练，同时增加了许多新的预训练任务。</p>
<p><img src="https://pic3.zhimg.com/80/v2-69dadd64eabe1db6c4f0fcbe612b55fa_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>以下是GPT-2的主要技术特点(其实除了规模大一点，和GPT-1变化不大)：</p>
<ol>
<li>大规模预训练：GPT-2使用了一种无监督学习的方法，在大规模文本语料库上进行预训练。在这个阶段，模型从语料库中学习文本序列的统计规律和语义信息。</li>
<li>非监督多任务学习：GPT-2具有多任务学习的能力，通过训练模型来执行多个不同的自然语言处理任务，从而提高模型的鲁棒性和泛化能力。</li>
<li>Transformer架构：GPT-2使用Transformer架构作为模型的基础，使得模型可以自适应地处理长距离依赖关系，从而更好地理解文本的语义。</li>
<li>无需人工标注数据：GPT-2在训练过程中不需要人工标注数据，可以自动从大规模文本语料库中学习自然语言的规律。</li>
<li>零样本学习：GPT-2具有零样本学习的能力，能够在只看到少量样本的情况下学习和执行新任务。</li>
</ol>
<p><img src="https://pic2.zhimg.com/80/v2-da6f030c641ace274d9714e4f09b6089_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>在成果方面，GPT-2模型在许多自然语言处理任务上取得了显著的成果，如问答系统、文本分类、命名实体识别、语言推理等。此外，GPT-2模型还在生成文本方面表现出色，能够生成具有逼真度的连贯文本，并且可以根据用户提供的开头和主题生成长篇文章。GPT-2模型被广泛认为是目前最强大的自然语言处理模型之一。</p>
<p>总之，GPT-2是一种无监督学习的多任务语言模型，具有大规模预训练、Transformer架构、多层结构、无需人工标注数据和零样本学习等特点。它在自然语言处理任务中取得了显著的成果，是自然语言处理领域中的一项重要进展。</p>
<h3 id="GPT-3（模型变大了也变强了）："><a href="#GPT-3（模型变大了也变强了）：" class="headerlink" title="GPT-3（模型变大了也变强了）："></a>GPT-3（模型变大了也变强了）：</h3><p>《Language Models are Few-Shot Learners》是一篇介绍GPT-3（Generative Pre-trained Transformer 3）模型的论文，它是2020年发表在OpenAI的博客上。</p>
<p><img src="https://pic2.zhimg.com/80/v2-be6d8bc742b011507817dfdb3c2346dd_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>GPT-3主要解决的问题是如何使一个预训练的语言模型具有迁移学习的能力，即在只有少量标注数据的情况下，能够快速适应到新的任务中。</p>
<p><img src="https://pic2.zhimg.com/80/v2-73356a2ed4c85a1282e2bf130d826b51_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>GPT-3模型采用了基于Transformer的架构，与前一代GPT-2类似(原话是：We use the same model and architecture as GPT-2)，但是在模型规模、预训练数据量和使用的预训练任务上都有所增加。GPT-3的模型规模为1750亿个参数，是前一代GPT-2的100倍以上。</p>
<p><img src="https://pic1.zhimg.com/80/v2-c676e8faa9b0ffdf44ef316ff8356254_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>GPT它变大了，也变强了:</p>
<p><img src="https://pic4.zhimg.com/80/v2-2c43f30240d2196e72ba8f3f8bf26da7_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>GPT-3使用了多个来源的数据，包括互联网上的文本、书籍、新闻和Wikipedia等。这些数据经过清洗和处理后，用于预训练和微调。</p>
<p><img src="https://pic2.zhimg.com/80/v2-ae35263d14f5ce6419f77a873518caa5_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p><img src="https://pic4.zhimg.com/80/v2-a5ea3a1e2b423667ebb4217f3558697b_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>GPT-3在多个NLP任务上表现出了惊人的能力。在自然语言推理任务中，GPT-3模型的准确率达到了近80%，超过了当时最好的模型。在问答任务中，GPT-3模型只需要给出几个样例输入就能够完成对新问题的回答。</p>
<p><img src="https://pic1.zhimg.com/80/v2-ce1900a67dabf18e36a04af1fd78cccc_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p><img src="https://pic2.zhimg.com/80/v2-e26222a3f9287a5bbf037641959e6089_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>在生成文本任务中，GPT-3模型能够生成逼真、连贯、富有创造性的文本，甚至可以写出短故事、诗歌和新闻报道等。</p>
<p>此外，GPT-3还具有零样本学习的能力，即能够在没有任何样本数据的情况下进行学习和预测。例如，当给定一个新的任务和一些文字描述时，GPT-3能够基于文字描述自动推理出该任务的执行过程。</p>
<p>总之，GPT-3模型的能力已经超出了传统的自然语言处理模型，展示了无监督学习和迁移学习在自然语言处理领域的潜力和前景。</p>
<h3 id="InstructGPT（还是要指导指导-Instruct-模型啊，要不总出幺蛾子）"><a href="#InstructGPT（还是要指导指导-Instruct-模型啊，要不总出幺蛾子）" class="headerlink" title="InstructGPT（还是要指导指导(Instruct)模型啊，要不总出幺蛾子）:"></a>InstructGPT（还是要指导指导(Instruct)模型啊，要不总出幺蛾子）:</h3><p>《InstructGPT: Training language models to follow instructions with human feedback》是一篇由OpenAI团队发表的论文，于2021年在ICML上发布。</p>
<p><img src="https://pic3.zhimg.com/80/v2-fa468db934d9f4866be45c3192d3ea7a_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>InstructGPT提出的背景：使语言模型更大并不意味着它们能够更好地遵循用户的意图，例如大型语言模型可以生成不真实、有毒或对用户毫无帮助的输出，即这些模型与其用户不一致。InstructGPT主要解决的问题是如何让语言模型能够更好地遵循人类给出的指令，并在实践中实现它们。此类模型可以广泛应用于自然语言生成、对话系统和语言翻译等领域。</p>
<p>InstructGPT模型在GPT-3基础上进一步强化。InstructGPT使用来自人类反馈的强化学习方案RLHF(reinforcement learning from human feedback)，通过对大语言模型进行微调，从而能够在参数减少的情况下，实现优于GPT-3的功能。</p>
<p><img src="https://pic1.zhimg.com/80/v2-ab0e37125d45a057a97cd33d92385140_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>OpenAI在GPT-3基础上根据人类反馈的强化学习方案RHLF，训练出奖励模型(rewardmodel)去训练学习模型(即：用AI训练AI的思路)</p>
<p>具体来说，该方法包括以下步骤：</p>
<p>定义指令：首先，定义指令集合，即人类需要模型生成的语言指令。这些指令通常是任务相关的，例如完成一项任务或回答某个问题。</p>
<p>生成指令：通过 InstructGPT 生成一个或多个备选指令，每个指令都对应一个相应的生成概率。这些备选指令会显示在屏幕上供人类评估。</p>
<p>人类反馈：人类对生成的备选指令进行评估，并提供一个奖励信号，表示该指令与预期指令的匹配程度。奖励信号可以表示为基于 BLEU、ROUGE 等指标的分数。</p>
<p>强化学习训练：根据人类反馈，训练模型以优化生成指令的质量。具体来说，使用强化学习算法，将生成的指令和人类反馈作为训练数据，迭代训练模型，以最大化生成指令的奖励信号。</p>
<p>该方法的优点是可以让语言模型更加有针对性地生成文本，以适应特定任务或场景，并且可以根据人类反馈进行动态调整，提高生成文本的质量和多样性。</p>
<p>InstructGPT的结果表明，在接受足够反馈的情况下，该模型可以在大多数指令数据集上达到95%以上的准确率，超过了其他常用模型。此外，InstructGPT还展示了其在指令执行、对话系统和游戏中的应用能力。例如，它可以在指令行动游戏中成功地执行多个连续的指令，如“向右移动、跳跃、开门”等，还可以在对话系统中通过遵循用户的指令来进行对话。</p>
<p>总之，InstructGPT通过将人类反馈作为训练和微调的关键组成部分，开发出了一种新的指令遵循框架，该框架可以提高语言模型的实际应用能力。这项工作为训练语言模型以更好地遵循指令提供了一个新的范例，未来可以在更多领域进行应用。</p>
<h3 id="ChatGPT（来聊聊吧）"><a href="#ChatGPT（来聊聊吧）" class="headerlink" title="ChatGPT（来聊聊吧）"></a>ChatGPT（来聊聊吧）</h3><p>ChatGPT 是OpenAI在2022年基于 GPT-3 模型的升级版，主要针对对话任务进行了优化，增加了对话历史的输入和输出，以及对话策略的控制。ChatGPT 在对话任务上表现出色，可以与人类进行自然而流畅的对话。不过没有详细的论文说明了，技术细节大致、应该和InstructGPT差不多吧。</p>
<h3 id="GPT-4（这个模型能自己考大学了）："><a href="#GPT-4（这个模型能自己考大学了）：" class="headerlink" title="GPT-4（这个模型能自己考大学了）："></a>GPT-4（这个模型能自己考大学了）：</h3><p>GPT-4是OpenAI在2023年发布的最新一代模型。可以理解图片。GPT-4也是只有技术报告。</p>
<p><img src="https://pic4.zhimg.com/80/v2-576c77431181c147eb1293f7ab8b5393_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>在随意谈话中，ChatGPT和GPT-4之间的区别是很微妙的。只有当任务的复杂性达到足够的阈值时，差异就出现了，GPT-4比ChatGPT更可靠、更有创意，并且能够处理更细微的指令。为了了解这两种模型之间的差异，OpenAI在各种基准测试和一些为人类设计的模拟考试上进行了测试，并且取得了非常好的结果。同时GPT-4有很强的多模态能力。在这个报告包括各种爆表的性能，例如：</p>
<p>在随意谈话中，GPT-3.5和GPT-4之间的区别是很微妙的。只有当任务的复杂性达到足够的阈值时，差异就出现了，GPT-4比GPT-3.5 更可靠、更有创意，并且能够处理更细微的指令。为了了解这两种模型之间的差异，OpenAI在各种基准测试和一些为人类设计的模拟考试上进行了测试。</p>
<p>GPT-4在各种考试中，有几个测试几乎接近了满分，如：USABO Semifinal 2020（美国生物奥林匹克竞赛），GRE Writing。以美国 BAR律师执照统考为例，GPT3.5可以达到 10%水平，GPT4可以达到90%水平。生物奥林匹克竞赛从GPT3.5的31%水平，直接飙升到 99%水平</p>
<p>此外，OpenAI 还在为机器学习模型设计的传统基准上评估了 GPT-4。从实验结果来看，GPT-4 大大优于现有的大型语言模型，以及大多数 SOTA 模型</p>
<p>英伟达AI科学家Jim Fan点评道：「GPT-4最强的其实就是推理能力。它在GRE、SAT、法学院考试上的得分，几乎和人类考生没有区别。也就是说，GPT-4可以全靠自己考进斯坦福了。」（Jim Fan自己就是斯坦福毕业的！）</p>
<p><img src="https://pic3.zhimg.com/80/v2-f8ad4bf19e09541980ca6c0d422eacf6_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p><img src="https://pic4.zhimg.com/80/v2-0bc1b594fa4ff77d54a8702cd30f60bb_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>GPT-4在不同语种上的能力表现：中文的准确度大概在 80% 左右，已经要优于GPT-3.5的英文表现了。</p>
<p>许多现有的 ML 基准测试都是用英语编写的。为了初步了解GPT-4其他语言的能力，研究人员使用 Azure翻译将 MMLU 基准（一套涵盖57个主题的14000个多项选择题）翻译成多种语言。</p>
<p><img src="https://pic3.zhimg.com/80/v2-69d649be07e3e45aaa0d56a3432aaa5a_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>GPT-4模型的一大重点是建立了一个可预测扩展的深度学习栈。因为对于像GPT-4这样的大型训练，进行广泛的特定模型调整是不可行的。因此，OpenAI团队开发了基础设施和优化，在多种规模下都有可预测的行为。为了验证这种可扩展性，研究人员提前准确地预测了GPT-4在内部代码库（不属于训练集）上的最终损失，方法是通过使用相同的方法训练的模型进行推断，但使用的计算量为1&#x2F;10000。</p>
<p><img src="https://pic3.zhimg.com/80/v2-e2c1adbc4447ca028c738dc19a15a10e_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p><img src="https://pic4.zhimg.com/80/v2-7bbec106974d71b334b7b6df779a2a3b_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>GPT-4的规模报告里也没有提，不过可以从它Token的数量上大致推测一下：GPT-4最大的模型有32,768个Token，对比GPT-3.5（ 4,096 个）及GPT-3 （2,049个）规模有很大提升。</p>
<p><img src="https://pic2.zhimg.com/80/v2-4171055d2fca12a5ae8fed75bb1a9af1_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p><img src="https://pic3.zhimg.com/80/v2-bc78342a2662624e3d58816b2481e3a6_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<h3 id="GPT的影响"><a href="#GPT的影响" class="headerlink" title="GPT的影响"></a>GPT的影响</h3><p>既然GPT这么厉害，那么对大家的工作会有多少影响呢？OpenAI发布了调研报告《GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models》针对GPT-4对于不同工作的影响进行了分析。下面给大家简要介绍以下它的分析方法及部分结论，大家可以看看自己的行业有多大影响。</p>
<p>文章开头，估计是为了吓唬读者一下，OpenAI又给大家亮了一下GPT-4在各种考试上横扫的成绩：</p>
<p><img src="https://pic4.zhimg.com/80/v2-039aa000035d5f3ae770cf9f6d8ad563_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>然后是分析方法和各种结论，摘要部分分享给大家。</p>
<p><strong>分析方法</strong></p>
<p>OpenAI的这篇产业报告（下称报告）主要针对美国的职业信息网络数据集O<em>NET进行了人工加机器的评测。报告里使用了来自该数据集里1016个职业的信息。其中每个职业又被细化为他们的详细职业活动描述和任务类型，共计2087个不同的职业活动描述和19265个不同的任务。报告主要根据O</em>NET数据集里的描述信息，使用人工和机器（GPT4）来对比判断不同职业受大模型的影响程度。</p>
<p>报告所基于的人工评测主要来自于几位作者以及OpenAI的InstructGPT里雇佣过的标注人群。标注方法主要是基于标注者的主观评判，将不同工作分为三个受影响等级：无影响，直接影响，和受可预见的大模型生态影响。其中直接影响指的是如果直接使用chatGPT可以将完成该工作的职业活动描述或任务的工作时间减少一半以上。受可预见的大模型生态影响指的是虽然直接使用chatGPT不会将工作任务所需时间减半。</p>
<p><strong>部分结论</strong></p>
<p>算法认为如果考虑到当前的大模型的能力和可能营造的生态来说，至少50%的工作有百分之五十以上的工作内容，会在引入AI大模型后缩减至少一半的工作时间。而人类对此的判断更为悲观，接近百分之六十。</p>
<p><img src="https://pic3.zhimg.com/80/v2-5e5b1b59c78d08cdb3998235df9330ba_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>现在chatGPT所掀起的风暴，相比于未来所可能得惊涛骇浪，仅仅是小儿科般的前兆。随着OpenAI在2023-03-24日官宣的大模型+工具的生态接口，大模型的能力将会极大提升，而受波及的职业及人口将极速增加。</p>
<p>报告里评测了受大模型所影响的职业和职业人口的相关关系，无论是人工评测还是GPT4模型评测，受影响的深浅程度与就业人口的多寡总体来说有联系，但影响不太直观。即无论该职业的就业人口多寡，在技术浪潮面前没有显著差异。</p>
<p><img src="https://pic1.zhimg.com/80/v2-733de351a6b3f691e0f97dbfd6e2bca4_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>总体来看，薪酬更高的职业受大模型及其相关生态的影响更大。尽管低薪职业的方差较大（即存在受影响微乎其微的职业，也存在受大幅影响的职业，下面会解释何种职业受影响程度高或低）</p>
<p><img src="https://pic4.zhimg.com/80/v2-a3c686a42dd65b2bd43d125a3e4f268b_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>所需能力与写作和编程相关的职业受到影响最大。其次是交流能力和主动倾听的能力。从现有的chatGPT所展现出的能力来看，需要写作相关能力的如营销文案策划，需要主动倾听能力的如心理咨询，需要交流能力的如客服等职业都会受到剧烈的冲击。而值得注意的是，<strong>需要编程相关的工作，其回归数值的大小远远大于其他能力的数值（程序员痛哭ing）</strong>。</p>
<p><img src="https://pic4.zhimg.com/80/v2-e01bb62c5b9f48384282c745a26ed82b_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>报告里细化讨论了不同学历，培训周期所对应的职业受影响程度的大小。结论依然是越高学历所受的冲击越大。其中需要人力从事直接体力劳动的职业（如餐饮，保洁等，普遍不需要太长的职业前培训和教育程度的工作）受该轮AI爆发的冲击极小。而相反，如律师，设计师等需要大量时间职业前培训和教育程度的职业反而受到了巨大的冲击。</p>
<p>如果从行业划分的话，所有与体力相关的行业，如制造业，农业，矿业受本轮AI浪潮影响最小。而与之相反的是金融证券，保险行业，出版行业则受到的影响最大。体力劳动行业，和部分中位数收入较低的职业，但所需技能集中在沟通写作类的群体如客服群体，其受影响程度截然不同。</p>
<h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><blockquote>
<p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</p>
</blockquote>
<h3 id="原始论文翻译-1"><a href="#原始论文翻译-1" class="headerlink" title="原始论文翻译"></a>原始论文翻译</h3><blockquote>
<p>线上地址：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1810.04805.pdf">https://arxiv.org/pdf/1810.04805.pdf</a></p>
</blockquote>
<h4 id="摘要-1"><a href="#摘要-1" class="headerlink" title="摘要"></a>摘要</h4><p>我们引入一个新的语言表示模型 <strong>BERT</strong>，即 <strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentations from <strong>T</strong>ransformers。 与最近的语言表示模型（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xpeters-etal:2018:_deep">Peters 等人，2018a</a>；<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xradford-etal:2018">Radford 等人，2018</a>）不同，BERT 的设计是通过对所有层中的左右上下文进行联合调节，从未标注的文本中预训练深度双向表示。 因此，只需使用一个额外的输出层就可以对经过预训练的 BERT 模型进行微调，以创建适用于各种任务（例如问题解答和语言推论）的最新模型，而无需大量与特定任务相关的结构改动。<br>BERT 在概念上很简单，在实际应用中非常强大。 它在 11 种自然语言处理任务上获得了最新的最优结果，包括将 GLUE 得分提高到 80.5％（提升绝对值 7.7％），MultiNLI 准确度达到 86.7％（提升绝对值 4.6％），SQuAD v1.1 问答测试 F1 达到 93.2（提升绝对值 1.5 个点）以及 SQuAD v2.0 测试集 F1 达到 83.1（提升绝对值 5.1 个点）。</p>
<h4 id="1-简介-1"><a href="#1-简介-1" class="headerlink" title="1 简介"></a>1 简介</h4><p>语言模型预训练已经被证明可以有效地改善许多自然语言处理任务（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xdai-le:2015:_semi">Dai 和 Le，2015</a>； <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xpeters-etal:2018:_deep">Peters 等人，2018a</a>； <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xradford-etal:2018">Radford 等人，2018</a>； <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xhoward-ruder:2018">Howard 和 Ruder，2018</a>）。 这些任务包括句子级任务和词符级任务；句子级任务如自然语言推理（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xbowman-etal:2015">Bowman 等人，2015</a>；<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xwilliams-nangia-bowman:2018">Williams 等人，2018</a>）和释义（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xdolan-brockett:2005:_autom">Dolan 和 Brockett，2005</a>）旨在通过对句子进行整体分析来预测句子之间的关系，词符级任务如命名实体识别和问题回答要求模型在词符级产生细粒度的输出（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xtjong-de:2003">Tjong Kim Sang 和 De Meulder，2003</a>；<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xrajpurkar-etal:2016:_squad">Rajpurkar 等人，2016</a>）。</p>
<p>有两种现有的策略可以将预先训练好的语言表示应用到下游任务中：<em>基于特征</em> 和<em>微调</em>。 基于特征的方法，如 ELMo（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xpeters-etal:2018:_deep">Peters 等人，2018a</a>），使用特定于任务的架构，将预先训练好的表示作为额外的特征。 微调方法，如生成式预训练变换器（OpenAI GPT）（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xradford-etal:2018">Radford 等人，2018</a>），引入最少的任务相关的参数，通过简单的微调<em>所有</em> 预训练的参数，对下游任务进行训练。 这两种方法在预训练期间具有相同的目标函数，它们使用单向语言模型来学习通用语言表示形式。</p>
<p>我们认为，当前的技术限制了预训练表示的能力，特别是对于微调方法。 主要的限制是标准语言模型是单向的，并且这限制了可以在预训练期间使用的结构的选择。 例如，在 OpenAI GPT 中，作者使用从左到右的结构，每个词符只能在 Transformer 的自关注层中关注之前的词符（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xvaswani-etal:2017:_atten">Vaswani 等人，2017</a> ）。 这种限制对于句子级的任务来说不是最优的，而且在应用基于微调的方法来处理词符级任务如问题回答时可能是非常有害的，因为在这种情况下从两个方向结合上下文至关重要。</p>
<p>在本文中，我们通过提出 BERT 对基于微调的方法进行改进：<strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentations from <strong>T</strong>ransformers。 BERT 受 Cloze任务(<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xtaylor:1953:_cloze">Taylor，1953</a>)的启发，通过使用 “屏蔽语言模型”（masked language model，MLM）预训练目标，缓解了前面提到的单向性约束。 屏蔽语言模型从输入中随机屏蔽部分词符，目标是仅根据上下文预测屏蔽掉的单词的原始词汇 ID。 与从左到右的语言模型预训练不同，MLM 目标使表示形式能够融合左右上下文，让我们可以预训练深层双向 Transformer。 除了屏蔽语言模型，我们还使用“预测下一个句子”任务来联合预训练文本对的表示。 本文的贡献如下：</p>
<ul>
<li>我们演示了双向预训练对于语言表示的重要性。 不同于 <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xradford-etal:2018">Radford 等人</a> （<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xradford-etal:2018">2018</a>），它使用单向语言模型进行预训练，BERT 使用屏蔽语言模型来实现预训练的深度双向表示。 这也不同于 <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xpeters-etal:2018:_deep">Peters 等人</a> （<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xpeters-etal:2018:_deep">2018a</a>），它使用经过独立训练的从左到右和从右到左 LM 的浅层连接。</li>
<li>我们表明，经过预训练的表示可以减少许多精心设计特定于任务的结构的需求。 BERT 是第一个在大量句子级<em>以及</em> 词符级任务上实现最先进性能的基于微调的表示模型，优于许多特定于任务的结构。</li>
<li>BERT 推进了 11 项 NLP 任务的最先进结果。 可以在<a href="https://link.zhihu.com/?target=https://github.com/google-research/bert">https://github.com/google-research/bert</a>上找到代码和经过预先训练的模型。</li>
</ul>
<h3 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2 相关工作"></a>2 相关工作</h3><p>预先训练通用语言表示形式已有很长的历史，本节中我们简要回顾使用最广泛的方法。</p>
<h3 id="2-1-基于特征的无监督方法"><a href="#2-1-基于特征的无监督方法" class="headerlink" title="2.1 基于特征的无监督方法"></a>2.1 基于特征的无监督方法</h3><p>几十年来，学习可广泛应用的单词表示方法一直是一个活跃的研究领域，包括非神经网络方法（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xbrown-etal:1992:_class">Brown 等人，1992</a>；<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xando-zhang:2005">Ando 和 Zhang，2005</a>；<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xblitzer-mcdonald-pereira:2006:_domain">Blitzer 等人，2006</a>）和神经网络方法（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xmikolov-etal:2013">Mikolov 等人，2013</a>； <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xpennington-socher-manning:2014:_glove">Pennington 等人，2014</a>）。 预训练的词嵌入是现代 NLP 系统不可或缺的一部分，与从零开始学习的嵌入相比有显着改进（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xturian-ratinov-bengio:2010:_word_repres">Turian 等人，2010</a>）。 为了预训练词嵌入向量，已使用从左到右的语言建模目标（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xminh09">Mnih 和 Hinton，2009</a>），以及在左右上下文中区分正确单词和错误单词的目标（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xmikolov-etal:2013">Mikolov 等人，2013</a>）。</p>
<p>这些方法已经推广到更粗粒度，例如句子嵌入（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xkiros-etal:2015:_skip">Kiros 等人，2015</a>；<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xlogeswaran2018an">Logeswaran 和 Lee，2018</a>）或段落嵌入（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xle-mikolov:2014:_distr">Le 和 Mikolov，2014</a>）。 为了训练句子表示，以前的工作使用对候选的下一句进行评分排序（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23XDBLP:journals/corr/JerniteBS17">Jernite 等人，2017</a>；<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xlogeswaran2018an">Logeswaran 和 Lee，2018</a>）的目标，给定前一个句子的表示从左到右生成下一个句子的单词（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xkiros-etal:2015:_skip">Kiros 等人，2015</a>）或去噪自动编码器派生的目标（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xhill16">Hill 等人，2016</a>）。</p>
<p><img src="https://pic4.zhimg.com/80/v2-ac609797b8f0ae4974db28969868d5cb_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p><strong>图1：</strong>BERT 的总体预训练和微调程序。除了输出层，在预训练和微调中都使用相同的体系结构。相同的预训练模型参数用于初始化不同下游任务的模型。在微调期间，所有参数都将进行微调。[CLS]是在每个输入示例前添加的特殊符号，而[SEP]是特殊的分隔符（例如，分隔问题&#x2F;答案）。</p>
<p>ELMo 及其前身（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xpeters-etal:2017:_semi">Peters等人，2017，2018a</a>）沿不同维度推广了传统单词嵌入的研究。 他们结合从左到右和从右到左的语言模型提取<em>上下文相关</em> 特征。 每个词符的上下文表示是从左至右和从右至左表示的首尾相连。 当将上下文词嵌入与现有的特定于任务的体系结构集成时，ELMo 改进了几种主要的 NLP 基准的最先进结果（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xpeters-etal:2018:_deep">Peters 等人，2018a</a>），包括问题回答（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xrajpurkar-etal:2016:_squad">Rajpurkar 等人，2016</a>）、情感分析（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xsocher-etal:2013:_recur">Socher 等人，2013</a>）和命名实体识别（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xtjong-de:2003">Tjong Kim Sang 和 De Meulder，2003</a>）。 <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xmelamud2016context2vec">Melamud 等人</a> （<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xmelamud2016context2vec">2016</a>）提出通利用 LSTM 从左右上下文中预测单个单词的任务学习上下文表征。 与 ELMo 相似，它们的模型是基于特征的并且不是深度双向的。 <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xfedus2018maskgan">Fedus 等人</a> （<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xfedus2018maskgan">2018</a>）表明，cloze 任务可用于提高文本生成模型的鲁棒性。</p>
<h3 id="2-2-无监督微调方法"><a href="#2-2-无监督微调方法" class="headerlink" title="2.2 无监督微调方法"></a>2.2 无监督微调方法</h3><p>与基于特征的方法一样，在这个方向上的早期工作只是在未标记的文本上预训练单词的嵌入参数（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xcollobert-weston:2008">Collobert 和 Weston，2008</a>）。</p>
<p>最近，产生上下文词符表示的句子或文档编码器已经从未标记的文本中预训练，并针对有监督的下游任务进行微调（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xdai-le:2015:_semi">Dai 和 Le，2015</a>；<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xhoward-ruder:2018">Howard 和 Ruder，2018</a>；<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xradford-etal:2018">Radford 等人，2018</a>）。 这些方法的优点是几乎不需要从头学习参数。 至少部分由于此优势，OpenAI GPT（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xradford-etal:2018">Radford 等人，2018</a>）在 GLUE 基准测试中许多句子级任务上获得了以前的最先进结果（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xwang-etal:2018:_glue">Wang 等人，2018a</a>）。 从左到右的语言建模和自动编码器目标已被用于预训练此类模型（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xhoward-ruder:2018">Howard 和 Ruder，2018</a>；<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xradford-etal:2018">Radford 等人，2018</a>；<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xdai-le:2015:_semi">Dai 和 Le，2015</a>）。</p>
<h3 id="2-3-从监督数据迁移学习"><a href="#2-3-从监督数据迁移学习" class="headerlink" title="2.3 从监督数据迁移学习"></a>2.3 从监督数据迁移学习</h3><p>也有工作显示了从大数据集监督任务的有效转移，如自然语言推理（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xconneau-EtAl:2017:EMNLP2017">Conneau 等人，2017</a>）和机器翻译（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xmccann-etal:2017:_learn_trans">McCann 等人，2017</a>）。 计算机视觉研究还证明了从大型预训练模型进行迁移学习的重要性，其中一个有效的方法是微调用 ImageNet 预训练的模型（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Ximagenet_cvpr09">Deng 等人，2009</a>； <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xyosinski2014transferable">Yosinski 等人，2014</a>）。</p>
<h3 id="3-BERT"><a href="#3-BERT" class="headerlink" title="3 BERT"></a>3 BERT</h3><p>我们将在本节中介绍 BERT 及其详细实现。 我们的框架有两个步骤：<em>预训练</em> 和<em>微调</em>。 在预训练期间，通过不同的预训练任务在未标记的数据上进行模型训练。 对于微调，首先使用预训练的参数初始化 BERT 模型，然后使用下游任务中的标记数据对所有参数进行微调。 每个下游任务都有单独的微调模型，即使它们使用相同的预训练参数进行了初始化。 图 <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23x1-3001r1">1</a> 中的问答示例将作为本节的运行示例。</p>
<p>BERT 的一个独有的特征是其跨不同任务的统一结构。 预训练的结构和最终的下游结构之间的差异很小。</p>
<p><strong>模型结构</strong> BERT 的模型结构是一种多层 Transformer 编码器，它基于的原始实现的描述位于 <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xvaswani-etal:2017:_atten">Vaswani 等人</a> （<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xvaswani-etal:2017:_atten">2017</a>）并发布在 tensor2tensor 库中。<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main2.html%23fn1x0">1</a> 因为 Transformer 的使用已经很普遍以及我们的实现与原始版本几乎相同，我们将省略模型结构的详尽背景说明并请读者参考 <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xvaswani-etal:2017:_atten">Vaswani 等人</a> （<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xvaswani-etal:2017:_atten">2017</a>）以及优秀的指南如“The Annotated Transformer”。<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main3.html%23fn2x0">2</a></p>
<p>在这项工作中，我们将网络层（即 Transformer 的网络模块）的数量表示为 <em>L</em>，将隐藏层大小表示为 <em>H</em>，并将自注意头的数量表示为 <em>A</em>。<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main4.html%23fn3x0">3</a> 我们主要报告两种模型大小的结果：<strong>BERTBASE</strong>(L&#x3D;12, H&#x3D;768, A&#x3D;12, Total Parameters&#x3D;110M) 和<strong>BERTLARGE</strong> (L&#x3D;24, H&#x3D;1024, A&#x3D;16, Total Parameters&#x3D;340M) 。</p>
<p>为了进行比较，选择BERT BASE具有与OpenAI GPT相同的模型大小。 但是，至关重要的是，BERT Transformer 使用双向自关注，而 GPT Transformer 使用受限的自我关注，其中每个词符只能关注其左侧的上下文。<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main5.html%23fn4x0">4</a></p>
<p><strong>输入&#x2F;输出表示</strong> 为了使 BERT 处理各种下游任务，我们的输入可以用一个词符序列明确地表示单个句子和一对句子（例如⟨Question, Answer ⟩）。 在整个工作中，“句子”可以是连续文本的任意范围，而不是实际的语言句子。 “序列”是指 BERT 的输入词符序列，它可以是一个句子或两个句子封装在一起。</p>
<p>我们使用 WordPiece 嵌入（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xwu-etal:2016:_googl">Wu 等人，2016</a>），具有 30,000 个词符的词汇表。 每个序列的第一个标记始终是特殊分类标记（[CLS]）。 与此标记对应的最终隐藏状态用作分类任务的聚合序列表示。 句子对封装在一起形成单个序列。 我们通过两种方式区分句子。 首先，我们使用特殊词符（[SEP]）将它们分开。 其次，我们向每个词符添加一个学习型嵌入，表明它是属于句子 A 还是句子 B。 如图 <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23x1-3001r1">1</a>所示，我们将输入嵌入表示为 <em>E</em>，将特殊的 [CLS] 词符的最终隐藏向量表示为 <em>C</em> ∈ ℝ<em>H</em>，第 <em>i</em> 个输入词符为的最终隐藏向量为 <em>Ti</em> ∈ ℝ<em>H</em>。</p>
<p>对于给定的词符，其输入表示是通过将相应的词符嵌入、分段嵌入和位置嵌入相加来构造的。 在图 <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23x1-8001r2">2</a> 中可以看到这种构造的可视化。</p>
<p><img src="https://pic2.zhimg.com/80/v2-e8887ce82afaf9125364b9bafc040d3d_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p><strong>图2：</strong>BERT 输入表示。输入的嵌入是词符嵌入、分段嵌入和位置嵌入的总和。</p>
<h3 id="3-1-预训练-BERT"><a href="#3-1-预训练-BERT" class="headerlink" title="3.1 预训练 BERT"></a>3.1 预训练 BERT</h3><p>不同于 <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xpeters-etal:2018:_deep">Peters 等人</a> （<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xpeters-etal:2018:_deep">2018a</a>）和 <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xradford-etal:2018">Radford 等人</a> （<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xradford-etal:2018">2018</a>），我们没有使用传统的从左到右或从右到左的语言模型对 BERT 进行预训练。 相反，我们使用本节中描述的两个无监督任务对 BERT 进行预训练。 该步骤显示在图 <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23x1-3001r1">1</a> 的左侧。</p>
<p><strong>任务＃1：屏蔽 LM</strong> 直观地讲，我们有理由相信，一个深层的双向模型严格来说比从左到右的模型或从左到右和从右到左的浅层模型连接更强大。 不幸的是，标准的条件语言模型只能从左至右<em>或</em> 从右至左进行训练，因为双向条件会让每个词间接地 “看到自己”，从而模型在多层上下文中可以十分容易地预测目标词。</p>
<p>为了训练深度双向表示，我们简单地随机屏蔽一定百分比的输入词符，然后预测这些屏蔽的词符。 尽管此过程在文献中通常被称为 <em>Cloze</em> 任务（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xtaylor:1953:_cloze">Taylor，1953</a>），但我们将该过程称为“屏蔽 LM”（MLM）。 这种情况下，和在标准 LM 中一样，与屏蔽词符相对应的最终隐藏向量被送入词汇表上的输出 softmax 中。 在所有实验中，我们随机屏蔽每个序列中所有 WordPiece 词符的 15％。 与去噪自动编码器（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xvincent:2008">Vincent 等人，2008</a>）不同，我们只预测被屏蔽的单词，而不重构整个输入。</p>
<p>尽管这可以使我们获得双向的预训练模型，但缺点是我们在预训练和微调之间造成了不一致，因为 [MASK] 词符不会在微调期间出现。 为了缓解这种情况，实际上我们并不总是用 [MASK] 词符替换“被屏蔽”的单词。 训练数据生成器随机选择词符位置的 15％ 进行预测。 如果选择第 <em>i</em> 个词符，我们替换这第 <em>i</em> 个词符为（1）80％ 的时间用 [MASK] 词符（2）10％ 的时间用随机词符（3）10％ 的时间维持第 <em>i</em> 词符不变。 然后，将用 <em>Ti</em> 以交叉熵损失预测原始词符。 我们在附录 <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23x1-45000C.2">C.2</a> 中比较了此过程的各种变体。</p>
<p><strong>任务 #2：下一句预测（NSP）</strong> 许多重要的下游任务，例如问答（QA）和自然语言推理（NLI）是是基于理解两个句子之间的<em>关系</em>，而这不是语言建模直接捕获的。 为了训练能够理解句子关系的模型，我们预训练了可以从任何单语语料库很容易生成的二值<em>下一个句子预测</em> 任务。 具体来说，当为每个预训练样本选择句子 A 和 B 时，50％ 的时间 B 是紧随其后的实际下一个句子 A（标记为IsNext），50％ 的时间是语料库中的随机句子（标记为 NotNext）。 如图 <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23x1-3001r1">1</a> 所示，<em>C</em>用于下一句预测（NSP）。<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main6.html%23fn5x0">5</a> ，尽管非常简单，我们在 <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23x1-190005.1">5.1</a> 节中展示此任务的预训练对 QA 和 NLI 都非常有益。 <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main7.html%23fn6x0">6</a> NSP 任务与表示学习目标很接近，参见 <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23XDBLP:journals/corr/JerniteBS17">Jernite 等人</a> （<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23XDBLP:journals/corr/JerniteBS17">2017</a>）和 <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xlogeswaran2018an">Logeswaran 和 Lee</a>（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xlogeswaran2018an">2018</a>）。 但是，在之前的工作中，只有句子嵌入被传输到下游任务，而 BERT 传输所有参数以初始化最终任务模型的参数。</p>
<p><strong>预训练数据</strong> 预训练过程在很大程度上遵循有关语言模型预训练的现有文献。 对于预训练语料库，我们使用 BooksCorpus（800 万个单词）（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xzhu:2015">Zhu 等人，2015</a>）和英语 Wikipedia（25 亿个单词）。 对于 Wikipedia，我们仅提取文本段落，而忽略列表、表格和标题。 为了提取长的连续序列，使用文档级语料库而不是洗乱的句子级语料库如 Billion Word Benchmark（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xchelba-etal:2013:_one">Chelba 等人，2013</a>）至关重要。</p>
<h3 id="3-2-微调-BERT"><a href="#3-2-微调-BERT" class="headerlink" title="3.2 微调 BERT"></a>3.2 微调 BERT</h3><p>微调很简单，因为 Transformer 中的自我关注机制允许 BERT 通过交换适当的输入和输出来建模许多下游任务（无论它们涉及单个文本还是文本对）。 对于涉及文本对的应用，常见的模式是在应用双向交叉注意之前对文本对进行独立编码，例如 <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xparikh-etal:2016">Parikh 等人</a> （<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xparikh-etal:2016">2016</a>）； <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xbidaf">Seo 等人</a>（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xbidaf">2017</a>）。 BERT 使用自我注意机制来统一这两个阶段，因为使用自我注意编码连接的文本对行实际上包括了两个句子之间的<em>双向</em> 交叉注意。</p>
<p>对于每个任务，我们只需将特定于任务的输入和输出送入 BERT，并端到端微调所有参数。 在输入处，来自预训练的句子 A 和句子 B 类比于（1）paraphrasing 中的句子对（2）entailment 中的假设-前提对（3）question answering 中使用的 question-passage 对，以及（4）在文本分类或序列标记中使用退化的 text-∅ 对。 在输出处，将词符表示输入到输出层中以进行词符级任务如序列标记或问题回答，将 [CLS] 表示输入到输出层中进行分类如蕴含或情感分析。</p>
<p>与预训练相比，微调代价相对较小。 从完全相同的预训练模型开始，论文中的所有结果都可以复现，在单个 Cloud TPU 上最多 1 个小时，在 GPU 上最多需要几个小时。<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main8.html%23fn7x0">7</a> 我们在第 <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23x1-130004">4</a> 节的相应小节中描述特定任务的细节。 更多详细信息，请参见附录 <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23x1-31000A.5">A.5</a>。</p>
<p><img src="https://pic3.zhimg.com/80/v2-85ce260e0391f4729089418cdc554e02_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p><strong>表格1：</strong>GLUE 测试结果，由评估服务器（<a href="https://link.zhihu.com/?target=https://gluebenchmark.com/leaderboard">https://gluebenchmark.com/leaderboard</a>）评分。每个任务下方的数字表示训练样本的数量。“平均”列与官方 GLUE 得分略有不同，因为我们排除了有问题的 WNLI 集。<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main9.html%23fn8x0">8</a>BERT 和 OpenAI GPT 是单模型、单任务。QQP 和 MRPC 报告的是 F1 分数，STS-B 报告的是 Spearman 相关性，其他任务报告的是准确率分数。我们不包括使用 BERT 作为其组件之一的条目。</p>
<h3 id="4-实验"><a href="#4-实验" class="headerlink" title="4 实验"></a>4 实验</h3><p>在本节中，我们介绍了11个NLP任务的BERT微调结果。</p>
<h3 id="4-1-GLUE"><a href="#4-1-GLUE" class="headerlink" title="4.1 GLUE"></a>4.1 GLUE</h3><p>通用语言理解评估（GLUE）基准（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xwang-etal:2018:_glue">Wang等人，2018a</a>）是各种自然语言理解任务的集合。 GLUE数据集的详细说明包含在附录<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23x1-33000B.1">B.1</a>中。</p>
<p>为了对 GLUE 进行微调，我们按照第 <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23x1-60003">3</a> 节中的描述表示输入序列（针对单个句子或句子对），并使用对应于第一个输入词符（[CLS]）的最终的隐藏向量 <em>C</em> ∈ℝ<em>H</em> 作为聚合表示。 微调期间引入的唯一新参数是分类层权重<em>W</em> ∈ ℝ <em>K</em> × <em>H</em>，其中<em>K</em>是标签数。 我们用 <em>C</em> 和 <em>W</em> 计算标准分类损失，即 log(<em>softmax</em>(<em>CWT</em> ))。</p>
<p>对于所有 GLUE 任务，我们使用的 batch size 为 32， 在数据上进行 3 个 epoch 的微调。 对于每个任务，我们在开发集上选择最佳的微调学习率（在5e-5、4e-5、3e-5 和 2e-5中）。 另外，对于 BERTLARGE，我们发现微调有时在小型数据集上不稳定，因此我们并在开发集上进行了几次随机重跑，选择最佳模型。 对于随机重跑，我们使用相同的预训练检查点，但执行不同的微调数据洗乱和分类器层初始化。<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main10.html%23fn9x0">9</a></p>
<p>结果显示在表 <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23x1-12002r1">1</a> 中。 BERTBASE 和 BERTLARGE 在所有任务上的性能均优于所有系统，分别比之前的最先进结果提高了 4.5％ 和 7.0％。 请注意，除了注意力屏蔽之外，BERTBASE 和 OpenAI GPT 在模型架构方面几乎相同。 对于最大且报告最广的 GLUE 任务 MNLI，BERT 获得 4.6％ 的绝对准确度改进。 在官方 GLUE 排行榜<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main11.html%23fn10x0">10</a> 上，BERTLARGE 得分为 80.5，而截至撰写之日 OpenAI GPT 得分为 72.8 。</p>
<p>我们发现在所有任务中，尤其是训练数据很少的任务，BERTLARGE 明显优于 BERTBASE。 在<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23x1-200005.2">5.2</a>部分中更全面地探讨了模型大小的影响。</p>
<h3 id="4-2-SQuAD-v1-1"><a href="#4-2-SQuAD-v1-1" class="headerlink" title="4.2 SQuAD v1.1"></a>4.2 SQuAD v1.1</h3><p>斯坦福问答数据集（SQuAD v1.1）是10万个众包问题&#x2F;答案对的集合（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xrajpurkar-etal:2016:_squad">Rajpurkar等人，2016</a>）。 给定一个问题以及Wikipedia中包含答案的段落，任务是预测段落中的答案文本范围。</p>
<p>如图 <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23x1-3001r1">1</a> 所示，在问题回答任务中，我们将输入的问题和段落表示封装成单个序列，其中问题使用 A 嵌入，而段落使用 B 嵌入。 在微调期间我们只引入一个开始向量 <em>S</em> ∈ℝ<em>H</em> 和一个结束向量 <em>E</em>∈ℝ<em>H</em>。 单词 <em>i</em> 是答案范围的开始的概率计算为 <em>Ti</em> 和 <em>S</em> 之间的点积，后面是一个段落上单词的 softmax：<em>Pi</em> &#x3D; </p>
<p><img src="https://pic3.zhimg.com/80/v2-502ebb8909c94ddae47c8a8d170e46fa_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>。 答案范围的结束用类似的公式。 候选范围位置 <em>i</em> 到位置 <em>j</em> 的分值定义为 <em>S</em>⋅<em>Ti</em> +<em>E</em>⋅<em>Tj</em>，分值最大的范围作为预测，其中 <em>j</em>≥<em>i</em>。 训练目标是正确的开始和结束位置的对数似然率的总和。 我们以 3e-5 的学习率和 32 的批处理大小微调 3 个 epoch。</p>
<p><img src="https://pic3.zhimg.com/80/v2-a637ae9556aaf25d2048c7316061e026_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>表 <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23x1-15001r2">2</a> 显示顶级排行榜和发表的顶级系统（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xbidaf">Seo 等人，2017</a>；<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xclark-gardner:2018:_simpl">Clark 和 Gardner，2018</a>；<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xpeters-etal:2018:_deep">Peters 等人，2018a</a>；<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xhu2017reinforced">Hu 等人，2018</a>）。 SQuAD 排行榜的最高结果没有公开最新系统的描述，<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main12.html%23fn11x0">11</a> ，允许使用任何公共数据训练他们的系统。 因此，我们首先在 TriviaQA 上进行微调（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xjoshi-etal:2017:_triviaq">Joshi等人，2017</a>），然后在 SQuAD 上进行微调，从而在系统中使用适度的数据增强。</p>
<p>我们表现最好的系统在集成时比排行榜最高的系统高出 +1.5 个 F1 值，在单一系统时高出 +1.3 个 F1 值。 实际上，就 F1 分数而言，我们的单一 BERT 模型优于最好的集成系统。 如果没有 TriviaQA 的微调数据，我们只会损失 0.1-0.4 个 F1 值，仍然远远胜过所有现有系统。<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main13.html%23fn12x0">12</a></p>
<h3 id="4-3-SQuAD-v2-0"><a href="#4-3-SQuAD-v2-0" class="headerlink" title="4.3 SQuAD v2.0"></a>4.3 SQuAD v2.0</h3><p>SQuAD 2.0任务通过允许在提供的段落中不存在简短答案的可能性扩展了SQuAD 1.1问题定义，从而使问题更加实际。</p>
<p>我们使用一种简单的方法来扩展SQuAD v1.1 BERT模型以完成此任务。 我们将没有答案的问题视为答案范围的开始和结束都位于 [CLS] 词符。 答案范围开始和结束位置的概率空间扩展为包括 [CLS] 词符的位置。 对于预测，我们比较非答案的范围 <em>snull</em> &#x3D;<em>S</em>⋅<em>C</em>+<em>E</em>⋅<em>C</em> 和最高的非空范围 <em>si,j</em> &#x3D; <em>maxj</em>≥<em>iS</em>⋅<em>Ti</em> +<em>E</em>⋅<em>Tj</em> 得分。 我们预测非空答案满足 <em>si,j</em> <em>&gt; snull</em> + <em>τ</em>，其中阈值 <em>τ</em> 在开发集上选择以最大化 F1 值。 我们没有为此模型使用TriviaQA数据。 我们微调了 2个 epoch，学习率为 5e-5，批次大小为 48。</p>
<p>与之前的排行榜和已发表的最好工作（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xunet">Sun 等人，2018</a>；<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xslqa">Wang 等人，2018b</a>）的比较结果显示在表 <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23x1-15003r3">3</a> 中，不包括使用 BERT 作为其组件之一的系统。 与之前的最佳系统相比，我们观察到 +1 F1 的改进。</p>
<h3 id="4-4-SWAG"><a href="#4-4-SWAG" class="headerlink" title="4.4 SWAG"></a>4.4 SWAG</h3><p>Situations With Adversarial Generations（SWAG）数据集包含 113k 个句子对补全样本，用于评估基础常识推理（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xzellers2018swag">Zellers 等人，2018</a>）。 这个任务是在给定一个句子的情况下，在四个选项中选择最合理的选项来延续这个句子。</p>
<p>在SWAG数据集上进行微调时，我们构造了四个输入序列，每个输入序列包含给定句子（句子A）和可能的延续词（句子B）的连接。 引入的唯一特定于任务的参数是一个向量，它与 [CLS] 词符表示 <em>C</em> 的点积表示每个选项的得分，并使用 softmax 层对其进行归一化。</p>
<p>我们对模型进行了 3个 epoch的微调，学习率为 2e-5，批处理大小为 16。 结果显示在表 <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23x1-17001r4">4</a> 中。 BERTLARGE 比作者的基准 ESIM + ELMo 系统高出 +27.1％，比 OpenAI GPT 高出 8.3％。</p>
<p><img src="https://pic3.zhimg.com/80/v2-d5fc06dd037eed926fdb21dc0b4c02d6_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p><strong>表4：</strong>SWAG 开发集和测试集准确率。 †在 SWAG 论文的报告中，人类的表现是用 100 个样本测量的。</p>
<h3 id="5-细分研究"><a href="#5-细分研究" class="headerlink" title="5 细分研究"></a>5 细分研究</h3><p>在本节中，我们将对 BERT 的多个方面进行细分实验，以更好地了解它们的相对重要性。 其它细分研究可在附录 <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23x1-43000C">C</a> 中找到。</p>
<h3 id="5-1-预训练任务的效果"><a href="#5-1-预训练任务的效果" class="headerlink" title="5.1 预训练任务的效果"></a>5.1 预训练任务的效果</h3><p>我们通过使用与 BERTBASE 完全相同的预训练数据、微调方案和超参数来评估两个预训练目标，证明了 BERT 深度双向性的重要性。<br><strong>No NSP</strong>：使用 “屏蔽 LM”（MLM）训练的双向模型，但没有 “下一句预测”（NSP）任务。<br><strong>LTR &amp; No NSP</strong>：一个仅有左侧上下文的模型，它是用标准的从左到右（LTR）LM，而不是 MLM 训练。 左约束也应用于微调，因为删除它会引入预训练&#x2F;微调不匹配，从而降低下游性能。 此外，该模型无需NSP任务即可进行预训练。 这可以直接与OpenAI GPT相提并论，但要使用更大的训练数据集，输入表示形式和微调方案。</p>
<p><img src="https://pic2.zhimg.com/80/v2-6c84952209b6ffd25491f13d9bfab35d_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p><strong>表5：</strong>使用 BERTBASE 架构细分预训练任务。 “No NSP”使用没有下一个句子预测任务进行训练。 “LTR &amp; No NSP”以从左到右的 LM 训练，没有下一个句子预测，和 OpenAI GPT 一样。 在微调期间，“+ BiLSTM”在“LTR + No NSP”模型的顶部添加一个随机初始化的 BiLSTM。</p>
<p>我们首先研究 NSP 任务带来的影响。 在表 <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23x1-19001r5">5</a> 中，我们显示删除 NSP 会严重损害 QNLI、MNLI 和 SQuAD 1.1 的性能。 接下来，我们通过比较“No NSP”与“LTR &amp; No NSP”来评估训练双向表示的影响。 在所有任务上，LTR 模型的性能都比 MLM 模型差，而 MRPC 和 SQuAD 的性能下降很大。</p>
<p>对于 SQuAD，直观上很清楚，因为词符级别的隐藏状态没有右侧上下文，所以LTR 模型在词符预测时的性能会很差。 为了使 LTR 系统得到加强，我们在上面添加了一个随机初始化的 BiLSTM。 这确实可以显着改善 SQuAD 上的结果，但结果仍然比预训练的双向模型的结果差很多。 BiLSTM 损害了 GLUE 任务的性能。</p>
<p>我们认识到，也有可能像 ELMo 一样训练单独的 LTR 和 RTL 模型并将每个词符表示为两个模型的连接。 但是：（a）代价是单个双向模型的两倍；（b）对于 QA 这样的任务，这是不直观的，因为 RTL 模型将无法确定问题的答案；（c）这绝对不像深度双向模型那么强大，因为它可以在每一层使用左右上下文。</p>
<h3 id="5-2-模型大小的影响"><a href="#5-2-模型大小的影响" class="headerlink" title="5.2 模型大小的影响"></a>5.2 模型大小的影响</h3><p>在本节中，我们探索模型大小对微调任务准确性的影响。 我们训练了许多具有不同层数，隐藏单元和注意头的BERT模型，而其他方面则使用了与之前所述相同的超参数和训练过程。</p>
<p>表<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23x1-20001r6">6</a>中显示了选定的GLUE任务的结果。 在此表中，我们报告了5次随机微调重新启动后的平均Dev Set精度。 我们可以看到较大的模型使得所有四个数据集的准确性提高，即使对于只有 3,600 个带标签的训练样本且与预训练任务大不相同的 MRPC 也是一样。 我们能够在相对于现有文献而言已经相当大的模型的基础上实现如此显着的改进，这也许也令人惊讶。 例如，在 <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xvaswani-etal:2017:_atten">Vaswani 等人</a> （<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xvaswani-etal:2017:_atten">2017</a>）（L &#x3D; 6、H &#x3D; 1024、A &#x3D; 16）参数为 100M 的编码器，我们在文献中找到的最大 Transformer 是（L &#x3D; 64、H &#x3D; 512、A &#x3D; 2）具有 235M 个参数（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xalrfou:2018">Al-Rfou 等人，2018</a>）。 相比之下，BERTBASE 包含 110M 参数，而 BERTLARGE 包含 340M 参数。</p>
<p><img src="https://pic4.zhimg.com/80/v2-472bcf60fd6706834964f90d6bf212b3_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p><strong>表6：</strong> BERT 模型大小的分解。 #L &#x3D; 层数；#H &#x3D; 隐藏大小； #A &#x3D; 注意头的数量。 “ LM（ppl）”是保留的训练数据的屏蔽 LM 困惑度。</p>
<p>众所周知，增加模型大小将导致大规模任务如机器翻译和语言建模的不断改进，表 <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23x1-20001r6">6</a> 所示的 LM 对持有的训练数据的迷惑性就证明了这一点。 但是，我们认为，这是第一个有说服力的工作，证明只要模型已经过充分的预训练，将模型缩放到极端的模型大小也可以在非常小的规模的任务上带来很大的改进。 <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xpeters2018dissecting">Peters 等人</a> （<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xpeters2018dissecting">2018b</a>）公开了将预训练的 bi-LM 大小从两层增加到四层在下游任务上的混合结果以及 <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xmelamud2016context2vec">Melamud 等人</a> （<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xmelamud2016context2vec">2016</a>）顺便提到了将隐藏大小从 200 增加到 600 有帮助，但进一步增加到 1000 并没有带来进一步的改进。 这两个先前的工作都使用基于特征的方法-我们假设当直接在下游任务上微调模型并且仅使用很少数量的随机初始化的附加参数时，特定于任务的模型可以从较大的模型中受益，即使下游任务数据非常小，也可以提供更具表现力的预训练表示形式。</p>
<h3 id="5-3-基于特征的-BERT-方法"><a href="#5-3-基于特征的-BERT-方法" class="headerlink" title="5.3 基于特征的 BERT 方法"></a>5.3 基于特征的 BERT 方法</h3><p>到目前为止，所有提出的 BERT 结果都使用微调方法，即在预训练模型中添加一个简单的分类层，在下游任务上对所有参数进行共同微调。 但是，基于特征的方法，即从预训练的模型中提取固定的特征，也有一定的优势。 首先，并非所有任务都可以由 Transformer 编码器结构轻松表示，因此需要添加特定于任务的模型结构。 其次，预计算一次训练数据的昂贵表示，然后在这个表示之上用更便宜的模型运行许多实验，有很大的计算优势。</p>
<p>在本节中，我们通过将 BERT 应用于 CoNLL-2003 命名实体识别（NER）任务（<a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23Xtjong-de:2003">Tjong Kim Sang 和 De Meulder，2003</a>）来比较这两种方法。 在 BERT 的输入中，我们使用一个大小写保留的 WordPiece 模型，并且最大程度包含数据提供的文档上下文。 按照标准惯例，我们将其公式化为标记任务，但在输出中不使用 CRF 层。 我们使用第一个子词符的表示作为 NER 标签集上词符级分类器的输入。</p>
<p>为了详细分解研究微调方法，我们应用基于特征的方法，在<em>没有</em> 微调 BERT 任何参数的情况下，提取一个或多个层的激活。 这些上下文嵌入用作分类层之前的随机初始化的两层 768 维 BiLSTM 的输入。</p>
<p>结果显示在表 <a href="https://link.zhihu.com/?target=https://www.yiyibooks.cn/__trs__/nlp/bert/main.html%23x1-21001r7">7</a> 中。 BERT LARGE与最先进的方法相比具有竞争力。 表现最好的方法将来自预训练的 Transformer 的顶部四个隐藏层的词符表示连接起来，这仅比微调整个模型低 0.3 F1。 这表明 BERT 对于微调和基于特征的方法均有效。</p>
<p><img src="https://pic2.zhimg.com/80/v2-decfaf3784a86093599d73d9fca9042d_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p><strong>表7：</strong>CoNLL-2003 命名实体识别结果。 超参数的选择使用开发集。 报告的开发集和测试集得分是用这些超参数在 5 次随机重启后的平均值。</p>
<h3 id="6-结论"><a href="#6-结论" class="headerlink" title="6 结论"></a>6 结论</h3><p>最近由于语言模型的迁移学习经验所带来的改进表明，丰富的、无监督的预训练是许多语言理解系统中不可或缺的一部分。 尤其是，这些结果使即使是资源很少的任务也可以从深度单向结构中受益。 我们的主要贡献是将这些发现进一步推广到更深的<em>双向</em> 体系结构中，从而使相同的预训练模型能够成功解决各种 NLP 任务。</p>
<h3 id="Bert实践"><a href="#Bert实践" class="headerlink" title="Bert实践"></a>Bert实践</h3><h2 id="XLM"><a href="#XLM" class="headerlink" title="XLM"></a>XLM</h2><h2 id="GPT-2"><a href="#GPT-2" class="headerlink" title="GPT-2"></a>GPT-2</h2><h2 id="RoBERTa"><a href="#RoBERTa" class="headerlink" title="RoBERTa"></a>RoBERTa</h2><blockquote>
<p>论文地址：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1907.11692.pdf">https://arxiv.org/pdf/1907.11692.pdf</a></p>
</blockquote>
<h3 id="论文翻译"><a href="#论文翻译" class="headerlink" title="论文翻译"></a>论文翻译</h3><h4 id="0、摘要："><a href="#0、摘要：" class="headerlink" title="0、摘要："></a>0、摘要：</h4><p>语言模式预训练已经带来了显著的性能提升，但仔细比较不同方法是一个挑战。训练的计算成本很高，通常是在不同大小的私有数据集上进行的，我们将展示，超参数选择对最终结果有重大影响。我们对BERT 预训练进行了一项复制研究，仔细测量了许多关键超参数和训练数据大小的影响。</p>
<p><strong>我们发现，BERT的训练明显不足</strong>，可以与发布后的每个模型的性能相匹配或超过。作者从模型设计选择(design choice)、训练策略、语料等方面入手，重新对BERT进行了预训练，得到RoBERTa。实验结果表明RoBERTa在GLUE、RACE和SQuAD都达到了SOTA。这些结果突出了以前被忽略的设计选择的重要性。</p>
<h4 id="1、介绍："><a href="#1、介绍：" class="headerlink" title="1、介绍："></a>1、介绍：</h4><p>自训练方法，如ELMo、GPT、XLM和XLNet带来了显著的性能提升，但是很难说清，这么方法中，哪一部分起到最关键的作用。训练在计算上是昂贵的，限制了可以进行的调整的数量，并且通常使用大小不同的私人训练数据进行训练，限制了我们测量建模效果的能力有所提高。</p>
<p>我们提出了一项BERT预训练的复制研究，其中包括对超参数调节和训练集大小的影响的仔细评估。我们发现BERT的训练明显不足，并提出了一种改进的训练BERT模型的方法，我们称之为RoBERTa，它可以匹配或超过所有post-BERT方法的性能。</p>
<p><strong>○ RoBERTa改进很简单，包括：</strong></p>
<ul>
<li>（1）用更多的数据、更大的batch_size、更长的时间来训练模型；</li>
<li>（2）移除BERT中的NSP任务；</li>
<li>（3）长序列训练；</li>
<li>（4）根据训练数据动态调整mask方案；</li>
<li>（5）使用新的更大的数据集CC-NEWS，以更好地控制训练集大小的影响。</li>
</ul>
<p>RoBERTa在GLUE任务上，创造了4项新的SOTA记录，并在SQuAD和RACE上也与SOTA水平相匹配。</p>
<p><strong>○ 总之，本文的贡献是：</strong></p>
<ul>
<li>（1）提出一系列重要的BERT设计选择和训练策略。</li>
<li>（2） 使用新的更大的数据集CC-NEWS，证明了更多的训练数据能够进一步提升BERT模型在下流任务中的表现。</li>
<li>（3）实验结果表明，masked language model在正确的模型设计配置下，比最近提出的其他模型方法都要好。</li>
</ul>
<h4 id="2、-背景："><a href="#2、-背景：" class="headerlink" title="2、 背景："></a>2、 背景：</h4><p>在本节中，我们简要概述了BERT的预训练方法和一些训练选择，我们将在下一节中进行实验研究。</p>
<h5 id="2-1-设置："><a href="#2-1-设置：" class="headerlink" title="2.1 设置："></a>2.1 设置：</h5><p>BERT将两段（tokens序列）的串联作为输入， x1, . . . , xN 及 y1, . . . , yM。片段通常由多个自然句子组成。这两个段以单个输入序列的形式呈现给BERT，并用特殊标记对其进行分隔： [CLS], x1, . . . , xN , [SEP], y1, . . . , yM, [EOS]。M和N受到约束，使得M+N&lt;T。其中T是一个参数，用于控制训练期间的最大序列长度。该模型首先在一个大型的未标记文本语料库上进行预训练，然后使用结束任务标记的数据进行微调。</p>
<h5 id="2-2-架构："><a href="#2-2-架构：" class="headerlink" title="2.2 架构："></a>2.2 架构：</h5><p>BERT使用了现在无处不在的Transformer架构，我们将不详细回顾。我们使用带有L层的Transformer架构。每个Block块使用一个self-attention heads，隐藏层的维度为 H。</p>
<h5 id="2-3-训练目标："><a href="#2-3-训练目标：" class="headerlink" title="2.3 训练目标："></a>2.3 训练目标：</h5><p><strong>○ 训练期间，BERT的两个训练目标：</strong></p>
<ul>
<li>（1）masked language modeling，<strong>MLM</strong>。</li>
<li>（2）next sentence prediction，<strong>NSP</strong>。</li>
</ul>
<p>**Masked Language Model (MLM)**：随机的输入序列中的token示例如下：选择并替换为特殊token[MASK]。在预测被masked的tokens时，MLM的目标函数是交叉熵损失函数。</p>
<p>○ <strong>BERT 的 mask策略：</strong><br>随机选择 15% 的 token；<br>这些15%要被 masked 的 token 并不会真的全替换成 [MASK]，而是从这些 token 中：<br>（1）随机选择 80% 替换成 [MASK] ；<br>（2）随机选择 10% 替换成随机 token；<br>（3）随机选择 10% 不改变原 token。<br>然后 使用<strong>交叉熵损失</strong>来预测原始的 token。</p>
<p>在最初的实现中，随机mask和replace在开始时执行一次，并在训练期间保存，尽管在实践中，数据是重复的，但每个训练句子的mask并不总是相同的。</p>
<p><strong>Next Sentence Prediction (NSP)<strong>：<br>下一句预测（NSP）NSP是一种二元分类损失，用于预测原始文本中两段话是否上下文。通过从文本语料库中选取</strong>连续的句子来构建正例</strong>。通过将不同文档中的片段配对，可以构建负例**。正反两个例子的抽样概率相等。</p>
<p><strong>NSP目标旨在提高下游任务的性能</strong>，例如自然语言推理，这些任务需要对句子对之间的关系进行推理。</p>
<h5 id="2-4-优化："><a href="#2-4-优化：" class="headerlink" title="2.4 优化："></a>2.4 优化：</h5><p><strong>BERT使用Adam进行优化</strong>，<strong>参数如下：</strong> β1 &#x3D; 0.9, β2 &#x3D; 0.999, ǫ &#x3D; 1e-6，L2重量衰减系数为0.01，在前10000个步骤中，学习速率被预热到1e-4的峰值，然后线性衰减。BERT训练时，所有层和注意力权重的dropout值均为0.1，并具有一个<strong>GELU激活函数</strong>。模型针对S&#x3D;1000000更新进行预训练，小批量包含最大长度为T&#x3D;512的 B&#x3D;256 序列标记。</p>
<h5 id="2-5-数据："><a href="#2-5-数据：" class="headerlink" title="2.5 数据："></a>2.5 数据：</h5><ul>
<li>BERT训练数据是BOOKCORPUS图书语料库和WIKIPEDIA英文维基百科的结合，总共16GB的未压缩文本。</li>
</ul>
<h4 id="3、实验设置："><a href="#3、实验设置：" class="headerlink" title="3、实验设置："></a>3、实验设置：</h4><p>在本节中，将介绍我们对BERT进行复制研究的实验设置。</p>
<h5 id="3-1-实施："><a href="#3-1-实施：" class="headerlink" title="3.1 实施："></a>3.1 实施：</h5><p>我们在FAIRSEQ重新实施了BERT。我们主要遵循原始BERT优化超参数，<strong>除了峰值学习率和预热步骤数</strong>，这两个参数分别针对每个设置进行调整。此外，我们<strong>还发现训练对Adam 的ε项非常敏感</strong>，在某些情况下，我们在调整后获得了更好的性能或稳定性。同样，我们发现当以较大的batch size训练时，将正则项系数β2&#x3D;0.98也能提升模型稳定性。</p>
<p>我们用最多T&#x3D;512个标记的序列进行预训练。与Devlin等人不同，我们不会随机注入短序列，也不会在前90%的更新中使用缩短的序列长度进行训练。我们只训练全长序列。我们在DGX-1机器上使用混合精度浮点算法进行训练，每台机器配备 8×32GB Nvidia V100 GPU，由Infiniband互连。</p>
<h5 id="3-2-数据："><a href="#3-2-数据：" class="headerlink" title="3.2 数据："></a>3.2 数据：</h5><p>BERT风格的预训练主要依赖于大量的文本。Baevski等人<strong>对比于BERT，通过几项工作针对更大、更多样化的数据集进行了训练，得以证明：增加数据大小可以提高最终任务的性能</strong>。</p>
<p>不幸的是，<strong>并非所有额外的数据集都可以公开发布</strong>。在文中的研究中，作者专注于收集尽可能多的实验数据，使能够在每次比较中匹配数据的整体质量和数量。</p>
<p>原文考虑五个不同大小和域的英语语料库，总计超过160GB的未压缩文本。使用以了下文本语料库：</p>
<ul>
<li>BOOKCORPUS：再加上英语维基百科。这是用于测试的原始数据训练BERT，大小为 16GB。</li>
<li>CC-NEWS：这是从新闻的英文部分收集的数据集。数据包含63个数以百万计的英语新闻文章，2016年过滤后为 76GB。</li>
<li>OPENWEBTEXT：这是对Radford等人中描述的WebText语料库的开源再创作。该文本是从Reddit上共享的URL中提取的web内容，至少有三次投票，大小为 38GB。</li>
<li>STORIES：Trinh和Le引入的一个数据集，其中包含经过过滤的CommonCrawl数据子集，以匹配Winograd模式的故事式风格，大小为 31GB。</li>
</ul>
<h5 id="3-3-评估："><a href="#3-3-评估：" class="headerlink" title="3.3 评估："></a>3.3 评估：</h5><p>在之前的工作之后，我们使用以下<strong>三个基准来评估下游任务的预训练模型</strong>。</p>
<ul>
<li><strong>（1）GLUE：</strong> <strong>G</strong>eneral <strong>L</strong>anguage <strong>U</strong>nderstanding <strong>E</strong>valuation</li>
</ul>
<h6 id="○-GLUE通用语言理解评估基准是评估自然语言理解系统的9个数据集的集合。"><a href="#○-GLUE通用语言理解评估基准是评估自然语言理解系统的9个数据集的集合。" class="headerlink" title="○ GLUE通用语言理解评估基准是评估自然语言理解系统的9个数据集的集合。"></a>○ GLUE通用语言理解评估基准是评估自然语言理解系统的9个数据集的集合。</h6><p>6项任务分为单句分类任务和句子对分类任务。GLUE组织者提供训练和开发数据拆分，以及一个提交服务器和排行榜，允许参与者评估和比较他们的系统与私有的测试数据。</p>
<p>对于第4节中的复制研究，我们报告了在相应的单任务训练数据（即，没有多任务训练或整合）上微调预训练模型后，开发集的结果。我们的微调程序遵循最初的BERT论文。在第5节中，我们还报告了从公共排行榜获得的测试集结果。这些结果取决于我们在第5.1节中描述的几个特定于任务的修改。</p>
<ul>
<li><strong>（2）SQuAD：</strong> <strong>S</strong>tanford <strong>Q</strong>uestion <strong>A</strong>nswering <strong>D</strong>ataset</li>
</ul>
<h6 id="○-斯坦福问答数据集（STAND）提供了一段上下文和一个问题。任务是回答这个问题通过从上下文中提取相关span。"><a href="#○-斯坦福问答数据集（STAND）提供了一段上下文和一个问题。任务是回答这个问题通过从上下文中提取相关span。" class="headerlink" title="○ 斯坦福问答数据集（STAND）提供了一段上下文和一个问题。任务是回答这个问题通过从上下文中提取相关span。"></a>○ 斯坦福问答数据集（STAND）提供了一段上下文和一个问题。任务是回答这个问题通过从上下文中提取相关span。</h6><p>我们评估了两个版本的SQuAD：V1.1 和 V2.0。在V1中上下文总是包含答案，而在V2中提供的上下文中，有些问题没有得到回答，使任务更具挑战性。</p>
<p>对于SQuAD 的V1.1版本，我们采用了与BERT相同的span预测方法。 对于SQuAD V2.2，我们添加了一个额外的二元分类器来预测问题是否可以回答，我们通过将分类项和span损失项相加来联合训练。在评估过程中，我们只预测分类为成对的span指数。</p>
<p><strong>（3）RACE：</strong> <strong>R</strong>eAding <strong>C</strong>omprehension from <strong>E</strong>xaminations</p>
<h6 id="○-RACE考试阅读理解任务是一个大规模的阅读理解数据集："><a href="#○-RACE考试阅读理解任务是一个大规模的阅读理解数据集：" class="headerlink" title="○ RACE考试阅读理解任务是一个大规模的阅读理解数据集："></a>○ RACE考试阅读理解任务是一个大规模的阅读理解数据集：</h6><p>包含28000多篇文章和近100000个问题。该数据集收集自中国为中学生设计的英语考试。在比赛中，每篇文章都有多个问题。对于每个问题的任务是从四个选项中选择一个正确答案。与其他流行的阅读理解数据集相比，RACE的语境要长得多，而且需要推理的问题比例非常大。</p>
<h4 id="4-训练程序分析："><a href="#4-训练程序分析：" class="headerlink" title="4 训练程序分析："></a>4 训练程序分析：</h4><p>该章节探索和量化什么样的设置对于预训练BERT是重要的。<br>作者实验的模型初始设置与BERT-base ( L &#x3D; 12 , H &#x3D; 768 , A &#x3D; 12 , 110 M params ) 一致。</p>
<h5 id="4-1-Static-vs-Dynamic-Masking："><a href="#4-1-Static-vs-Dynamic-Masking：" class="headerlink" title="4.1 Static vs. Dynamic Masking："></a>4.1 Static vs. Dynamic Masking：</h5><h6 id="○-BERT依赖于随机mask和预测标记。原始的BERT实现在数据预处理期间执行一次mask，从而产生一个静态mask。为了避免对每个epoch中的每个训练实例使用相同的mask，训练数据被复制了10次，以便在40个epoch中，每个序列以10种不同的方式被mask训练。因此，在训练过程中，每个训练序列都被用同一个mask观看四次。"><a href="#○-BERT依赖于随机mask和预测标记。原始的BERT实现在数据预处理期间执行一次mask，从而产生一个静态mask。为了避免对每个epoch中的每个训练实例使用相同的mask，训练数据被复制了10次，以便在40个epoch中，每个序列以10种不同的方式被mask训练。因此，在训练过程中，每个训练序列都被用同一个mask观看四次。" class="headerlink" title="○ BERT依赖于随机mask和预测标记。原始的BERT实现在数据预处理期间执行一次mask，从而产生一个静态mask。为了避免对每个epoch中的每个训练实例使用相同的mask，训练数据被复制了10次，以便在40个epoch中，每个序列以10种不同的方式被mask训练。因此，在训练过程中，每个训练序列都被用同一个mask观看四次。"></a>○ BERT依赖于随机mask和预测标记。原始的<strong>BERT实现在数据预处理期间执行一次mask，从而产生一个静态mask</strong>。为了避免对每个epoch中的每个训练实例使用相同的mask，训练数据被复制了10次，以便在40个epoch中，每个序列以10种不同的方式被mask训练。因此，在训练过程中，每个训练序列都被用同一个mask观看四次。</h6><p>我们将这种策略与动态mask进行比较，在<strong>动态mask</strong>中，我们每次向模型提供一个序列时都会生成mask模式。当进行更多步骤的预训练或使用更大的数据集时，这一点变得至关重要。</p>
<h6 id="○-动态mask：对每个序列进行mask的操作是在喂给模型该序列时执行的。这在预训练更多步骤或更大数据集时，至关重要。"><a href="#○-动态mask：对每个序列进行mask的操作是在喂给模型该序列时执行的。这在预训练更多步骤或更大数据集时，至关重要。" class="headerlink" title="○ 动态mask：对每个序列进行mask的操作是在喂给模型该序列时执行的。这在预训练更多步骤或更大数据集时，至关重要。"></a>○ 动态mask：对每个序列进行mask的操作是在喂给模型该序列时执行的。这在预训练更多步骤或更大数据集时，至关重要。</h6><p><img src="https://upload-images.jianshu.io/upload_images/6909373-12d552f1e6d4b3f0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/614" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>image.png</p>
<h6 id="实验结果："><a href="#实验结果：" class="headerlink" title="实验结果："></a>实验结果：</h6><p>表1发现：动态mask略比静态mask好，因此作者后面的实验均采用动态mask方式。</p>
<h5 id="4-2-模型输入格式和下一个句子预测："><a href="#4-2-模型输入格式和下一个句子预测：" class="headerlink" title="4.2 模型输入格式和下一个句子预测："></a>4.2 模型输入格式和下一个句子预测：</h5><p>Model Input Format and Next Sentence Prediction</p>
<p>在原始的BERT预训练过程中，模型观察到两个串联的文档段，它们要么是连续采样的来自同一文档（p&#x3D;0.5）或不同文档。除了MLM 建模目标，该模型还通过辅助下一句预测（NSP）来预测观察到的文档片段是否来自相同或不同的文档。</p>
<p>假设NSP loss是训练原始BERT模型的一个重要因素。Devlin等人观察到，移除NSP会损害性能，QNLI、MNLI和SQuAD1.1的性能显著下降。然而，最近的一些研究质疑了NSP loss 的必要性。为了更好地理解这种差异，我们比较了几种不同的训练形式：</p>
<ul>
<li>**SEGMENT-PAIR+NSP:**这遵循了BERT中使用的原始输入格式，带有 NSP<br>loss。每个输入都有一对段，每个段可以包含多个自然句子，但总的组合长度必须小于512个tokens。</li>
<li><strong>SENTENCE-PAIR+NSP</strong>：每个输入都包含一对自然句子，可以从一个文档的连续部分采样，也可以从另一个文档中采样单独的文件。由于这些输入明显短于512个tokens，因此我们增加了批处理大小 batch size ，以使tokens的总数与段对+NSP类似。我们保留NSP的loss。</li>
<li><strong>FULL-SENTENCES</strong>：每个输入都包含从一个或多个文档中连续采样的完整句子，因此总长度为最多512个tokens。输入可能会跨越文档边界。当我们到达一个文档的末尾时，我们开始从下一个文档中抽取句子，并在文档之间添加一个额外的分隔符标记。我们移除了NSP loss。</li>
<li><strong>DOC-SENTENCES</strong>：输入的结构类似于FULL-SENTENCES，只是它们不能跨越文档边界。在文档末尾附近采样的输入可能短于512个tokens，因此在这些情况下，我们会动态增加批处理大小，以获得与FULLSENTENCES相似的总tokens数，我们移除了NSP loss。</li>
</ul>
<p><img src="https://upload-images.jianshu.io/upload_images/6909373-3017ac91aba3d1f0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>image.png</p>
<h5 id="实验结果：-1"><a href="#实验结果：-1" class="headerlink" title="实验结果："></a>实验结果：</h5><ul>
<li>（1）作者发现使用单个句子会损害下游任务的性能，作者推测这是因为该模型无法学习远程依赖关系。</li>
<li>（2）去掉NSP任务会有略微的提升。</li>
<li>（3）DOC-SENTENCES比FULL-SENTENCES表现好。</li>
</ul>
<p>然而，由于DOC-sequences格式会导致不同的批量大小，我们在剩下的实验中使用FULL-SENTENCES，以便与相关工作进行比较。</p>
<h5 id="4-3-大批量训练："><a href="#4-3-大批量训练：" class="headerlink" title="4.3 大批量训练："></a>4.3 大批量训练：</h5><p>过去的工作在神经机器翻译已经表明，训练非常大的小批量可以同时提高优化速度和结束任务性能时，学习率适当增加。最近的研究表明，BERT很适合较大规模的batch训练。</p>
<p>Devlin等人最初对BERT-base进行了1M步的训练，批量大小为256个序列。这相当于计算成本，通过梯度累积，训练125K步，批量大小为2K序列，或批量为8K的31K步骤。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/6909373-231d06d00257283b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/624" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>image.png</p>
<p>在表3中，我们比较了BERT-base在增加batch size时的困惑度和最终任务性能，控制了通过训练数据的次数。我们观察到，大批量的训练提高了MLM模型目标的复杂性，以及最终任务的准确性。通过分布式数据并行训练，大批量也更容易并行化，在以后的实验中，我们使用8K序列进行批量训练。</p>
<p>值得注意的是，You等人用更大的批量训练BERT，最多32K序列。我们将进一步探索大批量训练的局限性留给未来的工作。</p>
<h6 id="总结：通过实验发现，增大batch-size能够进一步提升模型的性能，且更大的batch-size更易于分布式数据的并行训练。"><a href="#总结：通过实验发现，增大batch-size能够进一步提升模型的性能，且更大的batch-size更易于分布式数据的并行训练。" class="headerlink" title="总结：通过实验发现，增大batch size能够进一步提升模型的性能，且更大的batch size更易于分布式数据的并行训练。"></a>总结：通过实验发现，增大batch size能够进一步提升模型的性能，且更大的batch size更易于分布式数据的并行训练。</h6><h5 id="4-4-文本编码-Text-Encoding："><a href="#4-4-文本编码-Text-Encoding：" class="headerlink" title="4.4 文本编码 - Text Encoding："></a>4.4 文本编码 - Text Encoding：</h5><h6 id="○-字节对编码Byte-Pair-Encoding-BPE"><a href="#○-字节对编码Byte-Pair-Encoding-BPE" class="headerlink" title="○ 字节对编码Byte-Pair Encoding (BPE) :"></a>○ 字节对编码Byte-Pair Encoding (BPE) :</h6><p>是字符级和单词级表示的混合，允许处理自然语言语料库中常见的大型词汇。BPE依赖于子词单元，而不是完整的词，子词单元是通过对训练语料库进行统计分析来提取的。BPE词汇表的大小通常在10K-100K子单词单位之间。然而，unicode字符在其中占了相当大的一部分在建模大型和多样的语料库时使用的词汇，例如本工作中考虑的语料库。Radford et al介绍了一种巧妙的BPE实现，它使用字节而不是unicode字符作为基本子字单元。使用字节可以学习中等大小（50K个单位）的子单词词汇表，它仍然可以对任何输入文本进行编码，而不引入任何“未知”标记。</p>
<p>最初的BERT实现使用字符级别大小为30K的BPE词汇表，在使用启发式标记化规则对输入进行预处理后学习。在Radford等人之后，我们考虑用<strong>更大的字节级的BPE词汇</strong>来训练BERT，其中包含50K的子字单位，而不需要任何额外的预处理或令牌化。这分别为BERT - base和Bert - lagle增加了约15M和20M的额外参数。</p>
<h5 id="文本编码总结以下几点："><a href="#文本编码总结以下几点：" class="headerlink" title="文本编码总结以下几点："></a>文本编码总结以下几点：</h5><ul>
<li>字节对编码（BPE）是字符级和单词级表示形式的混合体，可以处理自然语言语料库中常见的大词汇。</li>
<li>Radford在GPT2里提出了一种更巧妙的BPE实现版本byte-level text encoding，该方法使用bytes作为基础的子词单元，这样便把词汇表的大小控制到了5w。它可以在不需要引入任何未知字符前提下对任意文本进行编码。</li>
<li>BERT原始版本使用字符级（character-level）的BPE词汇表，大小是3w，是用启发式分词规则对输入进行预处理学习得到的。</li>
<li>之前的一些实验结果表明，这两种文本编码的实验性能区别不大，可能Radford BPE Encoding在某些任务上的终端性能略微差点，但是RoBerta作者坚信通用的编码模式比性能上的轻微损失更重要，所以在实验中采用了byte-level text encoding。</li>
</ul>
<h4 id="5、RoBERTa："><a href="#5、RoBERTa：" class="headerlink" title="5、RoBERTa："></a>5、RoBERTa：</h4><ul>
<li>结合之前章节提出的改进配置，作者将这些配置结合起来，改进后的模型成为<strong>RoBERTa</strong>。</li>
</ul>
<h6 id="○-具体来说，RoBERTa使用："><a href="#○-具体来说，RoBERTa使用：" class="headerlink" title="○ 具体来说，RoBERTa使用："></a>○ 具体来说，RoBERTa使用：</h6><ul>
<li>（1）dynamic mask。</li>
<li>（2）FULL-SENTENCES without NSP。</li>
<li>（3）large mini-batches。</li>
<li>（4）byte-level BPE等设置。</li>
</ul>
<p>此外，我们还调查了在之前的工作中<strong>被低估的另外两个重要因素</strong>：</p>
<ul>
<li>（1）用于预训练的数据。</li>
<li>（2）通过数据的训练次数。</li>
</ul>
<p>例如，最近提出的XLNet架构使用比原始BERT多近10倍的数据进行预训练。它还以8倍于优化步骤一半的批量进行训练，因此在预训练中看到的序列数量是BERT的4倍。</p>
<p>为了将这些因素的重要性与其他建模选择区分开来，从训练RoBERTa开始遵循大型架构（L&#x3D;24，H&#x3D;1024，A&#x3D;16，355M参数）。在Devlin等人使用的可比图书语料库和维基百科数据集上预训练了10万步。使用1024个V100 GPU对模型进行了大约一天的预训练。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/6909373-f15267090405f251.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>image.png</p>
<h5 id="实验结果：-2"><a href="#实验结果：-2" class="headerlink" title="实验结果："></a>实验结果：</h5><ul>
<li>在控制训练数据时，观察到<strong>RoBERTa比最初报告的BERT-large结果有了很大的改进</strong>。</li>
<li>接下来，将这些数据与第3.2节中描述的三个附加数据集结合起来。与之前相同的训练步骤数（100K）。总共预训练了超过160GB的文本。观察到，整个计算性能进一步改善所有下游任务，<strong>证明了在预训练过程中，数据集规模和多样性的重要性</strong>。</li>
<li>最后，我们<strong>对RoBERTa进行了更长时间的预训练</strong>，将预训练步骤的数量从100K增加到300K，然后进一步增加到500K。我们再次观察到<strong>下游任务性能显著提高</strong>，在大多数任务中，300K和500K步长模型的性能优于XLNet-large。我们注意到，即使是我们训练时间最长的模型似乎也不会过度拟合我们的数据，并且可能会从额外的训练中受益。</li>
<li>作者在GLUE，SQuaD和RACE这三个不同的基准上评估了RoBERTa模型。具体来说，考虑RoBERTa在前面介绍的所有五个数据集上训练超过500 K步。</li>
</ul>
<h5 id="5-1-GLUE-结果："><a href="#5-1-GLUE-结果：" class="headerlink" title="5.1 GLUE 结果："></a>5.1 GLUE 结果：</h5><p><img src="https://upload-images.jianshu.io/upload_images/6909373-3065544bdeb07b78.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>image.png</p>
<h5 id="对于GLUE，我们考虑两个定影设置："><a href="#对于GLUE，我们考虑两个定影设置：" class="headerlink" title="对于GLUE，我们考虑两个定影设置："></a>对于GLUE，我们考虑两个定影设置：</h5><ul>
<li>在第一个设置（single task，dev）中，我们分别为每个GLUE任务微调RoBERTa，只使用相应任务的训练数据。我们考虑每个任务的有限超参数扫描，具有批量大小∈ {16, 32}以及学习率∈ {1e−5，2e−5，3e−5} ，在前6%的步骤中进行线性预热，然后线性衰减到0。我们为10个epochs和根据dev集合上每个任务的评估指标提前停止。其余超参数与训练前相同。在此设置中，我们报告了在五次随机初始化过程中每个任务的开发集结果中值，无需进行模型置乱。</li>
<li>在第二个场景（合奏、测试）中，我们通过GLUE排行榜将RoBERTa与测试集上的其他方法进行比较。虽然许多对GLUE排行榜的提交依赖于多任务微调，但我们的提交仅依赖于单任务微调。对于RTE、STS和MRPC我们发现，从MNLI单任务模型开始，而不是从基线预训练的RoBERTa开始，进行微调是有帮助的。我们探索了一个更广阔的领域附录中描述的超参数空间，以及每个任务5到7个模型之间的集成。</li>
</ul>
<h5 id="○-特定于任务的改进："><a href="#○-特定于任务的改进：" class="headerlink" title="○ 特定于任务的改进："></a>○ 特定于任务的改进：</h5><p>GLUE任务中的两个需要特定于任务的微调方法，以实现有竞争力的排行榜结果。</p>
<ul>
<li><p><strong>（1）QNLI</strong>：最近在GLUE排行榜上提交的报告采用了QNLI任务的两两排名公式，从训练集中挖掘候选答案并相互比较，以及单个（问题，候选）配对被归类为正例。<br>这个公式大大简化了任务，但不能直接进行比较致BERT。在最近的工作之后，我们在提交测试时采用了排名方法，但用于与BERT进行直接比较我们报告基于纯分类方法的开发集结果。</p>
</li>
<li><p><strong>WNLI</strong>：<br>我们发现提供的NLI格式数据处理起来很有挑战性。相反我们使用SuperGLUE重新格式化的WNLI数据，这表明查询代词和指代词的span。我们利用margin ranking loss微调。<br>对于给定的输入句子，使用spaCy来从句子中提取额外的候选名词短语，并对模型进行微调，使其对正例的指称短语的评分高于任何负例候选短语。但是缺点是，我们只能使用正例的训练样本，而这排除了提供的一半以上的训练样本。</p>
</li>
<li><p><strong>结果</strong>：</p>
</li>
<li><p>第一个设置（单任务，dev），RoBERTa实现所有9种GLUE的最新结果任务开发集。重要的是，RoBERTa使用与Bert-large一样的MLM预训练目标和架构，然而，它的表现始终优于XLNet - large。与我们在这项工作中探索的数据集大小和训练时间等更普通的细节相比，这就提出了关于模型体系结构和训练前目标的相对重要性的问题。</p>
</li>
<li><p>在第二个场景（合奏、测试）中，将RoBERTa提交给GLUE排行榜，并在9项任务中的4项任务中取得SOTA。这尤其令人兴奋，因为与其他大多数任务不同，RoBERTa不依赖于多任务微调。预计，未来的工作可能会通过结合更复杂的多任务微调程序进一步改善这些结果。</p>
</li>
</ul>
<h5 id="5-2-SQuAD-结果："><a href="#5-2-SQuAD-结果：" class="headerlink" title="5.2 SQuAD 结果："></a>5.2 SQuAD 结果：</h5><p>与过去的工作相比，我们对SQuAD采取了更简单的方法。特别是，虽然BERT和XLNet都使用额外的QA数据集来增加他们的训练数据，但我们仅使用提供的SQuAD训练数据对RoBERTa进行微调。Yang等人还采用了一个定制的分层学习速率计划layer-wise learning rate schedule来进行微调。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/6909373-d941cd4d51fc3afa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/614" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>image.png</p>
<p>对于SQuAD v1.1我们遵循与Devlin等人相同的微调程序。此外SQuAD v2.0，我们还对给定的这个问题是可以回答的；我们通过将分类项和span loss项相加，将该分类器与span 预测器联合训练。</p>
<h5 id="结果："><a href="#结果：" class="headerlink" title="结果："></a>结果：</h5><p>结果我们在表6中给出了我们的结果。SQuAD v1.1开发集，RoBERTa与XLNet最先进的设置相匹配（<strong>SOTA：state-of-the-art</strong>）。</p>
<p>在SQuAD v2.0开发集，RoBERTa设置了一个新的SOTA，提高了0.4个点（EM）和0.6个点（F1）。我们还将RoBERTa提交公SQuA D2.0排行榜，并评估其相对于其他系统的性能。</p>
<p>大多数顶级系统建立在BERT或XLNet。两者都依赖于额外的外部训练数据。相比之下，我们提交的工作没有使用任何其他数据。单一RoBERTa模型优于其他所有单一模型，是不依赖数据扩充的模型中得分最高的。</p>
<h5 id="5-3-RACE-结果："><a href="#5-3-RACE-结果：" class="headerlink" title="5.3 RACE 结果："></a>5.3 RACE 结果：</h5><p>在RACE中，系统提供一条通道文本、一个相关问题和四个候选答案。系统需要对以下哪项进行分类这四个候选答案是正确的。我们通过将每个候选答案与相应的问题和段落连接起来来改进RoBERTa的这项任务。然后我们对每个这四个序列并将[CLS]经过全连接层进行表示，用于预测正确答案。截断长度大于问题-答案对的128个tokens，甚至还包括通道，以便总长度最多为512个tokens。</p>
<ul>
<li>RACE测试集的结果如所示表7。RoBERTa均取得SOTA。</li>
</ul>
<p><img src="https://upload-images.jianshu.io/upload_images/6909373-879d3acbaa1f6b23.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/592" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>image.png</p>
<h4 id="6、相关工作："><a href="#6、相关工作：" class="headerlink" title="6、相关工作："></a>6、相关工作：</h4><p>最近的许多论文都为每个最终任务使用了微调模型的基本方式，并使用某种不同的MLM模型目标进行预训练。然而，较新的方法通过多任务微调、实体嵌入、span预测和自回归预训练的多种变体提高了性能。通过在更多数据上训练更大的模型，性能通常也会得到改善。我们的目标是复制、简化和更好地调整BERT的训练，作为更好地理解所有这些方法的相对性能的参考点。</p>
<h4 id="7、-结论："><a href="#7、-结论：" class="headerlink" title="7、 结论："></a>7、 结论：</h4><p>在预训练BERT模型时，我们仔细评估了一些设计策略。发现，通过对模型进行更长时间的训练，对更多数据进行更大批量的训练，可以显著提高性能；删除下一句预测目标NSP；长序列训练；以及动态地改变应用于训练数据的mask模式。这些改进称之为<strong>RoBERTa</strong>，在GLUE, RACE 和 SQuAD方面取得了SOTA，而不需要对GLUE进行多任务微调，也不需要为SQuAD提供额外的数据。</p>
<p>这些结果说明了这些之前被忽视的设计决策的重要性，并表明BERT的预训练目标与最近提出的备选方案相比仍然具有竞争力。此外，我们还使用了一个新的数据集CC-NEWS，并在以下网址发布了预训练和微调的模型和代码：<a target="_blank" rel="noopener" href="https://links.jianshu.com/go?to=https://github.com/pytorch/fairseq">https://github.com/pytorch/fairseq</a></p>
<h2 id="XLNet"><a href="#XLNet" class="headerlink" title="XLNet"></a>XLNet</h2><p><strong>XLNet论文推荐语：</strong></p>
<p>BERT本身很有效，但它本身也存在一些问题，比如不能用于生成、以及训练数据和测试数据的不一致(Discrepancy)。在本文中，我们重点介绍比BERT更强大的预训练模型-XLNet，它为了达到真正的双向学习，采用了Permutation语言模型、以及使用了双流自注意力机制，并结合了Transformer-XL的相对位置编码。</p>
<p><strong>XLNet的论文：</strong></p>
<p>【1】Yang Z, Dai Z, Yang Y, et al. Xlnet: Generalized autoregressive pretraining for language understanding[C]&#x2F;&#x2F;Advances in neural information processing systems. 2019: 5754-5764.</p>
<p><strong>目录：</strong></p>
<p><img src="https://pic1.zhimg.com/80/v2-86ade3ac6c46e5137feeed1607ebfa74_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<h3 id="1-Unsupervised-Pre-training"><a href="#1-Unsupervised-Pre-training" class="headerlink" title="1. Unsupervised Pre-training"></a><strong>1. Unsupervised Pre-training</strong></h3><p>目前神经网络在进行训练的时候基本都是基于后向传播（Back Propagation，BP）算法，通过对网络模型参数进行随机初始化，然后利用优化算法优化模型参数。但是在标注数据很少的情况下，通过神经网络训练出的模型往往精度有限，“预训练”则能够很好地解决这个问题，并且对一词多义进行建模。</p>
<p>预训练是通过大量无标注的语言文本进行语言模型的训练，得到一套模型参数，利用这套参数对模型进行初始化，再根据具体任务在现有语言模型的基础上进行精调。预训练的方法在自然语言处理的分类和标记任务中，都被证明拥有更好的效果。目前，热门的预训练方法主要有：ELMo、OpenAI GPT、BERT和XLNet等。</p>
<p><img src="https://pic4.zhimg.com/80/v2-7b3ac469170ba6ca12680c9d81a6a81b_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>在NLP中，早期的无监督预训练模型主要是word2vec（SkipGram、CBOW）和Glove，这些模型都使用了不考虑上下文嵌入的方式得到词向量。</p>
<p><img src="https://pic4.zhimg.com/80/v2-8246dd2fdf8254fddb2701a548938693_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>在2018年初，艾伦人工智能研究所和华盛顿大学的研究人员在题为《Deep contextualized word representations》一文中提出了ELMo。相较于传统的使用词嵌入（Word embedding）对词语进行表示，得到每个词唯一固定的词向量，ELMo 利用预训练好的双向语言模型，根据具体输入从该语言模型中可以得到在文本中该词语的表示。在进行有监督的 NLP 任务时，可以将 ELMo 直接当做特征拼接到具体任务模型的词向量输入或者是模型的最高层表示上。</p>
<p>在ELMo的基础之上，OpenAI的研究人员在《Improving Language Understanding by Generative Pre-Training》提出了OpenAI GPT。与ELMo为每一个词语提供一个显式的词向量不同，OpenAI GPT能够学习一个通用的表示，使其能够在大量任务上进行应用。在处理具体任务时，OpenAI GPT 不需要再重新对任务构建新的模型结构，而是直接在 Transformer 这个语言模型上的最后一层接上 softmax 作为任务输出层，再对这整个模型进行微调。</p>
<p>ELMo和OpenAI GPT这两种预训练语言表示方法都是使用单向的语言模型来学习语言表示，而Google提出的BERT则实现了双向学习，并得到了更好的训练效果。具体而言，BERT使用Transformer的编码器作为语言模型，并在语言模型训练时提出了两个新的目标：MLM（Masked Language Model）和句子预测。MLM是指在输入的词序列中，随机的遮挡上 15% 的词，并对遮挡部分的词语进行双向预测。为了让模型能够学习到句子间关系，研究人员提出了让模型对即将出现的句子进行预测：对连续句子的正误进行二元分类，再对其取和求似然。</p>
<p><img src="https://pic1.zhimg.com/80/v2-a9b56df57f141eff9b2e0c4ae21e7b00_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>自从ELMo、GPT和BERT出现之后，pretrain+finetune的两段式训练方法，成为NLP任务的主流做法。在公开的语料库上对大模型进行自监督或者无监督的预训练，然后在特定任务的语料库上对模型做微调。本文介绍另外一篇类似的算法XLNet。</p>
<p><strong>相关论文：</strong></p>
<p>【1】Peters M E, Neumann M, Iyyer M, et al. Deep contextualized word representations[J]. arXiv preprint arXiv:1802.05365, 2018.</p>
<p>【2】Radford A, Narasimhan K, Salimans T, et al. Improving language understanding by generative pre-training[J]. URL <a href="https://link.zhihu.com/?target=https://s3-us-west-2">https://s3-us-west-2</a>. amazonaws. com&#x2F;openai-assets&#x2F;researchcovers&#x2F;languageunsupervised&#x2F;language understanding paper. pdf, 2018.</p>
<p>【3】Devlin J, Chang M W, Lee K, et al. Bert: Pre-training of deep bidirectional transformers for language understanding[J]. arXiv preprint arXiv:1810.04805, 2018.</p>
<h3 id="2-AR与AE语言模型"><a href="#2-AR与AE语言模型" class="headerlink" title="2. AR与AE语言模型"></a><strong>2. AR与AE语言模型</strong></h3><ul>
<li>AR：Autoregressive Language Modeling</li>
<li>AE： Autoencoding Language Modeling</li>
</ul>
<p><img src="https://pic4.zhimg.com/80/v2-50404f055cb76d165a215f5a6905a8c7_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>AR语言模型：指的是，依据前面（或后面）出现的tokens来预测当前时刻的token，代表有 ELMO， GPT等。 </p>
<p>forward:p(x)&#x3D;∏t&#x3D;1Tp(xt|x&lt;t)backward:p(x)&#x3D;∏t&#x3D;T1p(xt|x&gt;t)</p>
<p>AE语言模型：通过<strong>上下文信息</strong>来预测被mask的token，代表有 BERT , Word2Vec(CBOW) 。</p>
<p>p(x)&#x3D;∏x∈Maskp(x|context)</p>
<p><strong>二者有着它们各自的优缺点：</strong></p>
<p><strong>AR 语言模型：</strong></p>
<ul>
<li><strong>缺点：</strong>它只能利用单向语义而<strong>不能同时利用上下文信息</strong>。 ELMO 通过双向都做AR 模型，然后进行拼接，但从结果来看，效果并不是太好。</li>
<li><strong>优点：</strong> 对生成模型友好，天然符合生成式任务的生成过程。这也是为什么 GPT 能够编故事的原因。</li>
</ul>
<p><strong>AE 语言模型：</strong></p>
<ul>
<li><strong>缺点：</strong> 由于训练中采用了 [MASK] 标记，导致<strong>预训练与微调阶段不一致的问题</strong>。BERT独立性假设问题，即没有对<strong>被遮掩（Mask）的 token 之间的关系</strong>进行学习。 此外对于生成式问题， AE 模型也显得捉襟见肘。</li>
<li><strong>优点：</strong> 能够很好的编码上下文语义信息（即考虑句子的双向信息）， 在自然语言理解相关的下游任务上表现突出。</li>
</ul>
<p>所以，AR方式所带来的自回归性学习了预测 token 之间的依赖，这是 BERT 所没有的；而 BERT 的AE方式带来的对深层次双向信息的学习，却又是像ELMo还有GPT单向语言模型所没有的，不管是有没有替换 “[MASK]”。于是，自然就会想，<strong>如何将两者的优点统一起来</strong>？这时就到了XLNet登场的时间。</p>
<h3 id="3-XLNet提出的方法"><a href="#3-XLNet提出的方法" class="headerlink" title="3. XLNet提出的方法"></a><strong>3. XLNet提出的方法</strong></h3><p><strong>3.1 Permutation Language Model</strong></p>
<p>作者发现，只要在 AR中再加入一个步骤，就能够完美地将AR与AE的优点统一起来，那就是提出<strong>Permutation Language Model</strong>（PLM）。</p>
<p><img src="https://pic2.zhimg.com/v2-894de1f6545c5f98b44f0913e1001ed9_b.webp" srcset="/blog/img/loading.gif" lazyload alt="动图"></p>
<p>具体实现方式是，通过随机取一句话的一种排列，然后将末尾一定量的词给“遮掩”（和 BERT 里的直接替换 “[MASK]” 有些不同）掉，最后用 AR 的方式来按照这种排列依次预测被“遮掩”掉的词。</p>
<p><img src="https://pic4.zhimg.com/80/v2-8ee6464348307333a9d14bb37a3ad073_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>我们可以发现通过随机取排列（Permutation）中的一种，就能非常巧妙地通过 <strong>AR 的单向方式来习得双向信息</strong>了。</p>
<p>论文中 Permutation 具体的实现方式是通过直接对 Transformer 的 <strong>Attention Mask</strong> 进行操作。</p>
<p><img src="https://pic1.zhimg.com/80/v2-30f1a5e0ecdc4d6631d7dd34bc87f504_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>比如说序号依次为 1234 的句子，先随机取一种排列3241。于是根据这个排列我们就做出类似上图的 Attention Mask。先看第1行，因为在新的排列方式中 1 在最后一个，根据从左到右 AR 方式，1 就能看到 234 全部，于是第一行的 234 位置是红色的（没有遮盖掉，会用到），以此类推。第2行，因为 2 在新排列是第二个，只能看到 3，于是 3 位置是红色。第 3 行，因为 3 在第一个，看不到其他位置，所以全部遮盖掉…</p>
<p>这就是这篇论文的核心思想，接下来会介绍，XLNet 对 PLM 理念的实现。</p>
<p><strong>3.2 Two-Stream Self-Attention</strong></p>
<p>为了实现 Permutation 加上 AR 预测过程，首先我们会发现，打乱顺序后位置信息非常重要，同时对每个位置来说，需要预测的是内容信息（对应位置的词），于是输入就不能包含内容信息，不然模型学不到东西，只需要直接从输入复制到输出就好了。</p>
<p>于是这里就造成了位置信息与内容信息的割裂，因此在 BERT 这样的位置信息加内容信息输入 Self-Attention (自注意力) 的流（Stream）之外，作者还增加了另一个<strong>只有位置信息作为 Self-Attention 中 query 输入的流</strong>。文中将前者称为 <strong>Content Stream</strong>，而后者称为 <strong>Query Stream</strong>。</p>
<p>这样就能利用 Query Stream 在对需要预测位置进行预测的同时，又不会泄露当前位置的内容信息。具体操作就是用两组隐状态（hidden states） g 和 h 。其中 g 只有位置信息，作为 Self-Attention 里的 Q。 h 包含内容信息，则作为 K 和 V。具体表示如下图所示：</p>
<p><img src="https://pic1.zhimg.com/80/v2-1054aa153cabdabd0361f069c76c4ddc_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>图：Query stream attention</p>
<p>假如，模型只有一层的话，其实这样只有 Query Stream 就已经够了。但如果将层数加上去的话，为了取得更高层的 h，于是就需要 Content Stream 了。h 同时作为 Q K V。如下图所示：</p>
<p><img src="https://pic1.zhimg.com/80/v2-4844c76e051e3b7e00e9e603a8ce1d80_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>图：Content stream attention</p>
<p>于是组合起来就是这样：</p>
<p><img src="https://pic3.zhimg.com/80/v2-8fdfceb19a40770998362d37ac3db12e_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>图：基于双流注意力机制的排序语言模型</p>
<p>上图中我们需要理解两点：</p>
<ul>
<li>第一点，最下面一层蓝色的 Content Stream 的输入是 e(xi) ，这个很好懂就是 x 对应的词向量 (Embedding)，不同词对应不同向量，但看旁边绿色的 Query Stream，就会觉得很奇怪，为什么都是一样的 w ？这个和Relative Positional Encoding 有关。</li>
<li>第二点，Query stream attention图中为了便于说明，只将当前位置之外的 h 作为 K 和 V，但实际上实现中应该是所有时序上的 h 都作为 K 和 V，最后再交给上图中的 Query stream 的 Attention Mask 来完成位置的遮盖。</li>
</ul>
<p><strong>3.3 Partial Prediction</strong></p>
<p>XLNet还使用了部分预测（Partial Prediction）的方法。因为LM是从第一个Token预测到最后一个Token，在预测的起始阶段，上文信息很少而不足以支持Token的预测，这样可能会对分布产生误导，从而使得模型收敛变慢。为此，XLNet只预测后面一部分的Token，而把前面的所有Token都当作上下文。具体来说，对长度为 T 的句子，我们选取一个超参数 K ，使得后面 1&#x2F;K的Token用来预测，前面的 1−1&#x2F;K 的Token用作上下文。注意， K 越大，上下文越多，模型预测得就越精确。</p>
<p>例如[1→2→3→4]只预测3和4，把1和2当作上下文信息。</p>
<p><strong>3.4 Transformer-XL</strong></p>
<p>为什么会提出Transformer-XL呢？它的提出主要是为了解决transformer的问题。我们首先分析一下RNN以及Transformer的优缺点。</p>
<p><strong>RNN</strong></p>
<ul>
<li><ul>
<li>优点：支持可变长，支持记忆，有序列顺序关系。</li>
<li>缺点：gradient vanish，耗时无法并行。</li>
</ul>
</li>
</ul>
<p><strong>Transformer</strong></p>
<ul>
<li><ul>
<li>优点：并行，考虑到sequence的long term dependency信息（相对于RNN），可解释性。</li>
<li>缺点：句子与句子之间的关系，batch size也不能很大，空间占用大（因为每个encoder的score matrix（sequenceLen*sequecenLen是 N2 的空间复杂度） 如下图：</li>
</ul>
</li>
</ul>
<p><img src="https://pic3.zhimg.com/80/v2-d779c12078589163e50aaef788785496_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>Transformer编码固定长度的上下文，即将一个长的文本序列截断为几百个字符的固定长度片段(segment)，然后分别编码每个片段，片段之间没有任何的信息交互。比如BERT，序列长度的极限一般在512。因此Transformer-XL提出的动机总结如下：</p>
<ul>
<li>Transformer无法建模超过固定长度的依赖关系，对长文本编码效果差。</li>
<li>Transformer把要处理的文本分割成等长的片段，通常不考虑句子（语义）边界，导致**上下文碎片化(context fragmentation)**。通俗来讲，一个完整的句子在分割后，一半在前面的片段，一半在后面的片段。</li>
</ul>
<p>《Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context》围绕如何建模长距离依赖，提出Transformer-XL（XL是extra long的意思）：</p>
<ul>
<li>提出**片段级循环机制(segment-level recurrence mechanism)<strong>，引入一个</strong>记忆(memory)**模块（类似于cache或cell），循环用来建模片段之间的联系。这使得长距离依赖的建模成为可能；也使得片段之间产生交互，解决上下文碎片化问题。</li>
<li>提出**相对位置编码机制(relative position embedding scheme)**，代替绝对位置编码。在memory的循环计算过程中，避免时序混淆，位置编码可重用。</li>
</ul>
<p><strong>Transformer-XL总结</strong>：片段级循环机制为了解决编码长距离依赖和上下文碎片化，相对位置编码机制为了实现片段级循环机制而提出，解决可能出现的时序混淆问题。也可以简单的理解Transformer-XL&#x3D;Transformer + RNN，即segment-wise的RNN模型，但是RNN模型的组件是Transformer的Encoder模块。</p>
<p><strong>相关论文：</strong></p>
<p>【1】Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[C]&#x2F;&#x2F;Advances in neural information processing systems. 2017: 5998-6008.</p>
<p>【2】Dai Z, Yang Z, Yang Y, et al. Transformer-xl: Attentive language models beyond a fixed-length context[J]. arXiv preprint arXiv:1901.02860, 2019.</p>
<p><strong>3.5 Relative Segment Encodings</strong></p>
<p><strong>3.5.1 Absolute Positional Encoding</strong></p>
<p><img src="https://pic1.zhimg.com/80/v2-8803cce6abbc6db6d58c2e13e52a605c_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>Transformer使用的是绝对位置编码，如果我们继续使用absolute positing encoding的话，对于所有的sequence序列，只要这个字在序列中的位置一样的话，它的position encoding也会一样，这样的话，对于我们concat之后的输出，我们无法区别每个字的位置。如下图：<code>The</code>和<code>that</code>的position encoding完全一样，模型无法区分两者位置区别。</p>
<p><img src="https://pic3.zhimg.com/80/v2-879ae28ecba30ecb8daacdabf06f314e_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p><strong>3.5.2 Relative Position Encoding</strong></p>
<p>所以Transformer-XL 首先分析了position encoding在计算中的作用，然后根据这个结果将交互项转化为relative position encoding。</p>
<p><img src="https://pic3.zhimg.com/80/v2-c0b1616b7a12c64e73e8d1164f96a48a_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>The notation (i,•) refers to the entire row i and (•,j) to the entire column j . 经过计算，这个式子可以分为4项。</p>
<ul>
<li>a) 这一项中没有包含 P 位置信息，代表的是在第 i 行的字应该对第 j 列的字提供多大的注意力。这是不管他们两个字的位置信息的。</li>
<li>b) 这一项捕获的是模型的global attention，指的是一个字在position i应该要对 position j 付出多大的注意力。例如两个字的位置越远，期望它们之间的注意力越小。</li>
<li>c) 这一项捕获的是在row i的字对其他位置的关注信息，例如在position i是一个字”狗”， 应该要对j&#x3D;i-1 这个位置特别注意，否则可能出现j&#x3D;i-1是“热”， 出现是“热狗”的情况。</li>
<li>d) 这个是c) 的逆向表示，指的是j的字要pay attention to 位置i的字。</li>
</ul>
<p>根据这个观测，转化relative position 通过了解了每一项的意义，我们了解了两个字的相对位置对这个score的作用。我们将 b), c) and d) 替换为如下式子。</p>
<p><img src="https://pic3.zhimg.com/80/v2-18eb2b6e46958e6b9131dd427aa2c806_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>我们可以看到主要的变化</p>
<ul>
<li>我们将使用的是相对的position encoding i.e. 取消 P_{•, j} P•,j 而采用 P•,i—j 相对位置。</li>
<li>每次使用 P•,i—j 我们都将 WK 替换为 WR (两者的形状相同)。这是为了区别 WK （仍使用） 和 WR ，使得两者可以各自捕获有意义的位置信息而不会相互干预，因为 WR 和 P•,i—j 相匹配，而 WK 和 E•,jT 像对于。</li>
<li>Pi,•WQ 这一项被替代为 u u 和 v ，这两个向量的维度为 (1, d_k)。因为我们使用的是相对位置编码，所以我们并不需要提供绝对位置 i ，所以我们可以直接把整项替换掉。这边使用两个向量的原因是因为一项是更换了相对位置(b)，一项没有(d)，所以这样能够focus on the general position and the position given the word we attend to as its the case of u and v respectively.（这边没有非常理解）</li>
</ul>
<p>所以 (QKT)i,j <em>的公式被替换为：</em>(QKT)i,j&#x3D;Ei,•WQ(WK)TE•,jT+u(WR)TP•,i−jT+Ei,•WQ(WR)TP•,i−jT+v(WK)TE•,jT</p>
<p><strong>3.5.3 Relative Segment Encodings</strong></p>
<p>为了通过输入形式 <strong>[A, SEP, B, SEP, CLS]</strong> 来处理句子对任务，于是需要加入标识 A 句和 B 句的段信息。BERT 里面很简单，直接准备两个向量，一个加到 A 句上，一个加到 B 句上。</p>
<p>但当这个遇上 Segment Recurrence Mechanism 时，和位置向量一样，也出问题了。万一出现了明明不是一句，但是相同了怎么办，于是我们就需要最后一块补丁，同样准备两个向量，<strong>s+</strong> 和 <strong>s-</strong> 分别表示在一句话内和不在一句话内。</p>
<p>具体实现是在计算 attention 的时候加入一项：</p>
<p><img src="https://pic2.zhimg.com/80/v2-3043cfc6596471e912e5c550bef9c50d_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>当 i 和 j 位置在同一段里就用 s+，反之用 s-，在 attention 计算权重的时候加入额外项。</p>
<h3 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h3><p>从XLNet论文的结果来看，其在问答、文本分类、自然语言理解等任务上大幅超越BERT。除了相比BERT增加了训练集之外，XLNet也在模型设计上有较大的改进，比如引入了新的优化目标Permutation Language Modeling（PLM），使用了双流自注意力机制（Two-Stream Self Attention, TSSA）和与之匹配的Mask技巧。此外，XLNet还使用了Transformer-XL作为Backbone，也使用了Transformer-XL的相对位置编码。所以，相比BERT，XLNet对长文本的支持更加有效。这些改进为BERT类预训练模型难以进行生成任务的问题提供了一个解决思路。可以期待，在不久的将来，NLP预训练模型能够突破一系列生成任务，实现NLP模型结构化的统一。</p>
<h2 id="Transformer-XL"><a href="#Transformer-XL" class="headerlink" title="Transformer-XL"></a>Transformer-XL</h2><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a><strong>前言</strong></h3><p>序列模型捕获数据长期依赖的能力在任何NLP任务中都是至关重要的，<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/42717426">LSTM</a>通过引进门机制将RNN的长期依赖的捕获能力提升到200个左右，<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/48508221">Transformer</a>的提出则进一步提升了获长期依赖的能力，但是Transformer的捕获长期依赖的能力是无限长的吗？如果有一个需要捕获几千个时间片的能力的模型才能完成的任务，Transformer能够胜任吗？答案从目前Transformer的设计来看，它还是做不到。</p>
<p>这篇文章介绍的Transformer-XL（extra long）则是为了进一步提升Transformer建模长期依赖的能力。它的核心算法包含两部分：片段递归机制（segment-level recurrence）和相对位置编码机制(relative positional encoding)。Transformer-XL带来的提升包括：1. 捕获长期依赖的能力；2. 解决了上下文碎片问题（context segmentation problem）；3. 提升模型的预测速度和准确率。</p>
<h3 id="1-Transformer回顾"><a href="#1-Transformer回顾" class="headerlink" title="1. Transformer回顾"></a><strong>1. Transformer回顾</strong></h3><p>关于Transformer的详细介绍可以参考论文或者我之前的文章<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/48508221">《详解Transformer（Attention is all you need）》</a>。Transformer-XL的提出当然是为了对传统的Transformer进行改进，在了解改进之前，我们得先看一下Transformer的缺点。</p>
<h4 id="1-1-输入"><a href="#1-1-输入" class="headerlink" title="1.1 输入"></a><strong>1.1 输入</strong></h4><p>NLP相关的任务都很难避免处理输入为变长数据的场景，这个问题的解决方案有两个，一是将数据输入到类似前馈神经网络这样的模型中得到长度固定的特征向量，这个方法往往因为计算资源的限制很难执行；另一个是通过数据切段或者padding的方式将数据填充到固定长度。Transfomer采取的便是第二个方案，这个值这里用 L 来表示， L 的值在Transformer的论文中为512。</p>
<p>将数据分完段之后，接下来便是将分段的数据依次喂到网络中进行模型的训练，如图（1）所示。</p>
<p><img src="https://pic1.zhimg.com/v2-ca380d3eb2f06a16bbf357dfb5c0afc0_b.jpg" srcset="/blog/img/loading.gif" lazyload alt="动图封面"></p>
<p>图1：Transformer的训练流程</p>
<p>这种分段式的提供数据的方式的一个很大的问题是数据并不会在段与段之间流通，因此模型能够捕获的长期依赖的上限便是段的长度。另外这种将数据分段，而不考虑段与段之间的关系无疑是非常粗暴的，对于模型的能力无疑是要打折的。这个问题便是我们所说的上下文碎片问题。</p>
<h4 id="1-2-Self-Attention"><a href="#1-2-Self-Attention" class="headerlink" title="1.2 Self-Attention"></a><strong>1.2 Self-Attention</strong></h4><p>这里以单头Transformer为例进行说明，对于一个长度为 n 的输入序列， x&#x3D;(x1,⋯,xn) ，通过Transformer得到的序列为 z&#x3D;(z1,⋯,zn) ， zi 的计算方式为 x 中各元素的加权和：</p>
<p>(1)zi&#x3D;∑j&#x3D;1nαi,j(xjWV)<br>权值 ai,j 是通过一个 softmax 运算得到：</p>
<p>(2)αi,j&#x3D;exp⁡ei,j∑k&#x3D;1nexp⁡ei,k</p>
<p>ei,j 的计算则是通过Q,K两个矩阵得到：</p>
<p>(3)ei,j&#x3D;(xiWQ)(xjWK)Tdk</p>
<p>其中 WQ,WK,WV 是三个权值矩阵。</p>
<h4 id="1-3-测试"><a href="#1-3-测试" class="headerlink" title="1.3 测试"></a><strong>1.3 测试</strong></h4><p>Transformer是一个自回归模型（auto-regressive），也就是说在测试时模型依次取时间片为 L的分段，然后将整个片段提供给模型后预测一个结果，如图2所示。在下个时间片时再将这个分段向右移一个单位，这个新的片段也将通过整个网络的计算后得到一个值。Transformer的这个特性导致其预测阶段的计算量是非常大的，这也限制了其应用领域。</p>
<p><img src="https://pic4.zhimg.com/v2-0062f7a1be5afd1b745731fa31833d63_b.jpg" srcset="/blog/img/loading.gif" lazyload alt="动图封面"></p>
<p>图2：Transformer的测试过程</p>
<h4 id="1-4-绝对位置编码"><a href="#1-4-绝对位置编码" class="headerlink" title="1.4 绝对位置编码"></a><strong>1.4 绝对位置编码</strong></h4><p>Tansformer的位置编码是以段为单位的，它使用的是无参数的sinusoid decoding matrix，表示为 U∈RLmax×d ，第 i 个元素 Ui 表示的是在这个分段中第 i 个元素的相对位置， Lmax 表示的是能编码的最大长度。然后这个位置编码会通过单位加的形式和词嵌入（word Embedding）合并成一个矩阵，表示为：</p>
<p>(4)hτ+1&#x3D;f(hτ,Esτ+1+U1:L)hτ&#x3D;f(hτ−1,Esτ+U1:L)</p>
<p>其中 Esτ∈RL×d 表示第 τ 个碎片 sτ 的词嵌入， f 表示转换方程。从(1)式中我们可以看出，对于第 τ 和第 τ+1 个片段来说，它们的时间位置编码是完全相同的，我们完全没法确认它属于哪个片段或者它在分段之前的输入数据中的相对位置。</p>
<p>在Transformer中，self-attention可以表示为:</p>
<p>(5)Attention(Q,K,V)&#x3D;softmax(QK⊤dk)V</p>
<p>考虑到词嵌入， QTK 的完整表达式为：</p>
<p>(6)Ai,jabs&#x3D;(Wq(Exi+Ui))⊤(Wk(Exj+Uj))</p>
<p>我们使用乘法分配律将其展开，展开式会在后面使用：</p>
<p>(7)Ai,jabs&#x3D;Exi⊤Wq⊤WkExj⏟(a)+Exi⊤Wq⊤WkUj⏟(b)+Ui⊤Wq⊤WkExj⏟(c)+Ui⊤Wq⊤WkUj⏟(d)</p>
<p>Transformer的问题是无论对于第几个片段，它们的位置编码 U1:L 都是一样的，也就是说Transformer的位置编码是相对于片段的绝对位置编码（absulate position encoding），与当前内容在原始句子中的相对位置是没有关系的。</p>
<h3 id="2-相对位置编码"><a href="#2-相对位置编码" class="headerlink" title="2. 相对位置编码"></a>2. 相对位置编码</h3><p>最先介绍相对位置编码的是论文《self-attention with relative positional representation》(后面简称RPR)。对比RNN系列的模型，Transformer的一个缺点是没有从网络结构上对位置信息进行处理，而只是把位置编码加入到了输入层。RPR的动机就是解决Transformer的这个天然缺陷，它的做法是把<strong>相对位置编码</strong>加入到了self-attention的<strong>内部</strong>。</p>
<p>例如图3的例子，输入序列为“I think therefore I am”，对RNN来说两个‘I’接收到的信息是不同的，第一个’I’接收的隐层状态是初始化的值，第二个’I’接收的隐层状态是经过’I think therefore’编码之后的。</p>
<p><img src="https://pic4.zhimg.com/80/v2-7814ee5f9833c232dc4a6fd9e9760cef_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>图3：RNN结构具有编码相对位置的能力</p>
<p>而对于Transormer来说，在没有位置编码的情况下，尽管两个‘I’在句子中的位置不同，但是两个‘I’的输入信息是完全一致的。正如我在分析Transformer的文章中所说的，只在输入中加入位置信息是显然不够的，Transformer也应该在其结构中加入序列信息。这样做的好处是当我们在计算权值或者特征值的时候，额外添加了位置信息，无疑将有助于这两个变量的计算。</p>
<p><img src="https://pic4.zhimg.com/80/v2-1a8893996fde98acd5763a210f17edc7_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>图4：Transformer不具有编码相对位置的结构特征</p>
<p>RPR提出的模型的原理是在计算第 i 个元素与第 j 个元素之间的attention的值和权值的时候加入 i 与 j 之间的距离编码，因为加入的是 i 与 j 之间的相对位置关系，因此叫做相对位置编码。</p>
<p>例如对一个长度为5的序列，它共有9个相对位置编码信息（当前位置编码，当前位置的前4个，当前位置的后四个），如下表所示：</p>
<table>
<thead>
<tr>
<th>Index</th>
<th>解释</th>
<th>Value</th>
</tr>
</thead>
<tbody><tr>
<td>0</td>
<td>位置i与位置i-4之间的距离</td>
<td>-4</td>
</tr>
<tr>
<td>1</td>
<td>位置i与位置i-3之间的距离</td>
<td>-3</td>
</tr>
<tr>
<td>2</td>
<td>位置i与位置i-2之间的距离</td>
<td>-2</td>
</tr>
<tr>
<td>3</td>
<td>位置i与位置i-1之间的距离</td>
<td>-1</td>
</tr>
<tr>
<td>4</td>
<td>位置i与位置i之间的距离</td>
<td>0</td>
</tr>
<tr>
<td>5</td>
<td>位置i与位置i+1之间的距离</td>
<td>1</td>
</tr>
<tr>
<td>6</td>
<td>位置i与位置i+2之间的距离</td>
<td>2</td>
</tr>
<tr>
<td>7</td>
<td>位置i与位置i+3之间的距离</td>
<td>3</td>
</tr>
<tr>
<td>8</td>
<td>位置i与位置i+4之间的距离</td>
<td>4</td>
</tr>
</tbody></table>
<p>通过加入上面的相对位置编码信息，我们再对比一下“I think therefore I am”中两个‘I’的输入有什么不同，如图5所示。(a)是第一个‘I’的相对位置编码信息，(b)是第二个‘I’的相对位置编码信息。RPR并没有根据输入序列的长度来确定需要考虑的相对位置之间的距离，而是用了一个固定的常数 k ，也就是说我们需要学习的相对位置编码的序列长度为 2k+1 。对于 k 的取值，论文中给出了不同值得对比实验结果，结论是当 k≥2 时，得到的效果非常接近。</p>
<p><img src="https://pic1.zhimg.com/80/v2-f9b61f803d54ac3bba45605c169438dc_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>图5：RPR的相对位置编码</p>
<p>刚刚我们已经说过，RPR需要为学习两个相对位置向量，一个是计算第 i 各词特特征 zi ，另一个用于计算第 i 个词到第 j 个词之间的权值系数 ei,j ，不同于投影矩阵，这两个嵌入在注意力头之间是共享的。</p>
<p>对比(1)式和(3)式，RPR在self-attention中添加了两个可学习的变量 ai,jV <em>和</em> ai,jK 。其中 zi 的计算方式改为：</p>
<p>(8)zi&#x3D;∑j&#x3D;1nαi,j(xjWV+ai,jV)</p>
<p>ei,j 式的变化和 zi 基本相同：</p>
<p>(9)ei,j&#x3D;(xiWQ)(xjWK+ai,jK)Tdk</p>
<p>这里用加法的原因是因为这样设计计算效率更高。关于计算效率的分析，自行翻阅参考文献中的[3]和[5]。</p>
<p>ai,jK <em>和</em> ai,jV 的计算方式相同，即在 [−k,k] 的范围之内计算相对距离，超出范围的用 0 或者 k 进行截断：<br>(10)ai,jK&#x3D;wclip(j−i.k)K</p>
<p>(11)ai,jV&#x3D;wclip(j−i.k)V</p>
<p>(12)clip(x,k)&#x3D;max(−k,min(k,x)) </p>
<h3 id="3-Transformer-XL介绍"><a href="#3-Transformer-XL介绍" class="headerlink" title="3. Transformer-XL介绍"></a><strong>3. Transformer-XL介绍</strong></h3><p>Transformer-XL的提出旨在解决上面所列出的Transformer的三个问题，为了解决上下文碎片和推理速度慢的问题，作者推出了片段递归机制，为了解决长期依赖，作者对绝对位置编码进行了改进，并推出了相对位置编码机制。下面分别详细介绍两个优化点。</p>
<h4 id="3-1-片段递归"><a href="#3-1-片段递归" class="headerlink" title="3.1 片段递归"></a><strong>3.1 片段递归</strong></h4><p>和Transformer一样，Transformer-XL在训练的时候也是以固定长度的片段的形式进行输入的，不同的是Transformer-XL的上一个片段的状态会被缓存下来然后在计算当前段的时候再重复使用上个时间片的隐层状态。因为上个片段的特征在当前片段进行了重复使用，这也就赋予了Transformer-XL建模更长期的依赖的能力。</p>
<p>那么Transformer-XL是如何重用上个片段的隐层状态呢，我们通过数学的形式具体说明。长度为 L 的连续两个片段表示为 sτ&#x3D;[xτ,1,⋯,xτ,L] <em>和</em> sτ+1&#x3D;[xτ+1,1,⋯,xτ+1,L] 。 sτ <em>的隐层节点的状态表示为</em> hτn∈RL×d ，其中 d 是隐层节点的维度。 sτ+1 <em>的隐层节点的状态</em> hτ+1n 的计算过程为：</p>
<p>(13)h~τ+1n−1&#x3D;[SG(hτn−1)∘hτ+1n−1]</p>
<p>(14)qτ+1n,kτ+1n,vτ+1n&#x3D;hτ+1n−1Wq⊤,h<del>τ+1n−1Wk⊤,h</del>τ+1n−1Wv⊤</p>
<p>(15)hτ+1n&#x3D;Transformer-Layer(qτ+1n,kτ+1n,vτ+1n)</p>
<p>其中 SG(⋅) 表示stop-gradient，表示这一部分并不参与BP的计算， [hu∘hv] 表示两个隐层节点在长度维度进行拼接， W 是模型需要学习的参数。注意 kτ+1n <em>和</em> vτ+1n 使用的是扩展了上个片段的隐层状态的 h~τ+1n−1 。这一部分如图6所示：</p>
<p><img src="https://pic1.zhimg.com/v2-9e06489e01aa509b1a00d567bf9326bc_b.jpg" srcset="/blog/img/loading.gif" lazyload alt="动图封面"></p>
<p>图6：Transformer-XL的训练过程</p>
<p>片段递归的另一个好处是带来的推理速度的提升，对比Transformer的自回归架构每次只能前进一个时间片，Transfomer-XL的推理过程（图7）通过直接复用上一个片段的表示而不是从头计算，讲推理过程提升到以片段为单位进行推理，这种简化带来的速度提升是成百上千倍的。</p>
<p><img src="https://pic4.zhimg.com/v2-88e4f76e9cab63dccf2122e70fb93b93_b.jpg" srcset="/blog/img/loading.gif" lazyload alt="动图封面"></p>
<p>图7：Transformer-XL的推理过程</p>
<p>Transformer-XL是一个典型的用空间换时间的方案，因为这个方法需要对上个片段的隐层节点的状态进行缓存，无疑将增大模型的显存占用量，但依照目前硬件的发展速度来看，对一个速度和准确率都大幅提升的模型，显存是不会成为它的瓶颈的。而且只要显存足够大，其实我们也可以复用更多的之前片段的隐层状态。</p>
<p>从这个角度看，Transformer-XL是一个和<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/42706477">残差网络</a>思想非常接近的一个模型，它相当于在两个片段之间添加了一条short-cut。而复用更多片段的结构则是一个<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/42708327">DenseNet</a>思想的模型。</p>
<h4 id="3-2-Transformer-XL的相对位置编码"><a href="#3-2-Transformer-XL的相对位置编码" class="headerlink" title="3.2 Transformer-XL的相对位置编码"></a><strong>3.2 Transformer-XL的相对位置编码</strong></h4><p>Transformer-XL的相对位置编码参考了RPR中把相对位置编码加入到self-attention中的思想，Transfomer-XL在(7)式的基础上做了若干变化，得到了下面的计算方法：</p>
<p>(16)Ai,jrel&#x3D;Exi⊤Wq⊤Wk,EExj⏟(a)+Exi⊤Wq⊤Wk,RRi−j⏟(b)+u⊤Wk,EExj⏟(c)+v⊤Wk,RRi−j⏟(d)</p>
<ul>
<li>第一个变化出现在了(a)，(b)，(c)，(d)中： Wk 被拆分成立 Wk,E<em>和</em>wk,R ，也就是说输入序列和位置编码不再共享权值。</li>
<li>第二个变化是(b)和(d)中将绝对位置编码 Uj<em>换成了相对位置编码</em>Ri−j ，其中 R 是Transformer中采用的不需要学习的sinsoid编码矩阵，原因正如第二借所介绍的，相对位置比绝对位置更为重要。</li>
<li>第三个变化是(c)，(d)中引入了两个新的可学习的参数 u∈Rd 和 v∈Rd 来替换Transformer中的query向量 Ui⊤Wq⊤ 。表明<strong>对于所有的query位置对应的query(位置)向量是相同的。</strong> 即无论query位置如何，对不同词的注意偏差都保持一致。</li>
</ul>
<p>改进之后(16)中的四个部分也有了各自的含义：</p>
<ul>
<li>(a) 没有考虑位置编码的原始分数，只是基于内容的寻址；</li>
<li>(b) 相对于当前内容的位置偏差；</li>
<li>(c) 从内容层面衡量键的重要性，表示全局的内容偏置；</li>
<li>(d) 从相对位置层面衡量键的重要性，表示全局的位置偏置。</li>
</ul>
<p>式(16)使用乘法分配律得到的表达式为:</p>
<p>(17)Ai,jrel&#x3D;(WqExi+u)⊤Wk,EExj+(WqExi+v)⊤Wk,RRi−j</p>
<h3 id="4-总结-1"><a href="#4-总结-1" class="headerlink" title="4. 总结"></a><strong>4. 总结</strong></h3><p>Transformer由于自回归的特性，每个时间片的预测都需要从头开始，这样的推理速度限制了它在很多场景的应用。Transformer-XL提出的<strong>递归机制</strong>，使得推理过程以段为单位，段的长度越长，无疑提速越明显，从实验结果来看，Transformer-XL提速了300-1800倍，为Transformer-XL的使用提供了基础支撑。同时递归机制增加了Transformer-XL可建模的长期依赖的长度（ O(NL) ），这对提升模型的泛化能力也是很有帮助的。</p>
<p>仿照RPR，Transformer-XL提出了自己的相对位置编码算法，此编码方法对比Transformer和RPR都有了性能的提升，而且从理论角度也有了可解释性。谷歌推出的XLNet也是使用了Transformer-XL为基础，我们会在之后的文章中进行分析。</p>
<h2 id="ALBERT"><a href="#ALBERT" class="headerlink" title="ALBERT"></a>ALBERT</h2><p><strong>ALBERT主要的改进就是4点：</strong></p>
<ol>
<li><p>把词向量维度和注意力hidden<em>size脱钩（bert里词向量维度&#x3D;注意力的hidden_size）</em></p>
</li>
<li><ol>
<li><em>词向量只是表示词汇信息，所以维度过高也没有用</em></li>
<li><em>注意力的hidden_size则要学习到上下文表征信息，所以提高这个参数对模型性能有用</em></li>
<li><em>实际方法就是词向量维度&#x3D;E， 然后再用一个线性变换转换成维度&#x3D;hidden_size</em></li>
<li><em>比如10万词汇，E&#x3D;100，hidden_size&#x3D;1000,，线性变换矩阵&#x3D;100×1000 全部参数量&#x3D;10万×100 + 100×1000 &#x3D; 1010万</em></li>
<li><em>上述情况原bert词向量矩阵参数量&#x3D;10万×1000&#x3D;1亿</em></li>
</ol>
</li>
<li><p><em>跨层参数共享：多层attention都使用相同的参数</em></p>
</li>
<li><ol>
<li><em>这主要是为了减少参数量（性能轻微降低，参数大量减少，这整体上是好的事情）</em></li>
<li><em>很多人评论说这点并不make sense</em></li>
<li><em>论文里的消融实验的分数也说明no-share的分数是最高的</em></li>
</ol>
</li>
<li><p><em>用SOP替换NSP</em></p>
</li>
<li><ol>
<li><em>NSP：下一句预测， 正样本&#x3D;上下相邻的2个句子，负样本&#x3D;随机2个句子</em></li>
<li><em>SOP：句子顺序预测，正样本&#x3D;正常顺序的2个相邻句子，负样本&#x3D;调换顺序的2个相邻句子</em></li>
<li><em>NSP任务过于简单，只要模型发现两个句子的主题不一样就行了，所以SOP预测任务能够让模型学习到更多的信息</em></li>
</ol>
</li>
<li><p><em>去掉dropout</em></p>
</li>
<li><ol>
<li><em>模型的内部任务（MLM，SOP等等）都没有过拟合</em></li>
<li><em>dropout是为了降低过拟合而增加的机制，所以对于bert而言是弊大于利的机制</em></li>
</ol>
</li>
</ol>
<h3 id="论文翻译-1"><a href="#论文翻译-1" class="headerlink" title="论文翻译"></a>论文翻译</h3><h4 id="一、论文概要"><a href="#一、论文概要" class="headerlink" title="一、论文概要"></a>一、论文概要</h4><p>本论文是由google和芝加哥丰田技术学院的六名作者于2020年发表于ICLR上面的。</p>
<p>文章提出了两种参数精简的技术，来解决BERT模型内存消耗太大，训练时间太长的问题。另外，文章使用了自监督的损失来建模句子连贯性，并证明该损失函数能够提升多句子作为输入的下游任务的性能。</p>
<p>本文所提出的模型ALBERT在 GLUE、RACE 和 SQuAD 这3个数据任务上都取得了新的SOTA结果，且参数量还少于 BERT-large。</p>
<p>论文链接：<a href="https://link.zhihu.com/?target=https://arxiv.org/pdf/1909.11942.pdf">https://arxiv.org/pdf/1909.11942.pdf</a></p>
<p>代码链接：<a href="https://link.zhihu.com/?target=https://github.com/brightmart/albert_zh">https://github.com/brightmart/albert_zh</a></p>
<p><a href="https://link.zhihu.com/?target=https://github.com/lonePatient/albert_pytorch">https://github.com/lonePatient/albert_pytorch</a></p>
<h4 id="二、背景介绍"><a href="#二、背景介绍" class="headerlink" title="二、背景介绍"></a>二、背景介绍</h4><p>过往的研究者们在诸多NLP任务上的实验已经表明，模型规模在取得SOTA结果上至关重要。在应用场景中通常是预训练一个大规模的模型，再对其进行蒸馏萃取出一个更小的模型。</p>
<p>模型太大，容易带来两个问题：</p>
<ol>
<li>内存受限。当下的各种SOTA模型动辄数亿甚至数十亿个参数，倘若要扩大模型规模，这个内存问题是无法回避的。</li>
<li>训练速度上的限制。由于通信开销与模型中参数的数量成正比，在分布式训练中训练速度将成为一大瓶颈。</li>
</ol>
<h4 id="三、方法简介"><a href="#三、方法简介" class="headerlink" title="三、方法简介"></a>三、方法简介</h4><p>ALBERT 引入两种参数精简技术，克服了扩展预训练模型面临的主要障碍。</p>
<p>第一种技术是对<strong>嵌入参数进行因式分解</strong>(factorized embedding parameterization)。将一个大的词汇嵌入矩阵分解为两个小矩阵，从而将隐藏层的大小与词汇嵌入的大小分离开来。这种分离便于后续隐藏层单元数量的增加，怎么说呢？就是增加隐藏层单元数量，并不显著增加词汇嵌入的参数量。</p>
<p>第二种技术是<strong>跨层参数共享</strong>。这一技术可以避免参数量随着网络深度的增加而增加。</p>
<p>这两种技术都显著降低了 BERT 的参数量，同时不显著损害其性能， 从而提升了参数效率。ALBERT 的配置类似于 BERT-large，但参数量仅为后者的 1&#x2F;18，训练速度却是后者的 1.7 倍。 这些参数精简技术还可以充当某种形式的正则化，可以使训练更加稳定，且有利于泛化。</p>
<p>为了进一步提升 ALBERT 的性能， 本文还引入了一个自监督损失函数，用于<strong>句子顺序预测</strong>（SOP，sentence-order prediction）。SOP 主要聚焦于句间连贯，用于解决原版 BERT中下一句预测（NSP）损失的低效问题。因为确实已有研究(Yang et al., 2019; Liu et al., 2019)表明NSP是可以去掉的。</p>
<p>基于上述的这3个设计，ALBERT 能够扩展为更大的版本，在参数量仍然小于 BERT-large的同时，性能可以显著提升。本文在GLUE、SQuAD 和 RACE 这3个自然语言理解基准测试上都刷新了记录：在 RACE 上的准确率为 89.4%，在 GLUE 上的得分为 89.4，在 SQuAD 2.0 上的 F1 得分为 92.2。</p>
<h4 id="四、模型组成"><a href="#四、模型组成" class="headerlink" title="四、模型组成"></a>四、模型组成</h4><h5 id="4-1-模型架构"><a href="#4-1-模型架构" class="headerlink" title="4.1 模型架构"></a>4.1 模型架构</h5><p>ALBERT 架构的主干网络与 BERT 相似，即使用 Transformer 编码器和 GELU 非线性激活函数。为便于论述，符号约定如下：词嵌入大小为 E 、编码器层数为 L 、隐藏层大小为 H 。本文将前馈网络&#x2F;滤波器的大小设置为 4H ，将注意力头的数量设置为 H&#x2F;64 。</p>
<p>ALBERT框架与BERT相比有以下3点不同：</p>
<ol>
<li><strong>嵌入参数的因式分解</strong><br>在BERT 及后续的 XLNet 和 RoBERTa 中，WordPiece 词嵌入大小 E 和隐藏层大小 H 是相等的，即 E≡H 。这在建模和实际使用中，看起来可能并不是最优的。</li>
</ol>
<p>(1)从建模的角度来说，WordPiece 词嵌入的目标是学习<strong>上下文无关的表示</strong>，而隐藏层嵌入的目标是<strong>学习上下文相关的表示</strong>。[Liu et al.,2019]通过<strong>上下文长度</strong>相关的实验表明，BERT的表征能力很大一部分来自于使用上下文以在学习过程提供<strong>上下文相关的表征信息</strong>。因此，将 WordPiece 词嵌入大小 E 从隐藏层大小 H 分离出来，可以更高效地利用总体的模型参数， 其中 H 远远大于 E 。<br>(2)从实践的角度，自然语言处理使用的词典大小 V 一般非常庞大，如果 E 恒等于 H ，即 E≡H ，那么增加 H 将直接增大词典嵌入矩阵的大小( V×E )。这会导致模型参数剧增，而模型训练过程中大部分参数的更新是很稀疏的。</p>
<p>因此，本文所提出的ALBERT对词嵌入参数进行了因式分解，将其分解为两个小矩阵。本文不再将独热向量直接映射到大小为H 的隐藏空间，而是先将它们映射到一个低维词嵌入空间 E ，然后再映射到隐藏空间。通过这种分解，可以将词嵌入参数从 O(V×H) 降低到 O(V×E+E×H) 。这在 H 远远大于 E 的时候，参数量减少得非常明显。</p>
<p><strong>2. 层之间参数共享</strong></p>
<p>ALBERT中还使用了一种跨层参数共享机制来进一步提升参数效率。下图展示了每一层输入与输出嵌入矩阵间的 L2 距离与余弦相似性。</p>
<p><img src="https://pic2.zhimg.com/80/v2-bb7907208ab15b9553f7d904df5d5b05_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>上图展示了BERT-Large 与 ALBERT-Large每一层输入嵌入与输出嵌入间的 L2 距离与余弦相似性。从图中可以发现 ALBERT 从一层到另一层的转换要比 BERT 平滑得多。这说明权重共享有效地提升了神经网络参数的鲁棒性。尽管相比于 BERT，ALBERT在两个评价指标都有所下降，但在 24 层以后，它们也不会收敛到 0。</p>
<p><strong>3. 句间连贯性损失</strong><br>除了语言建模损失外，BERT 还使用了下一句预测损失(NSP)。NSP损失本是为了提升下游任务的性能，但是后来很多研究 (Yang et al., 2019; Liu et al.,2019)发现这种机制并不是很高效，因此决定去除它。</p>
<p>据此我们猜测，NSP低效的原因，主要是它的难度太小。因为下一句预测将主题预测和连贯性预测结合到同一任务中，然而主题预测比连贯性预测简单得多，同时NSP与MLM损失函数学到的内容是有重合的。</p>
<p>由于句间建模在语言理解中非常重要，因此本文提出一种基于<strong>语言连贯性的损失函数</strong>。在ALBERT中本文使用了一个句子次序预测（SOP）损失函数，它会避免预测主题，而只关注建模句子之间的连贯性。SOP正样本获取方法与BERT相同，而负样本仅仅是将正样本的两个部分次序对换。实验结果表明SOP能够在合理范围内解决NSP任务。在使用了该损失函数后，ALBERT能显著提升下游多句子编码任务的性能。</p>
<h5 id="4-2-模型设置"><a href="#4-2-模型设置" class="headerlink" title="4.2 模型设置"></a>4.2 模型设置</h5><p>本文用到的BERT和ALBERT对应的超参数如下表所示。</p>
<p><img src="https://pic3.zhimg.com/80/v2-dc1e51ac29f4dd39a8982d0b654ddb62_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>可以看出，ALBERT的模型参数量远远小于对应的BERT模型。例如，ALBERT-large的参数量仅为BERT-large的1&#x2F;18，即18M vs 334M。对于ALBERT-xxlarge，由于24层网络结构与12层网络结构在结果上接近，但是计算量更大，所以本文主要采用12层的网络结构。</p>
<h4 id="五、实验结果"><a href="#五、实验结果" class="headerlink" title="五、实验结果"></a>五、实验结果</h4><h5 id="5-1-实验设置"><a href="#5-1-实验设置" class="headerlink" title="5.1 实验设置"></a>5.1 实验设置</h5><p>为了进行更公平的对比，本文一方面使用与原始 BERT相同的配置训练试验模型，另一方面采用 BOOKCORPUS 和 English Wikipedia 共计 16GB 的纯文本作为预训练任务的数据。与BERT一样，使用的词典大小是30000；此外还借鉴了XLNet中使用的SentencePiece。在MLM目标函数上使用 n -gram的masking，随机选用 n -gram的mask遮蔽输入。长度 n 的预测概率为：</p>
<p>(1)p(n)&#x3D;1&#x2F;n∑k&#x3D;1N1&#x2F;k<br>本文设置的n-gram最大长度为3，即MLM目标最多由3个词组成。</p>
<h5 id="5-2-BERT和ALBERT的综合对比"><a href="#5-2-BERT和ALBERT的综合对比" class="headerlink" title="5.2 BERT和ALBERT的综合对比"></a>5.2 BERT和ALBERT的综合对比</h5><p>从表2看出，ALBERT-xxlarge 的参数量只有 BERT-Large 70% ，但性能却能够显著超越BERT-large。具体表现在SQuAD v1.1上提升1.7%，在SQuAD v2.0上提升4.2%，MNLI上提升2.2%，在SST-2上提升3.0%，在RACE上提升8.5%。此外，还观察到BERT-xlarge在全部的指标上全面溃败于BERT-base。这说明形如BERT-xlarge的大参数模型相较于更小参数量的模型是更难训练的。</p>
<p><img src="https://pic4.zhimg.com/80/v2-bf1cf0f4c94a9e153592bae19235159f_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>另一个有趣的发现是，相同训练配置下(相同TPUs数量)训练一次，数据的吞吐速度差异。由于ALBERT模型通信更少，计算量更低，所以相比于BERT有更高的数据吞吐量。</p>
<p>接下来介绍ALBERT中3个主要部分对模型提升所带来的影响。</p>
<h5 id="5-3-嵌入参数的因式分解"><a href="#5-3-嵌入参数的因式分解" class="headerlink" title="5.3 嵌入参数的因式分解"></a>5.3 嵌入参数的因式分解</h5><p>表3展示了在ALBERT-base(具体设置参见上述表2)上修改词嵌入大小 E 带来的影响，它们的参数量及下游任务效果也都展示在内。</p>
<p><img src="https://pic4.zhimg.com/80/v2-5204d44c217cc855623b62351eecb04f_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>对于非共享下(BERT-style)，更大的嵌入尺寸能够取得更好的结果，但是提升的幅度其实不大。对于全共享(ALBERT-style)，嵌入大小128是最好的。基于上述这些结果，本文在后续的实验中的嵌入大小统一选用 E&#x3D;128</p>
<h5 id="5-4-层之间参数共享"><a href="#5-4-层之间参数共享" class="headerlink" title="5.4 层之间参数共享"></a>5.4 层之间参数共享</h5><p>表4展示了不同层之间参数共享的效果，同样使用 ALBERT-base 作为示例模型，此外还增加了嵌入大小为768的结果。对比了所有全共享策略(ALBERT-style)、非共享策略(BERT-style)及其介于二者之间的中间策略(仅注意力参数共享，FNN不共享；仅FNN参数共享，注意力参数不共享)。</p>
<p><img src="https://pic3.zhimg.com/80/v2-d70d523bcd92f44b15512444440ad2aa_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>从上述结果可以看出，全共享型策略在E&#x3D;768和E&#x3D;128上都会一定程度上降低性能。但是，需要说明的是，下降幅度较小，对于E&#x3D;128，平均下降1.5；对于E&#x3D;768，平均下降2.5。再细看，共享FFN层的参数，应该是罪魁祸首；而注意力机制的参数共享带来的影响不能一概而论，对于E&#x3D;128反而在平均性能上提升了0.1，对于E&#x3D;768平均性能下降0.7。</p>
<h5 id="5-5-句子次序预测-SOP"><a href="#5-5-句子次序预测-SOP" class="headerlink" title="5.5 句子次序预测(SOP)"></a>5.5 句子次序预测(SOP)</h5><p>表5展示了SOP与下一句预测损失（NSP）的对比效果。</p>
<p><img src="https://pic4.zhimg.com/80/v2-8c5e560ea52506c139987bfb4e2c237f_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>这里对比了3种策略：没有句子间损失(比如XLNet和RoBERTa)、NSP(比如BERT)、SOP(ALBERT)。这里采用的ALBERT也是ALBERT-base。</p>
<p>对比过程，一方面对比自身任务中的准确率，另一方面是下游任务的性能表现。</p>
<ul>
<li>在自身任务这一维度，可以看出NSP损失对于SOP几乎是没有任何益处，NSP训练后，在SOP上的表现只有52%，这跟瞎猜差不了多少。据此，可以得出结论：NSP建模止步于主题识别。反观SOP损失，确实一定程度上能够解决NSP任务，其准确率为78.9%，而自身的准确率为86.5%。</li>
<li>在下游任务上SOP损失统统起到促进作用，具体表现在SQuAD1.1提升1%，SQuAD 2.0提升2%，RACE提升1.7%。</li>
</ul>
<h5 id="5-6-相同训练时长下的对比"><a href="#5-6-相同训练时长下的对比" class="headerlink" title="5.6 相同训练时长下的对比"></a>5.6 相同训练时长下的对比</h5><p>从表3中的提速结果看出BERT-large的数据吞吐量大概是ALBERT-xxlarge的3.17倍。一般而言，训练越长性能越高，鉴于此，我们进一步对比相同训练时长下不同模型的表现。</p>
<p><img src="https://pic1.zhimg.com/80/v2-f3d98b4513cc8ecfce0d04200767082c_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>在训练了差不多相同的时间之后，ALBERT-xxlarge 明显优于 BERT-large。</p>
<h5 id="5-7-引入额外训练集和Dropout的影响"><a href="#5-7-引入额外训练集和Dropout的影响" class="headerlink" title="5.7 引入额外训练集和Dropout的影响"></a>5.7 引入额外训练集和Dropout的影响</h5><p>上述实验都是在 Wikipedia 和 BOOKCORPUS 数据集上进行的，那么，如果增加额外的数据会对结果产生怎样的影响？这里采用的额外数据与XLNet和RoBERTa中的相同。</p>
<p><img src="https://pic2.zhimg.com/80/v2-6711a118a7c25cf011adfd4daf013475_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>图2a 表明，添加额外数据后，模型的开发集 MLM 准确率显著提升。<br>添加额外数据后模型在下游任务中的性能情况，如表7所示：</p>
<p><img src="https://pic3.zhimg.com/80/v2-9babe3e4badca951b8516a6b698a8082_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>我们还注意到，即使在训练了 100 万步之后，最大的模型仍然没有过拟合。因此，尝试删除dropout，以进一步提高模型能力。如图2b 所示，去掉 dropout 可以显著提高 MLM 准确度。去掉dropout后在下游任务上的表现如表8所示：</p>
<p><img src="https://pic1.zhimg.com/80/v2-e9d57a11c90328fd2e91ab9d4b14fa20_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<h5 id="5-8-当下SOTA模型在NLU任务上的对比"><a href="#5-8-当下SOTA模型在NLU任务上的对比" class="headerlink" title="5.8 当下SOTA模型在NLU任务上的对比"></a>5.8 当下SOTA模型在NLU任务上的对比</h5><p>除了上述实验之外，ALBERT 在 GLUE、SQuAD 和 RACE 基准测试中都取得了 SOTA 结果，如表9、10 所示：</p>
<p><img src="https://pic3.zhimg.com/80/v2-49d391ad58c052092630bc4cd9eab826_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p><img src="https://pic4.zhimg.com/80/v2-a6f32ae4895fecbcca2df8ded95269db_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<h4 id="六、总结"><a href="#六、总结" class="headerlink" title="六、总结"></a>六、总结</h4><ul>
<li>本文提出了嵌入参数的因式分解和层之间参数共享两种精简参数量的方式，在大家都想着把模型做大的时候给大家指出了另一条可行的路，意义重大。</li>
<li>本文提出了SOP，很好地替换了NSP作为预训练任务，给模型表现带来了明显提升。</li>
<li>本文的精简参数使模型表现下降，结果更好主要是靠SOP、更大的 H 、更多的数据、去除dropout。那么如果不削减参数的话再使用SOP、加更多的数据、去除dropout呢？</li>
<li>本文的精简参数量参数量带来了模型训练速度的提升，但是ALBERT-xxlarge比BERT-xlarge参数量少了约1000M，而训练速度并没有太大的提升（只有1.2倍）。原因应该是更少的参数量的确能带来速度上的提升，但是本文提出的嵌入参数的因式分解引入了额外的矩阵运算，并且同时ALBERT-xxlarge大幅增加了 H ，实际上增加了模型的计算量。</li>
<li>本文还有两个小细节可以学习，一个是在模型不会过拟合的情况下不使用dropout；另一个则是warm-start，即在训练深层网络（例如12层）时，可以先训练浅层网络（例如6层），再在其基础上做微调，这样可以加快深层模型的收敛。</li>
</ul>
<h2 id="DistrillBERT"><a href="#DistrillBERT" class="headerlink" title="DistrillBERT"></a>DistrillBERT</h2><h3 id="论文解读"><a href="#论文解读" class="headerlink" title="论文解读"></a>论文解读</h3><h4 id="摘要："><a href="#摘要：" class="headerlink" title="摘要："></a>摘要：</h4><p>随着迁移学习在NLP领域越来越普及，在有限的GPU算力下用大型的model进行training or inference仍是一项具有挑战性的工作。在本文的工作中，我们提出了一种更加通用的更轻型的pre-trained language model，DistilBERT。对比其他的大型Model，DistilBERT在下游的fine-tune 任务中，仍具有很好的效果。前人的大多数研究都是基于蒸馏技术，构建某个具体领域的任务。而在本文的工作中在Pretrained 阶段利用了知识蒸馏技术，<strong>在保留97%的模型语言理解能力的条件下，减轻了40%模型的参数量，并且模型的推理速度提升了60%。</strong></p>
<h4 id="1-引言："><a href="#1-引言：" class="headerlink" title="1 引言："></a>1 引言：</h4><p><img src="https://pic2.zhimg.com/80/v2-a85b16eec66a4dd782986a78f48e2dbd_1440w.jpg" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>近年来，语言模型的参数量越来越大，基本上都是several hundred million parameters级别的。这些模型对于GPU的算力是一项巨大的挑战。在一些终端设备上，例如智能手机中运行如此巨大的模型是非常困难的。</p>
<p>本文的研究证明了<strong>使用更小的蒸馏后的模型</strong>，在下游任务中模型的表现仍是良好的，同时蒸馏后的更轻便的模型，也可以很好的部署到智能设备终端上。通过本文提出的triple loss，我们蒸馏出了一个比Transform 小40%的model，在各项NLP下游任务中推理速度提高了60%。triple loss的参数对于模型的训练有着非常重要的作用，在GitHub上的HuggingFace项目，我们提供经过训练的权重参数。</p>
<h4 id="2-知识蒸馏"><a href="#2-知识蒸馏" class="headerlink" title="2 知识蒸馏"></a>2 知识蒸馏</h4><p>知识蒸馏是图领奖得主，深度学习之父——Hinton， 提出的一个模型压缩技术。即有一组Teacher模型和Student模型， Teacher模型是首先训练出来更大的模型， 再由Teacher模型输出的Logits分布 去训练小一点的Student模型，同时Student模型也保有了Teacher模型的泛化能力。</p>
<p>补充知识蒸馏中的两个重要概念：</p>
<p>1).soft target:</p>
<p>传统的分类任务Training 过程是将Label值作为“groud truth” 求极大似然估计，称为“hard target”。而“soft target”是将Teacher模型中的输出的Logits 分布作为soft targets。</p>
<p>2).softmax-temperature:</p>
<p>pi&#x3D;exp(zi&#x2F;T)∑zj&#x2F;T</p>
<p>其中，T即是蒸馏温度，控制着输出分布的平滑程度，当T为1时就是标准的SoftMax公式啦， zi表示类别i的score。最终的损失函数是 Lce 和masked language modeling loss Lmlm 的线性组合，另外作者发现添加余弦嵌入损失（ Lcos ）有利于student 和 teacher 隐藏状态向量的方向一致。</p>
<p>知识蒸馏的具体步骤如下：</p>
<p>a.训练复杂的教师模型（teacher model）：先用硬目标（hard target），也就是正常的标签（label）训练大模型。</p>
<p>b.计算软目标（soft target）：利用训练好的大模型来计算软目标（soft target），也就是大模型“软化后”再经过softmax层的输出。</p>
<p>c.训练小模型，在小模型的基础上再加一个额外的软目标（soft target）的损失函数，通过比例参数来调节两个损失函数的比重。</p>
<p>d.预测时，将训练好的小模型来进行实验。</p>
<p>3 DistilBert: a distilled version of BERT</p>
<p>student model 具有和Bert相同的结构，当模型层数减少1&#x2F;2时，移除了Token type embedding 和pooler. Transformer 架构中 线性层和LN层，可以充分地优化模型结构，我们的研究表明最后一层张量维度的变化对计算效率影响 比层数变化要小，所以作者关注于减少模型层数。在初始化方面，作者从teacher model中每两层选择一层做初始化，蒸馏应用了Liu et al. [2019] 提出的 BERT 模型训练最佳实践。语料和 Bert 使用的一致。</p>
<p>在训练方面，DistilBERT 在8个16GB V100 GPU上训练了大约90小时，作为对比，RoBERTa模型在1024个32GB V100上训练了1天。</p>
<p>实验结果：</p>
<p><img src="https://pic1.zhimg.com/80/v2-c595801246af0687eb962aac36b20f10_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>ablation study(不知道怎么翻译啦…..) 表明了实际上masked language model loss 对模型的效果影响是很小的。 </p>
<p><img src="https://pic4.zhimg.com/80/v2-a0d066b7cc8ef13411e80676f4285adf_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<h4 id="结论："><a href="#结论：" class="headerlink" title="结论："></a>结论：</h4><p>DistilBert 比Bert在参数量方面小40%，比Bert 快60%，却保留了Bert 97%的语言理解能力。本文展示了，Bert成功的蒸馏案例，在未来终端应用上DistillBERT会成为焦点。</p>
<p>BERT 在很多 NLP 任务上都取得不错的效果，但是其模型体积与计算量都很大，而且现在出现了更多越来越大的模型，例如 roBERTa 和 GPT2。由于这些模型的太大，难以用于一些性能没那么好的机器上，也不能很好地用于实时性要求高的应用中。因此有不少针对 BERT 模型压缩的研究，其中模型蒸馏 Distillation 是一种比较好的解决方法，本文介绍两种基于模型蒸馏的 BERT 模型压缩方法。</p>
<h4 id="1-前言"><a href="#1-前言" class="headerlink" title="1.前言"></a>1.前言</h4><p><img src="https://upload-images.jianshu.io/upload_images/20030902-f00bb7c21ec6bd5b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>各种模型的大小</p>
<p>上图展示了很多基于 Transformer 的模型，模型下方的数字对应了模型的参数数量，单位是百万，可以看到这些模型变得越来越大。这些模型的体积也限制了其在现实世界中的使用，因为各方面因素：</p>
<ul>
<li>这种模型的训练花费大量的金钱，需要使用昂贵的 GPU 服务器才能提供大规模的服务。</li>
<li>模型太大导致 inference 的时间也变长，不能用于一些实时性要求高的任务中。</li>
<li>现在有不少机器学习任务需要运行在终端上，例如智能手机，这种情况也必须使用轻量级的模型。</li>
</ul>
<p>基于以上的原因，不少研究开始针对 BERT 模型压缩进行，常见的模型压缩方法有以下几种：</p>
<ul>
<li>模型蒸馏 Distillation，使用<strong>大模型</strong>的学到的知识训练<strong>小模型</strong>，从而让小模型具有大模型的泛化能力。</li>
<li>量化 Quantization，降低大模型的精度，减小模型。</li>
<li>剪枝 Pruning，去掉模型中作用比较小的连接。</li>
<li>参数共享，共享网络中部分参数，降低模型参数数量。</li>
</ul>
<p>前一篇文章<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/769e66e085fe">《RoBERTa 和 ALBERT》</a>中介绍的 ALBERT 也是一种 BERT 压缩方法，主要是用了参数共享和矩阵分解的方法压缩 BERT，但是 ALBERT 只减少模型的参数，并不能减少其 inference 的时间。</p>
<p>接下来介绍两种使用模型蒸馏压缩 BERT 的算法，第一种是 DistilBERT，将 12 层的 BERT-base 模型蒸馏到 6 层的 BERT 模型；第二种是将 BERT 模型蒸馏到 BiLSTM 模型。</p>
<h4 id="2-模型蒸馏-Distillation"><a href="#2-模型蒸馏-Distillation" class="headerlink" title="2.模型蒸馏 Distillation"></a>2.模型蒸馏 Distillation</h4><p>首先介绍模型蒸馏的概念，模型蒸馏是一种模型压缩的方法，由 Hinton 在论文《Distilling the Knowledge in a Neural Network》中提出，下面是模型蒸馏的要点：</p>
<ul>
<li>首先需要训练一个大的模型，这个大模型也称为 teacher 模型。</li>
<li>利用 teacher 模型输出的概率分布训练小模型，小模型称为 student 模型。</li>
<li>训练 student 模型时，包含两种 label，soft label 对应了 teacher 模型输出的概率分布，而 hard label 是原来的 one-hot label。</li>
<li>模型蒸馏训练的<strong>小模型会学习到大模型的表现以及泛化能力</strong>。</li>
</ul>
<p>在有监督训练中，模型会有一个预测的目标，通常是 one-hot label，模型在训练时需要最大化对应 label 的概率 (softmax 或者 logits)。在模型预测时的概率分布中，应该是正确的类别概率最大，而其他类别的概率都比较小，但是那些<strong>小概率类别之间的概率大小应该也是存在差异的</strong>，例如将一只狗预测成狼的概率比预测成树木的概率应该更大。这种概率之间的差异在一定程度上表明了模型的泛化能力，一个好的模型不是拟合训练集好的模型，而是具有泛化能力的模型。</p>
<p>模型蒸馏希望 student 模型学习到 teacher 模型的泛化能力，因此在训练模型时采用的 target 是 teacher 模型输出的概率分布，这也称为 soft target。有些模型蒸馏的方法在训练的时候也会使用原来的 one-hot label，称为 hard target。</p>
<p>为了更好地学习到 teacher 模型的泛化能力，Hinton 提出了 softmax-temperature，公式如下:</p>
<p><img src="https://upload-images.jianshu.io/upload_images/20030902-7cb2b2fcd75770f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1048" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>softmax-temperature</p>
<p>softmax-temperature 在 softmax 基础上添加了一个参数 T，T 越接近 0，则分布越接近 one-hot，T 越接近无穷大，则分布越接近平均分布。即 T 越大，分布会越平滑，选择合适的 T 可以让更加 student 模型更能观察到 teacher 模型的类别分布多样性。在训练的过程中，teacher 模型和 student 模型使用同样的参数 T，而后续使用 student 模型推断时，T 设置回 1，变成标准的 softmax。</p>
<h4 id="3-DistilBERT"><a href="#3-DistilBERT" class="headerlink" title="3.DistilBERT"></a>3.DistilBERT</h4><p>DistilBERT 模型是 HuggingFace 发布的，论文是《DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter》。DistilBERT 模型与 BERT 模型类似，但是 DistilBERT 只有 6 层，而 BERT-base 有 12 层，DistilBERT 只有 6600 万参数，而 BERT-base 有 1.1 亿参数。DistilBERT 在减少 BERT 参数和层数的情况下， 仍然可以保持比较好的性能，在 GLUE 上保留了 BERT 95% 的性能。</p>
<h5 id="3-1-DistilBERT-训练"><a href="#3-1-DistilBERT-训练" class="headerlink" title="3.1 DistilBERT 训练"></a>3.1 DistilBERT 训练</h5><p>DistilBERT 使用 KL 散度作为损失函数，q 表示 student 模型的分布，而 p 表示 teacher 模型输出的分布，损失函数如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/20030902-e7ff7ec41ff4d60a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>KL 散度</p>
<p>DistilBERT 最终的损失函数由 KL 散度 (蒸馏损失) 和 MLM (遮蔽语言建模) 损失两部分线性组合得到。DistilBERT 移除了 BERT 模型的 token 类型 embedding 和 NSP (下一句预测任务)，保留了 BERT 的其他机制，然后把 BERT 的层数减少为原来的 1&#x2F;2。</p>
<p>此外 DistilBERT 的作者还使用了一些优化的 trick，例如使用了 teacher 模型的参数对 DistilBERT 模型进行初始化；采用 RoBERTa 中的一些训练方法，例如大的 batch，动态 Mask 等。</p>
<h4 id="3-2-DistilBERT-实验结果"><a href="#3-2-DistilBERT-实验结果" class="headerlink" title="3.2 DistilBERT 实验结果"></a>3.2 DistilBERT 实验结果</h4><p><img src="https://upload-images.jianshu.io/upload_images/20030902-72ff74a8c3dfed1a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>DistilBERT 实验结果</p>
<p>上图是 DistilBERT 在 GLUE 基准开发集上的实验结果，可以看到在所有的数据集上，DistilBERT 的效果都比 ELMo 好，在一些数据集上效果甚至比 BERT 还好，整体上性能也达到了 BERT 的 97%，但是 DistilBERT 的参数量只有 BERT 的 60 %。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/20030902-b603502cf6887621.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>DistilBERT 参数和时间</p>
<p>上图是不同模型参数以及推断时间的对比，可以看到 DistilBERT 的参数比 ELMo 和 BERT-base 都少很多，而且推断时间也大大缩短。</p>
<h3 id="4-将-BERT-蒸馏到-BiLSTM"><a href="#4-将-BERT-蒸馏到-BiLSTM" class="headerlink" title="4.将 BERT 蒸馏到 BiLSTM"></a>4.将 BERT 蒸馏到 BiLSTM</h3><p>另一篇文章《Distilling Task-Specific Knowledge from BERT into Simple Neural Networks》则尝试将 BERT 模型蒸馏到 BiLSTM 模型中，称为 Distilled BiLSTM。即 teacher 模型是 BERT，而 student 模型是 BiLSTM。文章提出了两种模型，其中一个是针对单个句子的分类；另一个是针对两个句子做匹配。</p>
<h4 id="4-1-Distilled-BiLSTM-模型"><a href="#4-1-Distilled-BiLSTM-模型" class="headerlink" title="4.1 Distilled BiLSTM 模型"></a>4.1 Distilled BiLSTM 模型</h4><p><img src="https://upload-images.jianshu.io/upload_images/20030902-4631f955742021a5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/916" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>将 BERT 模型蒸馏到 BiLSTM 模型</p>
<p>上图是第一种 BiLSTM 模型，用于单个句子的分类，将句子的所有单词的词向量输入一个 BiLSTM，然后将前向和后向 LSTM 的隐藏层向量拼接在一起，传入全连接网络中进行分类。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/20030902-6f3432c9edacb7c4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/859" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>将 BERT 模型蒸馏到 BiLSTM 模型</p>
<p>上面是第二种 BiLSTM 模型，用于两个句子的匹配，两个 BiLSTM 输出的隐藏向量分别为 h1 和 h2，则需要用将两个向量拼接在一起，再进行分类。h1 和 h2 拼接的公式如下：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/20030902-5a1081e5fefc8ce6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<h4 id="4-2-Distilled-BiLSTM-训练"><a href="#4-2-Distilled-BiLSTM-训练" class="headerlink" title="4.2 Distilled BiLSTM 训练"></a>4.2 Distilled BiLSTM 训练</h4><p>将 BERT 蒸馏到 BiLSTM 模型，使用的损失函数包含两个部分：</p>
<ul>
<li>一部分是 hard target，直接使用 one-hot 类别与 BiLSTM 输出的概率值计算交叉熵。</li>
<li>一部分是 soft target，使用 teacher 模型 (BERT) 输出的概率值与 BiLSTM 输出的概率值计算均方误差 MSE。</li>
</ul>
<p><img src="https://upload-images.jianshu.io/upload_images/20030902-c6429f83950f3d96.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/987" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>在训练过程中，太小的数据集不足以让 student 模型学习到 teacher 模型的所有知识，所以作者提出了三种数据增强的方法扩充数据：</p>
<ul>
<li>Masking，使用 [mask] 标签随机替换一个单词，例如 “I have a cat”，替换成 “I [mask] a cat”。</li>
<li>POS-guided word replacement，将一个单词替换成另一个具有相同 POS 的单词，例如将 “I have a cat” 替换成 “I have a dog”。</li>
<li>n-gram，在 1-5 中随机采样得到 n，然后采用 n-gram，去掉其他单词。</li>
</ul>
<h4 id="4-3-Distilled-BiLSTM-实验结果"><a href="#4-3-Distilled-BiLSTM-实验结果" class="headerlink" title="4.3 Distilled BiLSTM 实验结果"></a>4.3 Distilled BiLSTM 实验结果</h4><p><img src="https://upload-images.jianshu.io/upload_images/20030902-b769e80c56154a36.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>Distilled BiLSTM 实验结果</p>
<p>上面是 Distilled BiLSTM 模型的结果，可以看到比单纯使用 BiLSTM 模型的效果好很多，在 SST 和 QQP 数据集上的效果甚至比 ELMo 好，说明模型能够学习到一部分 BERT 的泛化能力。但是 Distilled BiLSTM 的效果还是比 BERT 差了很多，说明还是有很多知识不能迁移到 BiLSTM 中。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/20030902-370b535311b9f148.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>上面是 Distilled BiLSTM 的参数和推断时间，BiLSTM 的参数要远远少于 BERT-large，比 BERT-large 少了 335倍，推断时间比 BERT-large 快了 434 倍。压缩效果还是比较明显的。</p>
<h3 id="5-总结"><a href="#5-总结" class="headerlink" title="5.总结"></a>5.总结</h3><p>DistilBERT 模型的效果相对更好，而 Distilled BiLSTM 压缩得更小。</p>
<p>DistilBERT 模型使用了 KL 散度计算 soft target，而 Distilled BiLSTM 使用 MSE 计算。HuggingFace 在博客中给出的原因是，DistilBERT 训练的是语言模型，而 Distilled BiLSTM 针对下游分类任务，语言模型的输出空间维度要大很多，这种时候使用 MSE 可能不同 logit 之间会相互抵消。</p>
<h2 id=""><a href="#" class="headerlink" title=""></a></h2><h2 id="BART"><a href="#BART" class="headerlink" title="BART"></a>BART</h2><h3 id="论文翻译-2"><a href="#论文翻译-2" class="headerlink" title="论文翻译"></a>论文翻译</h3><h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a><strong>Abstract</strong></h4><p>我们提出了BART模型，可以用来预训练seq-to-seq模型的降噪自动编码器（autoencoder）。BART的训练包含两步：</p>
<p>1） 利用任意一种噪声函数分解文本</p>
<p>2） 学习一个模型来重构回原来的文本</p>
<p>它基于标准的transformer模型，尽管它（transformer）很简单，但可以被看作是通用的bert(由于双向编码器)，GPT(具有从左到右的解码器)，以及其他许多最近的预处理方案。（figure 1）我们评估了许多噪声处理方法，通过随机打乱原始句子的顺序和使用一种新的填充方案来寻找最佳性能，该方案将文本的范围替换为单个掩码标记。在对文本进行微调以及阅读理解方面，BART有很好的效果。在GLUE和SQuAD数据集上，BART的性能可以与RoBERTA媲美，在提取文章摘要、问答以及文章总结方面达到了最先进的水平，高达6 ROUGE。在仅使用与训练的目标语言的情况下，BART的BLEU得分比基于反向翻译的机器翻译高1.1，我们还报告了在BART框架内复制其他训练前方案的消融实验，以更好地衡量哪些因素对最终任务表现的影响最大。</p>
<h4 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a><strong>1.</strong> <strong>Introduction</strong></h4><p>自监督方法已经在NLP任务中取得的显著的成功。最成功的方法是掩码语言模型（masked language model）的变体，掩码语言模型是一个用来重建被掩盖住随机单词文本的自动编码器。最近的工作已经显示了通过改进掩盖单词的位置分布、掩码被预测的顺序以及待敌掩码标记部分可选的上下文而取得的成果。然而，这些方法典型的聚焦在特有类型的末端任务（比如跨度预测，范围生成等），限制了他们的适用性。</p>
<p>在这篇论文中，我们提出了BART（Bidirectional and Auto-Regressive Transformers），它是一个在seq-to-seq模型中的降噪自动编码器，广泛适用于各种末端任务。</p>
<p><img src="https://pic2.zhimg.com/80/v2-454fd76ed09d20490c0d627a49879821_1440w.jpg" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>这个方法的一个关键优点是噪声的灵活性，原来的文本可以经过各种变换的处理，包括改变长度。我们评估大量的噪声方法，通过随机打乱原始局势的数据并使用一个新的填充方案来发现最好的性能，该方案是把原来的句子任意长度的单词（包括0）用一个单独的mask代替。该方法通过迫使模型对整个句子长度进行更多的推理，并对输入进行更大范围的转换，从而推广了BERT中的原词掩蔽和下一个句子预测目标。</p>
<p>在对文本进行微调以及阅读理解方面，BART有很好的效果。在GLUE和SQuAD数据集上，BART的性能可以与RoBERTA媲美，在提取文章摘要、问答以及文章总结方面达到了最先进的水平，高达6 ROUGE。</p>
<p>BART也打开了一个关于微调的新思路。我们提出了一个新的策略：将BART叠放在机器翻译中，叠加正在一些额外的transformer网络中。这些网络用来翻译外语为有噪声的英语。通过BART进行传播，从而使用BART作为预先训练的目标端语言模型。这个方法相比于后向翻译机器翻译的baseline,在WMT Romanian-English 数据集中BLEU得分提高了1.1,。</p>
<p>我们还报告了在BART框架内复制其他训练前方案的消融实验，以更好地衡量哪些因素对最终任务表现的影响最大。我们小心的控制一些因素，包括数据和优化参数，这些部分已经被证明了在训练中起着很重要的作用，我们发现，在我们考虑的所有任务中，bard表现出了最始终如一的出色表现。</p>
<h4 id="2-Model"><a href="#2-Model" class="headerlink" title="2. Model"></a><strong>2.</strong> <strong>Model</strong></h4><p>BART是一个降噪自动解码器。它被实现为一个序列到序列的模型，具有对损坏文本的双向编码器和一个从左到右的自回归解码器。为了预训练，我们优化了原始文档的负对数似然值。</p>
<p>2.1 Architecture</p>
<p>BART 使用标准的seq-to-seq的transformer模型，而且还准许GPT,我们用GELUs代替RELU激活函数，并且随机初始化参数N(0,0.02).作为我们的基本模型，在编码器和解码器中，我们使用6个层。在我们的大网络中，使用12层。这个框架和BERT很大程度相关，主要有以下两点不同</p>
<p>1） 解码器的每一层在编码器的最终隐藏层上交叉关注</p>
<p>2） BERT在词预测之前，使用了一个前馈网络，BART没有使用。</p>
<p>3） 总的来说，在相同大小的模型中，BART比BERT多大约10%的参数。</p>
<p>2.2 Pre-training BART</p>
<p>BART通过被分解（corrupting）的文献训练，然后优化重构损失——利用在解码器的模型输出与原文献的交叉熵。不像现有的降噪自动编码器那样只是针对特有的噪声特别构造，BART允许我们使用任何类型的文档分解。在极端的例子中，所有信息包括源信息都丢失了，BART相当于一个语言模型。</p>
<p>我们对一些先前提出的和新的转变进行了实验，但我们相信在开发其他新的替代方案方面有很大的潜力。我们使用的转变如下所示，并且例子见Figure 2.</p>
<p>**Token Masking:**遵循BERT，随机的标记被采样并被代替为【mask】</p>
<p><strong>Token Deletion:</strong> 随机标记被删除，和token masking相反，这个模型必须确定哪个位置缺失。</p>
<p><strong>Text Infilling：</strong> 大量的文本跨度被抽样，跨度长度复合参数为3的泊松分布，每个跨度都被代替为一个单独的【mask】，0长度的跨度也被允许。Text infilling被SpanBERT启发，但是SpanBERT抽样的跨度长度服从不同的分布，并且替代这些跨度用和这个跨度长度相同的【mask】序列。Text filling教这个模型去预测在这个跨度中有多少token缺失。</p>
<p><strong>Sentence Permutation：</strong> 一个文档根据句号被分成句子，这些句子以随机的顺序打乱。</p>
<p><strong>Document Rotation:</strong> 将统一随机选择一个标记，并旋转文档，以便该文档以该标记开始。该任务训练模型识别文档的开始位置。</p>
<p><img src="https://pic3.zhimg.com/80/v2-0d677f0002d6d274137b5126710ad94a_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<h4 id="3-Fine-tuning-BART"><a href="#3-Fine-tuning-BART" class="headerlink" title="3. Fine-tuning BART"></a><strong>3.</strong> <strong>Fine-tuning BART</strong></h4><p>由BART产生的表示可以以多种方式用于下游应用程序.</p>
<p>3.1 Sequence Classification Tasks</p>
<p>对于序列分类任务，相同的输入被输入到编码器和解码器中，最终解码器标记的最终隐藏状态被输入到新的多类线性分类器中。这个方法和在BERT中的CLS token相关。但是我们添加了额外的token到末尾，这样解码器中的token表示就可以从完整的输入中进行解码器。（Figure 3a）</p>
<p>3.2 Token Classification Tasks</p>
<p>对于标记分类任务，例如answer endpoint classification for SQuAD,我们输入一个完整的文献到编码器和解码器，并且使用解码器的最高隐藏层作为每个单词的代表，这个代表被使用来分类token。</p>
<p>3.3 Sequence Generation Tasks</p>
<p>因为BART有自动回归的解码器，对于句子生成任务，它可以直接微调，例如抽象问题的回答和总结。在这些任务中，输入的与去噪训练前的目标密切相关信息被复制然后被处理。在这里编码器输入是输入的句子，解码器通过自回归生成输出。</p>
<p>3.4 Machine Translation</p>
<p>我们也探索使用BART去改善机器翻译解码器，先前的工作已经展示了模型可以通过合并预训练解码器改善，通过添加从bitext学习到的一组新的编码器参数但是从使用在解码器中预训练语言模型的受益是有限的。我们展示了使用整个的BART模型（包括编码器和解码器）作为一个单独的预训练解码器是有限的。通过添加从bitext学习到的一组新的编码器参数。（figure 3b）</p>
<p><img src="https://pic1.zhimg.com/80/v2-62ded005d51dc8fab73a7040dc685750_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>更准确地，我们用一个随机的初始化编码器代替BART的编码器向量层。这个模型是端到端训练的，它训练这个新的编码器把外国文字映射为BART可以去噪的英文输入。这个新的编码器可以使用与原始BART模型不同的词汇表。</p>
<p>我们用两步训练这个源编码器，在这两种情况下，从BART模型的输出反向传播交叉熵损失。</p>
<p>第一步，我们固定BART的大部分参数了，只更新随机初始化的源编码器，和BART的位置向量和BART的编码器第一层中自注意力输入投影矩阵。</p>
<p>第二步，我们以较小的迭代次数训练整个模型参数。</p>
<h4 id="4-Comparing-pre-training-objectives"><a href="#4-Comparing-pre-training-objectives" class="headerlink" title="4. Comparing pre-training objectives"></a><strong>4.</strong> <strong>Comparing pre-training objectives</strong></h4><p>BART 支持比之前工作更大范围的噪声解决方案。我们比较了使用基础大小模型（6个解码器层和6个编码器层，有768个隐藏大小），评估任务的一个代表性子集，我们将在第5部分中考虑完整的大规模实验。</p>
<h5 id="4-1-Comparison-Objectives"><a href="#4-1-Comparison-Objectives" class="headerlink" title="4.1 Comparison Objectives"></a>4.1 Comparison Objectives</h5><p>虽然已经提出了许多预处理目标，但很难对它们进行公平的比较，至少部分原因是培训数据、培训资源、模型之间的架构差异以及微调程序的差异，我们复现了最近提出的用于区分和生成任务的强的预训练方法。我们的目的是尽可能的控制不同预训练方法的变量。然而，为了提高性能，我们对学习率和层标准化的使用做了微小的改变(针对每个目标分别进行调优)。以供参考，我们将我们的实现与BERT发布的数字进行了比较，BERT也在书籍和Wikipedia数据上也进行了1M步的训练。我们在下面几方面进行比较：</p>
<p><strong>Language Model</strong>：类似于GPT，我们训练一个从左到右的transformer语言模型，这个模型等同于BART没有交叉注意的解码器。</p>
<p><strong>Permuted Language Model:</strong> 基于XLNet，我们采样token的1&#x2F;6，然后自回归地随机的集成他们。为了与其他模型保持一致，我们不实现相对位置嵌入或从XLNet跨段关注。</p>
<p><strong>Multitask Masked Language Model:</strong> 我们用额外的自注意力掩码训练一个掩码语言模型自注意力mask以下面的比例被随机选择，1&#x2F;6从左到右，1&#x2F;6从右到左，1&#x2F;3没被标记，剩下的1&#x2F;3前百分之50是没被标记，剩下的是从左到右进行标记。</p>
<p><strong>Masked Seq-to-Seq:</strong> 受到MASS启发，我们掩盖（mask）一个跨度中50%的tokens，然后训练一个seq-to-seq的模型去预测被掩盖的token.</p>
<p>对于排序语言模型，掩码语言模型和多任务掩码模型，我们使用两个流注意力去高效计算句子输出部分的可能性(在输出上使用对角自注意掩码从左到右预测单词)。</p>
<p>我们的实验</p>
<p>1） 把任务看作是标准的seq-to-seq问题，这个源输入到编码器器并且目标是解码器的输出。</p>
<p>2） 或者将源作为前缀添加到解码器中的目标中，只在序列的目标部分有损失。</p>
<p>我们发现第一个工作BART做的更好，第二个工作其他模型做的更好。</p>
<p>为了更直观的比较我们的模型对其微调目标建模的能力（the log likelihood of the human text），我们在Table1中提供了困惑度</p>
<h5 id="4-2-Tasks"><a href="#4-2-Tasks" class="headerlink" title="4.2 Tasks"></a>4.2 Tasks</h5><p><strong>SQuAD:</strong> 一个在维基百科的段落抽取式的问题回答任务。回答是在从北给的文献内容的文本范围内提取。类似BERT，我们把问题和文章内容连接起来作为BART编码器的输入，并且额外的传递他们给解码器。该模型包括预测每个标记的开始和结束索引的分类器。</p>
<p><strong>MNLI:</strong> 一个bitext分类任务去预测一个句子是否可以蕴含另一个。这个微调模型链接这两个句子和一个附加的EOS标记，然后把他们传递给编码器和解码器，和BERT相反，利用EOS token的表示对句子关系进行分类。</p>
<p><strong>ELI5：</strong>一个长形式的抽象问题回答数据集。模型根据问题和支持文档的连接来生成答案。</p>
<p><strong>XSum:</strong> 一个带着精准总结的新闻总结数据集。</p>
<p><strong>ConvAI2:</strong> 一个对话回应生辰过任务，以情节和人物角色为条件。</p>
<p><strong>CNN&#x2F;DM:</strong> 一个新闻总结数据集，总结与原句子相关</p>
<h5 id="4-3-Result"><a href="#4-3-Result" class="headerlink" title="4.3 Result"></a>4.3 Result</h5><p>结果放在了table 1,几个趋势是明显的：</p>
<p><strong>在不同的任务中预训练方法表现差别很大</strong>： 预训练方法的高效和任务高度相关，例如：一个简单的语言模型在ELI5上获得最好的性能，在SQuAD上却表现最差。</p>
<p><strong>Token的掩盖很重要：</strong>基于旋转文件或句子排列的预训练objectives单独地表现不佳。这个成功的方法要么使用token deletion 要么使用masking或自注意力masking. 删除似乎比masking生成任务的性能更好.</p>
<p><strong>Left-to-right 预训练改善了生成器：</strong>掩体语言模型和排列语言模型在生成上的表现不如其他模型好，并且是我们认为在训练前不包括从左到右的自回归语言模型的唯一模型。</p>
<p><strong>对于SQuAD，双向编码器是重要的：</strong>正如前面工作提到的，在SQuAD上仅仅使用从左向右的解码器性能很差，因为文章后面的内容在分类决策上也是重要的。然而，BART在双向层数只有一半的情况下也实现了类似的性能。</p>
<p><strong>The pre-training objective不是唯一重要的因素：</strong>我们序列语言模型性能比XLNet略差。这个差异可能是由于没有包括其他框架的改进，比如相对位置嵌入层和片段及递归（segment-level recurrence）</p>
<p><strong>在ELI5上，纯粹的语言模型性能更好：</strong> ELI5是一个例外，它是唯一一个其他模型优于BART的生成任务。纯语言模型表现最好，这表明当输出仅受输入的松散约束时，BART的效率较低。</p>
<p><strong>BART取得了最稳定的强劲表现：</strong>除了ELI5，使用text-infilling的BART模型在所有任务上表现最好。</p>
<p><img src="https://pic2.zhimg.com/80/v2-66bc90f8a1a6178e4d8bd63a4545090d_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<h4 id="5-Large-scale-Pre-training-Experiments"><a href="#5-Large-scale-Pre-training-Experiments" class="headerlink" title="5. Large-scale Pre-training Experiments"></a><strong>5.</strong> <strong>Large-scale Pre-training Experiments</strong></h4><p>最近工作已经展示了当预处理训练和语料库规模扩大到大批量时，下游性能可以显著提高。为了测试BART在大批量的性能，并且为了创造一个对下游任务有用的模型，我们使用与RoBERTa相同规模来训练BART。</p>
<h5 id="5-1-Experimental-Setup"><a href="#5-1-Experimental-Setup" class="headerlink" title="5.1 Experimental Setup"></a>5.1 Experimental Setup</h5><p>我们与训练一个编码器和解码器都带和12层的大模型，隐藏层大小为1024，按照RoBERTa，我们我们使用的batch size 为8000，并且训练模型50000步，文档使用与GPT-2相同的字节对编码进行标记。基于第4部分的结果，我们使用一个text infilling 和sentence permutation的组合。在每一个文档，我们mask30%的token，然后排序所有句子。尽管句子序列只展示了在CNN&#x2F;DM总结数据集上的重要的添加收益，我们猜想更大的预训练模型可能从这个任务中学的更好。为了帮助这个模型更好地适应这个数据，我们在最后10%的训练中取消了dropout。我们使用的预训练数据包括160Gb的新闻，书，故事和网页文字。</p>
<p><img src="https://pic3.zhimg.com/80/v2-4aff4d8a4b767b84be746457c86c3916_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<h5 id="5-2-Discriminative-Tasks"><a href="#5-2-Discriminative-Tasks" class="headerlink" title="5.2 Discriminative Tasks"></a>5.2 Discriminative Tasks</h5><p>Table 2比较了BART和一些在SQuAD和GLUE任务中表现较好方法的性能。</p>
<p>最直接比较的baseline是RoBERTa，他们使用相同的方法去预训练，但是objectives不同。总的来说，BART表现和其他的模型类似，在大多数任务中只有少量不同。这表明BART对新一代任务的改进并不是以牺牲分类性能为代价的。</p>
<p><img src="https://pic2.zhimg.com/80/v2-91aa6bf74c7552923832ce0234ec3c55_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<h5 id="5-3-Generation-Tasks"><a href="#5-3-Generation-Tasks" class="headerlink" title="5.3 Generation Tasks"></a>5.3 Generation Tasks</h5><p>我们也实验了一些文本生成任务。BART被标准seq-to-seq模型从输入到输出进行微调。在微调期间，我们使用标签平滑交叉熵损失，平滑参数设为0.1.在生成期间，我们设施束的大小为5，删除波束搜索中的重复三元图，并在验证集上使用min-len,max-len, length penalty对模型进行了优化。</p>
<p><strong>总结</strong>（summarization 这里指总结任务）：为了在总结方面中提供一个和最先进水平的对比，我们列出了在两个有不同特征的总结数据集（CNN&#x2F;DailyMail，XSum）的结果。</p>
<p>在CNN&#x2F;DailyMail上总结趋向于类似原句子。提取模型（Extractive model）在这方面做得很好，但BART在所有的工作中表现是最优的。</p>
<p>和上一个数据集相反，XSum是高度抽象的，提取模型的性能很差。BART在先前的工作中表现做最好它利用了bert，在所有ROUGE指标上大约有6.0点——这代表了在这个问题上的一个显著的进步。在质量上，样品质量高(见第6部分)。</p>
<p>对话任务（Dialogue）：我们在CONVAI2上评估对话回应生成，其中，必须生成以前一个上下文和文本指定的角色为条件的响应。在两个自动化测度上，BART 表现性能比之前的工作更好。</p>
<p>抽象问答（Abstractive QA）我们使用最近提供的ELI5数据集去测试这个模型在生成长自由形式回答的能力。我们发现BART比之前表现最好的模型还要高1.2 ROUGE-L。但是数据集仍然具有挑战性，因为答案只是由问题弱指定的。</p>
<p><img src="https://pic2.zhimg.com/80/v2-c507bef3a7a49874cb98bbe0da60ad79_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p><img src="https://pic2.zhimg.com/80/v2-3019b15fa5a42b76bd3792738a3cc9b9_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<h5 id="5-4-Translation"><a href="#5-4-Translation" class="headerlink" title="5.4 Translation"></a>5.4 Translation</h5><p>我们还在增加了反向翻译的WMT16 Romanian-English数据集上评估性能，我们使用一个6层的transformer原编码器将Romanian映射为一个BART可以去噪成英文的表示。遵循3.4中提到的方法。实验结果如Table 6所示。比较我们的结果和baseline(transformer框架)，我们在固定的BART和调优的BART行中展示了模型的这两个步骤的性能。对于每一行我们在之前提到的增加了反向翻译的WMT16 Romanian-English数据集上进行实验，我们使用一个扇束宽度为5，长度惩罚为1，初步结果表明，如果没有反向翻译数据，我们的方法效率较低，而且容易过拟合——未来的工作应探索额外的正则化技术。</p>
<h4 id="6-Qualitative-Analysis"><a href="#6-Qualitative-Analysis" class="headerlink" title="6. Qualitative Analysis"></a><strong>6.</strong> <strong>Qualitative Analysis</strong></h4><p>BART在总结指标上显示出了很大的改进，比最先进的水平高出6个百分点。为了理解bart在自动化度量之外的表现，我们对它的生成器（generations）进行了定性分析。</p>
<p>Table 7展示了使用BART模型生成总结的模型。这些例子取自创建训练前语料库后发布的WikiNews文章，以消除描述的事件存在于模型训练数据中的可能性。我们在总结文章之前去掉了文章的第一句话，所以不容易提取文档的摘要。</p>
<p>文档的概要。不出所料，模型输出的是流利且符合语法的英语。然而，模型输出也是高度抽象的，很少从输入复制短语。输出通常在事实上是准确的，并将来自整个输入文档的支持证据与背景知识集成在一起(例如，正确填写姓名，或推断PG&amp;E在加利福尼亚运营)。在第一个例子中，推断鱼保护珊瑚礁免受全球变暖的影响需要从文本中进行非琐细的推理。然而，该研究发表在《科学》杂志上的说法并没有得到来源的支持。</p>
<p>这些样本表明，BART前训练已经学习了很强的自然语言理解和生成的结合。</p>
<h2 id="T5"><a href="#T5" class="headerlink" title="T5"></a>T5</h2><h3 id="论文解读-1"><a href="#论文解读-1" class="headerlink" title="论文解读"></a>论文解读</h3><p>相信大多 NLP 相关者，在时隔 BERT 发布近一年的现在，又被谷歌刚发布的 <strong>T5</strong> 模型震撼到了。又是一轮屠榜，压过前不久才上榜自家的ALBERT，登上 GLUE 榜首。</p>
<p>当然，最大的冲击还是财大气粗，bigger and bigger，但翻完它长达 34 页的论文，发现其中的分析无疑是诚意满满（都是钱）。类似这样的大型实验探索论文也有一些，首先<strong>提出一个通用框架，接着进行了各种比对实验，获得一套建议参数，最后得到一个很强的 baseline</strong>。而我们之后做这方面实验就能参考它的一套参数。</p>
<p>对于 T5 这篇论文，<em>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</em>，无疑也是类似的论文。它的意义不在烧了多少钱，也不在屠了多少榜（砸钱就能砸出来），其中 idea 创新也不大，它最重要作用是给<strong>整个 NLP 预训练模型领域提供了一个通用框架</strong>，把所有任务都转化成一种形式，正如论文里所说的</p>
<blockquote>
<p>introducing a unified framework that converts every language problem into a text-to-text format.</p>
</blockquote>
<p>之后未来做 NLP 实验时，可能就不再是自己怎么调一些模型了，而是无论什么任务，直接拿来一个超大预训练模型，然后<strong>主要工作就变成了怎么把任务转换成合适的文本输入输出</strong>，于是我们就成了带引号的”数据科学家“。而且可以用于多种任务，而模型对这些任务的区分只是根据你构建的输入输出形式，其实这让我想起 Jeff Dean 在某次谈话中谈到的谷歌未来方向，想做一个超级模型，什么任务都能直接处理，而它内部可以是稀疏的，或者可以局部 Distill，来对单独任务进行处理。</p>
<p>关于论文，作者们做了很多实验，如下图 </p>
<p><img src="https://pic4.zhimg.com/80/v2-e350f609979139b4fa9127b12a05d4e7_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>将近七十个实验，这也是大家吐槽财大气粗的原因，太有冲击力了，小家小业的话估计跑里面个小实验就够呛了。</p>
<p>正因为如此多实验，所以才对预训练模型中的大量技巧获得了一个较公平的比对和分析，但这也使得整篇论文长度巨长，读起来头晕。不是 idea 的冲击，而都是些琐碎细节，看了几大段后发现，还是看图表一目了然。</p>
<p>这里就简单介绍一下里面做了哪些实验，之后各取所需回看论文。</p>
<h4 id="Why-Text-to-Text？"><a href="#Why-Text-to-Text？" class="headerlink" title="Why Text-to-Text？"></a><strong>Why Text-to-Text？</strong></h4><p>首先为什么叫 T5 模型，因为是 <strong>Transfer Text-to-Text Transformer</strong> 的简写，和 XLNet 一样也不在芝麻街玩了，也有说法是吐槽谷歌 <strong>T5 Level</strong>（高级软件工程师）。</p>
<p>Transfer 来自 Transfer Learning，预训练模型大体在这范畴，Transformer 也不必多说，那么 Text-to-Text 是什么呢。那就是作者在这提出的一个统一框架，靠着大力出奇迹，<strong>将所有 NLP 任务都转化成 Text-to-Text （文本到文本）任务</strong>。</p>
<p><img src="https://pic2.zhimg.com/80/v2-82deada7be746017fe4d3808b6657af9_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>举几个例子就明白了，比如英德翻译，只需将训练数据集的输入部分前加上“translate English to German（给我从英语翻译成德语）” 就行。假设需要翻译”That is good”，那么先转换成 “translate English to German：That is good.” 输入模型，之后就可以直接输出德语翻译 “Das ist gut.”</p>
<p>再比如情感分类任务，输入”sentiment：This movie is terrible!”，前面直接加上 “sentiment：”，然后就能输出结果“negative（负面）”。</p>
<p>最神奇的是，对于需要输出连续值的 STS-B（文本语义相似度任务），居然也是直接输出文本，而不是加个连续值输出头。以每 0.2 为间隔，从 1 到 5 分之间分成 21 个值作为输出分类任务。比如上图中，输出 3.8 其实不是数值，而是一串文本，之所以能进行这样的操作，应该完全赖于 T5 模型强大的容量。</p>
<p>通过这样的方式就能将 NLP 任务都转换成 Text-to-Text 形式，也就可以<strong>用同样的模型，同样的损失函数，同样的训练过程，同样的解码过程来完成所有 NLP 任务</strong>。其实这个思想之前 GPT2 论文里有提，上斯坦福 cs224n 时 Socher 讲的 The Natural Language Decathlon 也有提。</p>
<h4 id="Data：C4-（Bomb-）"><a href="#Data：C4-（Bomb-）" class="headerlink" title="Data：C4 （Bomb!）"></a><strong>Data：C4 （Bomb!）</strong></h4><p>作者从 Common Crawl（一个公开的网页存档数据集，每个月大概抓取 20TB 文本数据） 里清出了 750 GB 的训练数据，然后取名为 ” Colossal Clean Crawled Corpus （超大型干净爬取数据）“，简称 C4，论作者取名之恶趣味。</p>
<p>大概清理过程如下：</p>
<ul>
<li>只保留结尾是正常符号的行；</li>
<li>删除任何包含不好的词的页面，具体词表参考**<a href="https://link.zhihu.com/?target=https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words">List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words</a>**库（笔者按：宝藏库，到里面转了一圈，看了看熟悉的几门语言，瞬间涨了不少新姿势 ）；</li>
<li>包含 Javascript 词的行全去掉；</li>
<li>包含编程语言中常用大括号的页面；</li>
<li>任何包含”lorem ipsum（用于排版测试）“的页面；</li>
<li>连续三句话重复出现情况，保留一个。</li>
</ul>
<h4 id="Architecture：The-Best-One"><a href="#Architecture：The-Best-One" class="headerlink" title="Architecture：The Best One"></a><strong>Architecture：The Best One</strong></h4><p>首先作者们先对预训练模型中的多种模型架构（Transformer）进行了比对，最主要的模型架构可以分成下面三种。</p>
<p><img src="https://pic2.zhimg.com/80/v2-b1a8d9af6110e6d1b6a7615fc300a229_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>第一种，<strong>Encoder-Decoder 型</strong>，即 Seq2Seq 常用模型，分成 Encoder 和 Decoder 两部分，对于 Encoder 部分，输入可以看到全体，之后结果输给 Decoder，而 Decoder 因为输出方式只能看到之前的。此架构代表是 MASS（今年WMT的胜者），而 BERT 可以看作是其中 Encoder 部分。</p>
<p>第二种， 相当于上面的 <strong>Decoder 部分</strong>，当前时间步只能看到之前时间步信息。典型代表是 GPT2 还有最近 CTRL 这样的。</p>
<p>第三种，<strong>Prefix LM（Language Model） 型</strong>，可看作是上面 Encoder 和 Decoder 的融合体，一部分如 Encoder 一样能看到全体信息，一部分如 Decoder 一样只能看到过去信息。最近开源的 UniLM 便是此结构。</p>
<p>上面这些模型架构都是 Transformer 构成，之所以有这些变换，主要是<strong>对其中注意力机制的 Mask 操作</strong>。</p>
<p><img src="https://pic1.zhimg.com/80/v2-b06b504f19febe0f1582f8b162cfbb9c_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>通过实验作者们发现，在提出的这个 Text-to-Text 架构中，Encoder-Decoder 模型效果最好。于是乎，就把它定为 T5 模型，因此<strong>所谓的 T5 模型其实就是个 Transformer 的 Encoder-Decoder 模型</strong>。</p>
<h4 id="Objectives：Search，Search，Search"><a href="#Objectives：Search，Search，Search" class="headerlink" title="Objectives：Search，Search，Search"></a><strong>Objectives：Search，Search，Search</strong></h4><p>之后是对预训练目标的大范围探索，具体做了哪些实验，下面这张图就能一目了然。</p>
<p><img src="https://pic3.zhimg.com/80/v2-247e53593f78282caf557d84c1d2c1fa_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>总共从四方面来进行比较。</p>
<p>第一个方面，<strong>高层次方法（自监督的预训练方法）对比</strong>，总共三种方式。</p>
<ol>
<li><strong>语言模型式</strong>，就是 GPT-2 那种方式，从左到右预测；</li>
<li><strong>BERT-style 式</strong>，就是像 BERT 一样将一部分给破坏掉，然后还原出来；</li>
<li>Deshuffling （顺序还原）式，就是将文本打乱，然后还原出来。</li>
</ol>
<p><img src="https://pic3.zhimg.com/80/v2-4188a5cef8a88085705b0e7cc9991ff2_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>其中发现 Bert-style 最好，进入下一轮。</p>
<p>第二方面，对文本一部分进行<strong>破坏时的策略</strong>，也分三种方法。</p>
<ol>
<li><strong>Mask 法</strong>，如现在大多模型的做法，将被破坏 token 换成特殊符如 [M]；</li>
<li><strong>replace span（小段替换）法</strong>，可以把它当作是把上面 Mask 法中相邻 [M] 都合成了一个特殊符，每一小段替换一个特殊符，提高计算效率；</li>
<li><strong>Drop 法</strong>，没有替换操作，直接随机丢弃一些字符。</li>
</ol>
<p><img src="https://pic4.zhimg.com/80/v2-f5b13a845911a7f57dec821cfe57713f_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>此轮获胜的是 <strong>Replace Span 法</strong>，类似做法如 SpanBERT 也证明了有效性。</p>
<p>当当当，进入下一轮。</p>
<p>第三方面，到底该<strong>对文本百分之多少进行破坏</strong>呢，挑了 4 个值，10%，15%，25%，50%，最后发现 BERT 的 <strong>15%</strong> 就很 ok了。这时不得不感叹 BERT 作者 Devlin 这个技术老司机直觉的厉害。</p>
<p>接着进入更细节，第四方面，因为 Replace Span 需要决定<strong>对大概多长的小段进行破坏</strong>，于是对不同长度进行探索，2，3，5，10 这四个值，最后发现 <strong>3</strong> 结果最好。</p>
<p>终于获得了完整的 T5 模型，还有它的训练方法。</p>
<ul>
<li>Transformer Encoder-Decoder 模型；</li>
<li>BERT-style 式的破坏方法；</li>
<li>Replace Span 的破坏策略；</li>
<li>15 %的破坏比；</li>
<li>3 的破坏时小段长度。</li>
</ul>
<p>到此基本上 T5 预训练就大致说完了，之后是些细碎探索。</p>
<h4 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a><strong>Datasets</strong></h4><p>接着作者们拿着 C4 数据集做了各种实验，比如说从里面分出各种类型的数据集，单独训练 T5 模型，之后看在下游任务的表现，发现一些情况<strong>领域内的预训练数据可以增强下游任务</strong>（想当然的）。而 C4 完整数据集因为数据太多太杂，可能反而不如这种领域内较少数据集。</p>
<p>还有从 C4 中抽出不同量数据做实验，发现<strong>数据少时，模型会记住数据所以之后表现会比较差</strong>（这个也是想当然）。</p>
<p><img src="https://pic3.zhimg.com/80/v2-9f03eee913d4674432d6d7fced11633e_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<h4 id="Training：Multi-Task-Learning"><a href="#Training：Multi-Task-Learning" class="headerlink" title="Training：Multi-Task Learning"></a><strong>Training：Multi-Task Learning</strong></h4><p>作者们之后又针对 MTDNN 给 T5 做了一系列类似训练，在一堆监督和非监督数据上进行预训练。</p>
<p>结果发现，只要<strong>混合训练比例调得OK，和前面说的非监督预训练性能差不多</strong>。</p>
<h4 id="Scaling：bigger-is-better"><a href="#Scaling：bigger-is-better" class="headerlink" title="Scaling：bigger is better?"></a><strong>Scaling：bigger is better?</strong></h4><p>接着又做了当放大模型某方面规模的相关实验，分别是增大模型，增大数据，还有在一定资源限制下的集成。</p>
<p>结论是，当<strong>这些因素放大时对性能都有提高，但其中大模型是最必要的</strong>。</p>
<h4 id="Models"><a href="#Models" class="headerlink" title="Models"></a><strong>Models</strong></h4><p>最后就是结合上面所有实验结果，训练了不同规模几个模型，由小到大：</p>
<ul>
<li>Small，Encoder 和 Decoder 都只有 6 层，隐维度 512，8 头；</li>
<li>Base，相当于 Encoder 和 Decoder 都用 BERT-base；</li>
<li>Large，Encoder 和 Decoder 都用 BERT-large 设置，除了层数只用 12 层；</li>
<li>3B（Billion）和11B，层数都用 24 层，不同的是其中头数量和前向层的维度。</li>
</ul>
<p>11B 的模型最后在 GLUE，SuperGLUE，SQuAD，还有 CNN&#x2F;DM 上取得了 SOTA，而 WMT 则没有。看了性能表之后，我猜想之所以会有 3B 和 11B 模型出现，主要是为了刷榜。看表就能发现</p>
<p><img src="https://pic4.zhimg.com/80/v2-802c0e96bf5ac1a341cc4e3c04a0372b_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>比如说 GLUE，到 3B 时效果还并不是 SOTA，大概和 RoBERTa 评分差不多都是 88.5，而把模型加到 11B 才打破 ALBERT 的记录。然后其他实验结果也都差不多，3B 时还都不是 SOTA，而是靠 11B 硬拉上去的。除了 WMT 翻译任务，可能感觉差距太大，要拿 SOTA 代价过大，所以就没有再往上提。根据这几个模型的对比，可以发现<strong>即使是容量提到 11B，性能提升的间隔还是没有变缓</strong>，<strong>因此我认为再往上加容量还是有提升空间</strong>。</p>
<h2 id="ELECTRA"><a href="#ELECTRA" class="headerlink" title="ELECTRA"></a>ELECTRA</h2><blockquote>
<p>ELECTRA模型是对BERT的一次改进，该改进主要体现在对样本的使用效率上。具体实现方式，是引入了比较像GAN的一种架构——首先，使用一个较小的generator（生成器）将随机mask掉的token再预测出来，然后再将重新修复后的句子交给discriminator（判别器）进行判断，判断input中每个单词是否有被generator替换过。相比之下，BERT是先对一部分token进行随机mask，然后再对被mask的词使用上下文进行预测，预测的样本空间是整个词表。最终经实验验证，ELECTRA在相同的算力、数据和模型参数的情况下，效果优于BERT；而和效果相近的RoBERTa和XLNet相比，ELECTRA消耗的计算量不到它们的1&#x2F;4；在相同的计算量的加持下，ELECTRA则要更优。</p>
</blockquote>
<p><img src="https://pic2.zhimg.com/80/v2-bdb84aefe5ed810e60afa0f7f63c5195_1440w.jpg" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<ul>
<li>论文地址：<a href="https://link.zhihu.com/?target=https://openreview.net/pdf?id=r1xMH1BtvB">https://openreview.net/pdf?id=r1xMH1BtvB</a></li>
<li>论文代码：<a href="https://link.zhihu.com/?target=https://github.com/google-research/electra">https://github.com/google-research/electra</a></li>
</ul>
<h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>现如今，效果优异的NLP预训练模型大多采用了MLM（masked language model）方法进行训练（BERT、XLNet等），尽管相比于传统语言模型，它们使用Transformer在正反两个方向上对语义进行学习，这已经提升了它们的学习效率；但由于这些模型只对input序列中的一小部分（一般是15%）进行学习，所以它们仍旧耗费了很多的算力。</p>
<p>为了进一步提升预训练语言模型的学习效率，我们提出了RTD（replaced token detection）任务作为MLM任务的替代品——这是一种架构有点类似于GAN的学习任务，首先通过一个较小的generator对BERT中的特殊token [MASK]进行替换，然后再训练一个discriminator对input中的每个单词进行预测，即模型会从input序列中的全部tokens进行学习，这有别于BERT的15%，而我们认为这也是使得ELECTRA训练比BERT更快的原因。如图1所示，ELECTRA总能使用更少的算力、更少的模型参数，达到比BERT等模型更好的结果。 </p>
<p><img src="https://pic1.zhimg.com/80/v2-19ee55181a3a60ac4c7d390d8c7194e8_1440w.jpg" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>图1 模型算力消耗对比图</p>
<h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p><img src="https://pic3.zhimg.com/80/v2-a689eeb8e5946de97b9286eebd92035a_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>图2 ELECTRA模型结构图</p>
<p>如图2所示，我们给出了该模型的结构图。generator可以是任意模型，它的任务是对被随机mask掉的token进行预测，它一般是一个较小的BERT模型，与discriminator一起进行训练，而discriminator的任务就是分辨出到底哪个token是被generator篡改过以致于和原token不一致的。尽管整个模型架构有点像GAN，但在我们的实验中发现，使用MLE的方式进行训练，效果会优于对抗式的训练方式[1]，具体原因后面会详述。在预训练完成后，我们会丢弃generator而保留discriminator（真正的ELECTRA model），参与下游的fine-tune任务。</p>
<p>具体来说，生成器G和判别器D是我们训练得到的两个神经网络，二者都包含encoder（即Transformer网络），从而将input序列从<strong>x</strong>映射到**h(x)**。而二者的任务又是不同的，所以我们会选择不同的损失函数对它们的错误进行衡量。具体公式如下：</p>
<p><img src="https://pic3.zhimg.com/80/v2-c7ef0ad1fee80c67fadd6172163bd13a_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p><img src="https://pic4.zhimg.com/80/v2-51d2af7d3d3762993bb2e26d5735c263_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>简单来说，公式1代表generator，其任务是根据上下文对被随机mask掉的token进行预测，所以采样空间是V（全部词表），因而损失函数也就是softmax损失；而discriminator的任务是对一个token是不是和原文中的一致进行判断，所以是个二分类任务，损失也就是交叉熵损失。</p>
<p><img src="https://pic2.zhimg.com/80/v2-48a1b4d743b6a1ab73d29731faa5a2b1_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>如图所示， xmasked 代表被随机mask过后的序列。原序列x长度为n，而其中k个token被特殊token [MASK]替换掉。而 xcorrupt代表被generator做了二次修改的序列。 xmasked序列的[MASK] token使用generator生成的结果进行替换，generator生成的结果遵循softmax分布。</p>
<p>最终，ELECTRA的最终损失函数是由生成器G的损失和判别器D的损失一同组成。因为生成器G规模较小而且任务相对更难，所以一般Loss比判别器G的Loss更大，而我们希望联合训练时同时关注generator和discriminator二者的loss，所以我们在这里给判别器D的loss加以系数λ。</p>
<p><img src="https://pic3.zhimg.com/80/v2-e5fa905b4b018176cb48a907af0148c6_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>此外，为何模型架构很像GAN但仍和GAN有三点明显的差别——</p>
<ol>
<li>若generator碰巧预测对了，经实验发现反而会对下游任务有好处；（这也就味着，在适当的区间内，generator更强，有助于discriminator的训练）</li>
<li>generator是用MLE训练的而非对抗训练方式，因为无法把loss传递回generator；</li>
<li>输入不是noise vector（GAN是这么干的）</li>
</ol>
<h3 id="主要的实验细节"><a href="#主要的实验细节" class="headerlink" title="主要的实验细节"></a>主要的实验细节</h3><h4 id="参数共享机制"><a href="#参数共享机制" class="headerlink" title="参数共享机制"></a>参数共享机制</h4><p>本文作者首先想到通过参数共享的方法减少参数，并提升参数的学习效率。当generator和discriminator规模相等时，二者之间可以将参数完全共享，即二者其实就是一套模型。然而经实验所得，更小的generator其实对结果更有帮助，所以我们只共享了generator和discriminator的embedding层的参数（包括token和position的embedding）。经实验，不进行参数共享GLUE score&#x3D;83.6，全部进行参数共享GLUE score&#x3D;84.4，而只在embedding层进行参数共享GLUE score&#x3D;84.3.究其原因，我们认为，因为判别器只会更新input序列中涉及到的token的参数，而生成器却会对词表中的全部单词进行权重更新，完全共享参数会导致浪费。</p>
<h4 id="更小的生成器"><a href="#更小的生成器" class="headerlink" title="更小的生成器"></a>更小的生成器</h4><p>如果让生成器和判别器的规模相当，则整个预训练的时间会大约变为原来的2倍。如图所示，我们发现最佳的生成器大小为判别器规模的1&#x2F;4~1&#x2F;2。左边图显示，当判别器规模不变的情况下，生成器的规模越大，反而可能会导致结果的下降——究其原因，可能是过强的生成器，可能会给出太难的题目，以致于让判别器无法学习。 </p>
<p><img src="https://pic3.zhimg.com/80/v2-26018dfe87c35752e341af5a0f862a66_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>图3 对比实验示意图</p>
<h4 id="训练方法"><a href="#训练方法" class="headerlink" title="训练方法"></a>训练方法</h4><p>文中主要提及了3种训练方法，分别是ELECTRA最终使用的联合训练方法、两步式训练方法和与GAN相似的对抗训练方法。下面我们先着重介绍一下两步式训练方法：</p>
<ol>
<li>首先，只对生成器连续训练n-steps；</li>
<li>使用生成器的参数对判别器进行初始化，然后连续训练判别器n-steps，与此同时保持生成器的参数不变。（注意，这要求生成器和判别器要有相同的规模）</li>
</ol>
<p>我们最终发现，如果在两步式训练中不使用参数共享，最终学得的判别器可能在很多类别上什么也没学到——作者猜测，这可能是因为generator启动速度比discriminator快得多，即生成器已经学会了初中知识并出初中题目，而判别器还只是小学生水平；此外，参数共享可能还在无意中让generator起到了“榜样作用”，给discriminator现身说法，展示如何从一个菜鸡到大佬的转变，这也可以在一定程度上解释为何训练换挡时GLUE score会有明显的跃升。</p>
<p>其次我们聊聊对抗式训练方法。从图3中的右图可以看出，对抗式训练方法（橘黄色实线）是不如传统的MLE训练方法的，原因可能有2点：</p>
<ol>
<li>对于RF而言，全部文本组成的action space太大了，造成其采样效率太低；</li>
<li>对抗训练得到的结果是低熵的输出分布，大多数的概率质量落在单个token上，导致generator的输出没有足够的多样性，这样的题目discriminator做起来没有足够的学习价值</li>
</ol>
<h3 id="模型效果对比"><a href="#模型效果对比" class="headerlink" title="模型效果对比"></a>模型效果对比</h3><h4 id="小模型之间的对比"><a href="#小模型之间的对比" class="headerlink" title="小模型之间的对比"></a>小模型之间的对比</h4><p><img src="https://pic4.zhimg.com/80/v2-fc8533240faa97a9a24c1c323955084b_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>图4 模型效果对比（基于GLUE的验证集)</p>
<p>如图4，图中所出现的BERT&#x2F;ELECTRA-Small，相比于BERT-Base，相关模型参数发生了如下缩减：其序列长度 512-&gt;128，batch size 256-&gt;128，hidden size 768-&gt;256，embedding size 768-&gt;128。从图中可见，在相同的计算量的情况下，ELECTRA-Small效果超越了BERT-Small（为了保证二者使用了相同的计算量，ELECTRA-Small训练了1M steps，而BERT-Small则是1.5M steps），而ELECTRA-Base同样也超越了BERT-Base。</p>
<h4 id="大模型之间的对比"><a href="#大模型之间的对比" class="headerlink" title="大模型之间的对比"></a>大模型之间的对比</h4><p><img src="https://pic1.zhimg.com/80/v2-f5dbcef2795c28265208a717dfc5f898_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>图5 与RoBERTa&#x2F;XLNet对比（基于GLUE验证集）</p>
<p>如图5所示，ELECTRA-400k在达到与RoBERTa-500k和XLNet相近的效果的情况下，算力消耗却只是其1&#x2F;4不到；而当我们使用足够的计算量时，ELECTRA模型在大部分任务上都达到了sota。</p>
<h4 id="模型学习效率的分析"><a href="#模型学习效率的分析" class="headerlink" title="模型学习效率的分析"></a>模型学习效率的分析</h4><p>作者前文提到，ELECTRA相对于BERT的效果和学习效率的提升，可能来自于ELECTRA预测了全部input tokens（BERT是15%），以及BERT存在的由于引入[MASK]特殊token导致的“miss-match”问题不存在于ELECTRA。鉴于此，作者对这一问题进行了更深入的分析。</p>
<p>作者提出三个实验模型，分别是：</p>
<ol>
<li>ELECTRA 15%：让判别器只计算input中15% token的损失（降低ELECTRA对样本的使用效率）；</li>
<li>Replace MLM：训练BERT MLM，其中预训练中input里的[MASK]使用generator生成的token进行替换（探究只在预训练中才出现[MASK]对模型的影响）；</li>
<li>All-Tokens MLM：接着用Replace MLM，只不过BERT的目标函数变为预测所有的token，比较接近ELECTRA。</li>
</ol>
<p><img src="https://pic2.zhimg.com/80/v2-f7471e75ffe65eb5941059f3967c8f21_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>图6 模型学习效率实验</p>
<p>三者的效果如图6所示，我们可以看出：</p>
<ol>
<li>ELECTRA 15%比ELECTRA效果差得多，说明在全部tokens上学习确实对结果有很大提升；</li>
<li>Replace MLM效果比BERT略好，这说明miss-match问题确实会对BERT有所伤害。如果BERT中没有使用10%[MASK]替换的trick，作者估计其效果还要再差一点。</li>
<li>All-Tokens MLM的效果是所有实验模型中最接近ELECTRA的，这说明对全词的预测确实会提升BERT的效果</li>
</ol>
<p>以上实验结果说明，ELECTRA相比于BERT带来的学习效率的提升主要来自于对全部token的学习，次要来自于消弭了预训练和fine-tune之间的miss-match。此外，All-Tokens MLM和ELECTRA之间的差距也显示了ELECTRA比BERT不仅仅提升在“学得更快”上。从图7的左和中图可见，模型规模越小，ELECTRA对BERT的效果优势越大。而图7的右图显示，对于同样的小模型而言，将二者都训练完全直至收敛，ELECTRA-256会得到比BERT-256更好的下游任务效果。</p>
<p>我个人对这个实验的理解是这样的。相比之下，BERT模型会对全部不同语境下的token进行学习，然后全部“记录在”其大脑（hidden state）中；而ELECTRA像是走了捷径，它最终任务是二分类，更简洁却也更贫乏，因为最终的embedding表示空间会被压缩到两个空间中去。所以形象地理解“模型规模越小、ELECTRA优势更大”这一现象，似乎可以用“烧脑论”来解释，如果你脑容量够大，那么就足够学习并“记住”丰富的BERT习得的词表示；但如果你脑容量不足，那还不如选择简洁明快的ELECTRA，这就好比考试是只背公式和重点，还是从头开始，知其然也知其所以然吧。</p>
<p><img src="https://pic4.zhimg.com/80/v2-a3cc541fae8ebfa48caaab28fc65257b_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>图7 对比BERT和ELECTRA</p>
<h3 id="ELECTRA优缺点的探究和总结"><a href="#ELECTRA优缺点的探究和总结" class="headerlink" title="ELECTRA优缺点的探究和总结"></a>ELECTRA优缺点的探究和总结</h3><h4 id="作者尝试过但失败了的方案"><a href="#作者尝试过但失败了的方案" class="headerlink" title="作者尝试过但失败了的方案"></a>作者尝试过但失败了的方案</h4><p>鉴于前文已证，（在一定范围内）较小或者较弱的generator会对结果有所帮助，所以我们尝试过两种继续削弱generator的方案：一是故意阻止generator采样到正确token，即在采样时从词表中删去正确token；二是提升generator输出的温度，从而降低正确token输出的置信度。结果发现它们都对结果没有帮助。</p>
<p>根据RTD任务，我们还开发了一个和其相似的、句子层面的任务，即我们保证input序列中的20%没有被generator修改过，然后使用discriminator对input的句子是否被修改过进行判断和学习。令作者惊讶的是，这反而会对下游任务效果产生伤害。</p>
<h4 id="其他读者发现的或认为的优缺点"><a href="#其他读者发现的或认为的优缺点" class="headerlink" title="其他读者发现的或认为的优缺点"></a>其他读者发现的或认为的优缺点</h4><p>【<strong>优点</strong>】</p>
<ol>
<li><strong>任务难度的提升，是导致ELECTRA优于BERT的一大原因。</strong>对于BERT的MLM任务而言，它是随机mask token；而对于ELECTRA，相当于通过generator来“选择出”更难的token，再交给discriminator去学习。这就好比，对于相同的input sequence：“我想吃苹果”，普通的MLM方法给出的mask结果可能是”我想吃苹[MASK]”，而ELECTRA则是”我[MASK]吃苹果”，显而易见后者的预测更可能出错、也就对应了更大的loss，而模型是通过loss（错误）来学习的。所以generator其实相当于一个“难题筛选器”，将更难的任务交给discriminator，从而更好地锻炼discriminator的能力；（难题让你变得更好效果）</li>
<li><strong>将主任务从V分类变成二分类，算法复杂的下降</strong>。对于BERT而言，其MLM任务是在整个样本空间V中进行采样，而ELECTRA只需要进行二分类，不需要对全体数据的分布进行建模。而作者也对BERT和ELECTRA的小模型进行了对比，证明全体参数最终其实是被完整地训练了；（没有吃顶了效果）</li>
<li><strong>增加了被预测token的自身信息利用</strong>。MLM中，被mask的token的预测是不会用到其本身的信息的（我们认为其信息全部蕴含在其上下文中），而ELECTRA的RTD任务则是通过上下文和待预测token自身一起进行预测，输入信息量更多、且相对更直接。</li>
</ol>
<p>【<strong>缺点</strong>】</p>
<ol>
<li><strong>判别器的二分类属性，导致其可能不适用于下游任务</strong>。BERT使用MLM进行预训练然后接下游任务，看起来是足够合理的，因为我们可以认为MLM对全词采样的过程中，其实是给所有token都建立了一个基于其上下文的词表示；然而判别器任务是二分类，也就是将token的表示划分到两个空间中，可能会导致其hidden space中信息的过早退化。知乎上有用户发现ELECTRA模型对NER任务不太友好，因而面对复杂的下游任务时，ELECTRA判别器生成的表示可能不够丰富。此外，判别器本身是用二分类任务来预训练的，所以当它面对“接近二分类”的任务时会明显有所助益（如GLUE的CoLA任务），但面对诸如序列标注、文本生成等不那么“分类”的任务时，效果可能就较差。</li>
</ol>
<h2 id="Longformer"><a href="#Longformer" class="headerlink" title="Longformer"></a>Longformer</h2><h2 id="DeBERTa"><a href="#DeBERTa" class="headerlink" title="DeBERTa"></a>DeBERTa</h2><h3 id="论文解读-2"><a href="#论文解读-2" class="headerlink" title="论文解读"></a>论文解读</h3><h4 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h4><p>预训练的神经语言模型的最新进展已显着提高了许多自然语言处理(NLP)任务的性能。在本文中，我们提出了一种新的模型架构DeBERTa(解耦注意力解码增强BERT)，它使用两种新颖的技术改进了BERT和RoBERTa模型。首先是解耦注意力机制，其中对每个单词分别使用其内容和位置的编码向量表示，单词间的注意力权重使用解耦矩阵分别计算其内容和相对位置。其次，增强的mask解码器用于在解码层中合并绝对位置，以预测模型预训练中的mask token。此外，一种新的虚拟对抗训练方法用于微调，以改善模型的通用性。我们表明，这些技术显着提高了模型预训练的效率以及自然语言理解(NLU)和自然语言生成(NLG)下游任务的性能。与RoBERTaLarge相比，在一半的训练数据上训练的DeBERTa模型在各种NLP任务上始终表现更好，对MNLI的改进为+0.9％(90.2％对91.1％)，对SQuAD v2.0的改进为+2.3％(88.4％vs. 90.7％)和RACE + 3.6％(83.2％vs. 86.8％)。值得注意的是，我们通过训练更大的版本来扩展DeBERTa，该版本由48个包含15亿个参数的Transform层组成。显着的性能提升使单个DeBERTa模型在macro-average上得分(89.9与89.8)，首次超过了(Wang等人，2019a)SuperGLUE基准上的人类性能，而组合的DeBERTa模型截至2021年1月6日，在SuperGLUE排行榜的表现优于人类基线(90.3对89.8)。我们将向公众发布DeBERTa模型和源代码。 </p>
<p>Transformer已成为用于神经语言模型的最有效的神经网络体系结构。与按顺序处理文本的循环神经网络(RNN)不同，transformer应用self-attention功能来并行计算输入文本中的每个单词的注意力权重，从而衡量每个单词对另一个单词的影响，因此与RNN相比，它可以并行大规模模型训练(Vaswani et al.,2017)。自2018年以来，我们已经看到了一系列基于transformer的大规模预训练语言模型(PLM)的兴起，例如GPT(Radford等人，2019; Brown等人，2020)，BERT(Devlin等人)。等人，2019)，RoBERTa(Liu等人，2019c)，XLNet(Yang等人，2019)，UniLM(Dong等人，2019)，ELECTRA(Clark等人，2020)，T5(Raffel等人)等人，2020)，ALUM(Liu等人，2020)，StructBERT(Wang等人，2019c)和ERINE(Sun等人，2019)。这些PLM使用特定任务label进行了微调，并在许多下游自然语言处理(NLP)任务中创造了新的技术水平(Liu等人，2019b； Minaee等人，2020； Jiang等人，2020 ; He等人，2019a; b; Shen等人，2020)。 </p>
<p>在本文中，我们提出了一种新的基于Transformer的神经语言模型DeBERTa(具有解耦注意力的解码增强BERT)，它使用两种新颖的技术改进了以前的最新PLM：<strong>解耦注意力机制和增强的mask解码器。</strong> </p>
<p><strong>解耦注意力:</strong> 与BERT不同，在BERT中，输入层中的每个单词都使用一个向量来表示，该向量是其单词(内容)嵌入和位置嵌入的总和， 单词间的权重分别根据其内容和相对位置使用解耦的矩阵进行计算。 这是由于观察到一个单词对的注意力权重不仅取决于它们的内容，而且还取决于它们的相对位置。 例如，单词“deep”和“learning”相邻时的的关系要强于出现在不同句子中。 </p>
<p><strong>增强的mask解码器:</strong> 与BERT一样，DeBERTa也使用mask语言模型(MLM)进行了预训练。 MLM是一项填空任务，在该任务中，模型被教导要使用mask token周围的单词来预测mask的单词应该是什么。 DeBERTa将上下文的内容和位置信息用于MLM。解耦注意力机制已经考虑了上下文词的内容和相对位置，但没有考虑这些词的绝对位置，这在很多情况下对于预测至关重要。考虑一下句子“a new store opened beside the new mall”，其斜体字“store”和“mall”被mask以进行预测。尽管两个单词的局部上下文相似，但是它们在句子中扮演的句法作用不同。 (这里，句子的主题是“store”而不是“mall”。)这些句法上的细微差别在很大程度上取决于单词在句子中的绝对位置，因此考虑单词在语言模型中的绝对位置是很重要的。 DeBERTa在softmax层之前合并了绝对单词位置嵌入，在该模型中，模型根据词内容和位置的聚合上下文嵌入对Masked单词进行解码。 </p>
<p>此外，我们提出了一种新的<strong>虚拟对抗训练</strong>方法，用于将PLM微调到下游NLP任务。 该方法可有效改善模型的泛化性能。 </p>
<p>我们通过全面的实证研究表明，这些技术大大提高了预训练的效率和下游任务的性能。在NLU任务中，与RoBERTa-Large相比，在一半的训练数据上训练的DeBERTa模型在各种NLP任务中始终表现更好，对MNLI的改进为+ 0.9％(90.2％对91.1％)。 SQuAD v2.0改进为+2.3％(88.4％对90.7％)，RACE增长了+ 3.6％(83.2％对86.8％)。在NLG任务中，DeBERTa在Wikitext-103数据集上的困惑度从21.6降低到19.5。我们通过预训练由48个包含15亿个参数的Transformer层组成的更大模型来进一步扩展DeBERTa。单个15亿参数的DeBERTa模型在SuperGLUE基准上(Wang等人，2019a)大大优于110亿个参数的T5(0.6％(89.3％对89.9％))，并首次超过了人类基线(89.9对89.8)。截至2021年1月6日，集成的DeBERTa模型位于SuperGLUE排行榜的顶部，比人类的基线要高出很多(90.3对89.8)。 </p>
<h4 id="二、背景"><a href="#二、背景" class="headerlink" title="二、背景"></a>二、背景</h4><p>基于transformer的语言模型由堆叠的transformer块组成(Vaswani等，2017)。每个块都包含一个多头self-attention层，后面是一个全连接的位置前馈网络。标准的self-attention机制缺乏编码单词位置信息的自然方法。因此，现有的方法给每个输入词的嵌入增加了位置偏差，使得每个输入词由一个向量表示，其值取决于其内容和位置。位置偏差可以使用绝对位置嵌入(Vaswani等，2017; Radford等，2019; Devlin等，2019)或相对位置嵌入(Huang等，2018; Yang等，2019)来实现)。研究表明，相对位置表示对于自然语言理解和生成任务更有效(Dai等，2019; Shaw等，2018)。提出的解解耦注意力机制与所有现有方法的不同之处在于，我们表示每个输入词，通过使用两个的对词的内容和位置进行编码的单独向量，并使用解耦的矩阵分别计算其内容和相对位置来计算词间的注意力权重。 </p>
<h5 id="2-2-mask语言模型"><a href="#2-2-mask语言模型" class="headerlink" title="2.2 mask语言模型"></a>2.2 mask语言模型</h5><p>基于transformer的PLM通常在大量文本上进行预训练，称为mask语言模型(MLM)模型，使用自监督目标来学习上下文单词表示(Devlin等。2019)。 具体来说，给定序列 X&#x3D;{xi} ，我们通过随机masking其15％的token将其分解为 X~ ，然后训练由θ参数化的语言模型，通过预测以˜X为条件的被masking的token x~ 来重构 X~ ： </p>
<p>(1)maxθlog⁡pθ(X∣X<del>)&#x3D;maxθ∑i∈Clog⁡pθ(x</del>i&#x3D;xi∣X~)</p>
<p>其中C是序列中被mask token的索引集。BERT的作者提议保持10％的mask token不变，另外10％的token由随机选择的token代替，其余的token由[MASK] token代替。 </p>
<h4 id="三、-DEBERTA架构"><a href="#三、-DEBERTA架构" class="headerlink" title="三、 DEBERTA架构"></a>三、 DEBERTA架构</h4><h5 id="3-1-注意力解耦：内容和位置嵌入的2向量方法"><a href="#3-1-注意力解耦：内容和位置嵌入的2向量方法" class="headerlink" title="3.1 注意力解耦：内容和位置嵌入的2向量方法"></a>3.1 注意力解耦：内容和位置嵌入的2向量方法</h5><p>对于序列中位置i处的token，我们使用两个向量， {Hi} 和 {Pi|j} 表示它，它们分别表示其内容和与位置j处的token的相对位置。 token i和j之间的交叉注意力得分的计算可以分解为四个部分:</p>
<p>(2)Ai,j&#x3D;{Hi,Pi∣j}×{Hj,Pj∣i}⊤&#x3D;HiHj⊤+HiPj∣i⊤+Pi∣jHj⊤+Pi∣jPj∣i⊤</p>
<p>也就是说，<strong>一个单词对的注意力权重可以使用其内容和位置的解耦的矩阵计算为四个注意力(内容到内容，内容到位置，位置到内容和位置到位置)的得分的总和。</strong></p>
<p>现有的相对位置编码方法在计算注意力权重时使用单独的嵌入矩阵来计算相对位置偏差(Shaw等人，2018; Huang等人，2018)。 这等效于仅使用等式2中的“内容到内容”和“内容到位置”来计算注意力权重。我们认为位置到内容也很重要，因为单词对的注意力权重不仅取决于它们的内容。根据它们的相对位置，只能使用内容到位置和位置到内容进行完全建模。 <strong>由于我们使用相对位置嵌入，因此位置到位置项不会提供太多附加信息，因此在我们的实现中将其从等式2中删除。</strong> </p>
<p>以单头注意力为例，标准的self-attention操作(Vaswani等，2017)可以表述为：</p>
<p>(3)Q&#x3D;HWq,K&#x3D;HWk,V&#x3D;HWv,A&#x3D;QK⊤dHo&#x3D;softmax⁡(A)V</p>
<p>其中， H∈RN×d 代表输入隐藏向量， Ho∈RN×d 表示self-attention输出， Wq,Wk,Wv∈Rd×d 为投影矩阵， A∈RN×N 为注意力矩阵，N为输入序列的长度，d为隐藏层的维度。</p>
<p>将k表示为最大相对距离，将 δ(i,j)∈[0,2k) 表示为从token i到token j的相对距离，定义为：</p>
<p>(3)δ(i,j)&#x3D;{0 for i−j⩽−k2k−1 for i−j⩾ki−j+k others. </p>
<p>我们可以用等式4表示具有相对位置偏差的解耦的self-attention，其中Qc，Kc和Vc是分别使用投影矩阵 Wq,c,Wk,c,Wv,c∈Rd×d 生成的<strong>投影内容</strong>向量， P∈R2k×d 表示所有层之间共享的<strong>相对位置</strong>嵌入向量(即，在向前传播期间保持固定)，以及Qr和Kr分别是使用投影矩阵 Wq,r,Wk,r,Wv,r∈Rd×d 生成的投影相对位置向量。 </p>
<p>(4)Qc&#x3D;HWq,c,Kc&#x3D;HWk,c,Vc&#x3D;HWv,c,Qr&#x3D;PWq,r,Kr&#x3D;PWk,rA<del>i,j&#x3D;QicKjc⊤⏟(a) content-to-content +QicKδ(i,j)r⏟(b) content-to-position +KjcQδ(j,i)r⏟(c) position-to-content Ho&#x3D;softmax⁡(A</del>3d)Vc</p>
<p>A<del>i,j 是注意力矩阵 A</del> 的元素，表示从token i到token j的注意力得分。 Qic 是Qc的第i行。 Kjc 是Kc的第j行。 Kδ(i,j)r 是关于相对距离 δ(i,j) 的Kr的第 δ(i,j) 行。 Qδ(j,i)r 是关于相对距离 δ(j,i) 的Qr的第 δ(j,i) 行。 请注意，此处我们使用δ(j，i)而不是δ(i，j)。 这是因为对于给定的位置i，位置到内容计算是对于i处的query位置和j处的key内容的注意力权重，因此相对距离为 δ(j,i) 。 位置到内容的计算公式为 KjcQδ(j,i)r⊤ 。 内容到位置的计算方法与此类似。 </p>
<p>最后，我们将缩放因子 13d 应用在 A~ 上。 该因子对于稳定模型训练非常重要(Vaswani等人，2017)，尤其是对于大型PLM。 </p>
<p><img src="https://pic2.zhimg.com/80/v2-ed5beda068e549e51c81ad3b8e457c59_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>算法1： 解耦注意力</p>
<h4 id="3-1-1-有效实现"><a href="#3-1-1-有效实现" class="headerlink" title="3.1.1 有效实现"></a>3.1.1 有效实现</h4><p>对于长度为N的输入序列，需要 O(N2d) 的空间复杂度(Shaw等人，2018; Huang等人，2018; Dai等人，2019)来存储每个token的相对位置嵌入。 但是，以内容到位置为例，我们注意到由于 δ(i,j)∈[0,2k) 和所有可能的相对位置的嵌入始终是 Kr∈R2k×d 的子集，因此我们可以对于所有quiries在注意力计算中重用Kr。 </p>
<p>在我们的实验中，我们将最大相对距离k设置为512，以进行预训练。可以使用算法1高效地计算出解耦的注意力权重。令δ是根据等式3的相对位置矩阵，即 δ[i,j]&#x3D;δ(i,j) 。我们没有为每个query分配不同的相对位置嵌入矩阵，而是将每个query向量 Qc[i,:] 乘以 KrT∈Rd×2k ，如第3-5行所示。然后，我们使用相对位置矩阵δ作为索引，如第6-10行中所示。为计算位置到内容的注意力得分 A<del>p→c[:,j] ，即注意力矩阵 A</del>p→c 的列向量，如第11-13行所述，通过 QrT 乘以每个key向量 Kc[j,:] 。最后，如第14-18行那样，我们通过相对位置矩阵δ提取相应的注意力得分作为索引。不需要分配内存来存储每个query的相对位置嵌入，从而降低了空间复杂度O(kd)(对于存储Kr和Qr)。 </p>
<h4 id="3-2-绝对单词位置的增强型mask解码器"><a href="#3-2-绝对单词位置的增强型mask解码器" class="headerlink" title="3.2 绝对单词位置的增强型mask解码器"></a>3.2 绝对单词位置的增强型mask解码器</h4><p>DeBERTa是使用MLM进行预训练的，在该模型中，模型被训练为使用mask token周围的单词来预测mask词应该是什么。 DeBERTa将上下文的内容和位置信息用于MLM。 <strong>解耦注意力机制已经考虑了上下文词的内容和相对位置，但没有考虑这些词的绝对位置</strong>，这在很多情况下对于预测至关重要。 </p>
<p>给定一个句子“a new store opened beside the new mall”，并用“store”和“mall”两个词mask以进行预测。 仅使用局部上下文(例如，相对位置和周围的单词)不足以使模型在此句子中区分store和mall，<strong>因为两者都以相同的相对位置在new单词之后。 为了解决这个限制，模型需要考虑绝对位置，作为相对位置的补充信息。</strong> 例如，句子的主题是“store”而不是“mall”。 这些语法上的细微差别在很大程度上取决于单词在句子中的绝对位置。 </p>
<p><strong>有两种合并绝对位置的方法。 BERT模型在输入层中合并了绝对位置。 在DeBERTa中，我们在所有Transformer层之后将它们合并</strong>，然后在softmax层之前进行mask token预测，如图2所示。这样，DeBERTa捕获了所有Transformer层中的相对位置，仅将绝对位置用作补充信息。 解码被mask的单词时。 因此，我们将DeBERTa的解码组件称为增强型Masked解码器(EMD)。 在实证研究中，我们比较了合并绝对位置的这两种方法，并观察到EMD效果更好。 我们推测，BERT所使用的绝对位置的早期合并可能会不利地阻碍模型学习足够的相对位置信息。 此外，EMD还使我们能够引入除位置以外的其他有用信息，以进行预训练。 我们将其留给以后的工作。</p>
<p>除NLU任务外，DeBERTa还可以扩展为处理NLG任务。 为了使DeBERTa像自回归模型一样操作以生成文本，我们使用了一个三角形矩阵进行self-attention，并按照Dong等人(2019)的方法将self-attention Masked的上三角部分设置为负无穷。 </p>
<h4 id="四、规模不变微调"><a href="#四、规模不变微调" class="headerlink" title="四、规模不变微调"></a>四、规模不变微调</h4><p>本节介绍了一种新的虚拟对抗训练算法， Scale-invariant-Fine-Tuning 不变微调(SiFT)，它是Miyato等人(Jiang et al2020)中描述的算法的一种变体，用于微调。 </p>
<p>虚拟对抗训练是一种改进模型泛化的正则化方法。 它通过对抗性样本提高模型的鲁棒性，对抗性样本是通过对输入进行细微扰动而创建的。 对模型进行正则化，以便在给出特定于任务的样本时，该模型产生的输出分布与该样本的对抗性扰动所产生的输出分布相同。 </p>
<p>对于NLP任务，扰动将应用于单词嵌入，而不是原始单词序列。 但是，嵌入向量的value范围(范数)在不同的单词和模型之间有所不同。 对于具有数十亿个参数的较大模型，方差会变大，从而导致对抗训练有些不稳定。 </p>
<p>受层归一化的启发(Ba et al.,2016)，我们提出了SiFT算法，该算法通过应用扰动的归一化的词嵌入来提高训练稳定性。 具体来说，在我们的实验中将DeBERTa微调到下游NLP任务时，<strong>SiFT首先将单词嵌入向量归一化为随机向量，然后将扰动应用于归一化的嵌入向量。</strong> 我们发现，归一化大大改善了微调模型的性能。 对于较大的DeBERTa模型，此改进更为突出。 我们将SiFT的全面研究留给未来的工作。 </p>
<h4 id="五、实验"><a href="#五、实验" class="headerlink" title="五、实验"></a>五、实验</h4><p>本节报告DeBERTa关于各种NLU和NLG任务的结果。 </p>
<h5 id="5-1-NLU任务的主要结果"><a href="#5-1-NLU任务的主要结果" class="headerlink" title="5.1 NLU任务的主要结果"></a>5.1 NLU任务的主要结果</h5><p>根据先前对PLM的研究，我们使用large模型和base模型报告结果。 </p>
<h5 id="5-1-1-大型模型的性能"><a href="#5-1-1-大型模型的性能" class="headerlink" title="5.1.1 大型模型的性能"></a>5.1.1 大型模型的性能</h5><p>我们使用BERT设置对大型模型进行预训练(Devlin等，2019)，只是我们使用Radford等(2019)的BPE单词表。 对于训练数据，我们使用Wikipedia(英语Wikipedia dump2; 12GB)，BookCorpus(Zhu等人，2015)(6GB)，OPENWEBTEXT(公开Reddit内容(Gokaslan＆Cohen，2019); 38GB)和STORIES(以下内容的子集) CommonCrawl(Trinh＆Le，2018); 31GB)。 重复数据删除后的总数据大小(Shoeybi et al.,2019)约为78G。 有关预训练数据集的详细说明，请参阅附录A.2。 </p>
<p>我们使用6台DGX-2机器(96个V100 GPU)来训练模型。 一个训练有2K批次大小和1M步长的模型大约需要20天。 有关详细的超参数，请参阅附录A。 </p>
<p>我们在表1中总结了GLUE的8个NLU任务的结果(Wang等人，2019b)，其中将DeBERTa与以前基于类似结构的基于Transform的PLM进行了比较(即24层，hidden size为1024)，包括BERT，RoBERTa，XLNet，ALBERT和ELECTRA。请注意，RoBERTa，XLNet和ELECTRA在160G训练数据上进行了预训练，而DeBERTa在78G训练数据上进行了预训练。 RoBERTa和XLNet预训练了500K步，每步中有8K样本，总计40亿个训练样本。对DeBERTa进行了100万步的预训练，每步2K样本。这相当于20亿训练样本，大约是RoBERTa或XLNet的一半。表1显示，与BERT和RoBERTa相比，DeBERTa在所有任务上的性能始终如一。同时，DeBERTa在八项任务中有六项的表现优于XLNet。特别是，MRPC(比XLNet高1.1％，比RoBERTa高1.0％)，RTE(比XLNet高2.4％，比RoBERTa高1.7％)和CoLA(比XLNet高1.5％，比RoBERTa高2.5％)的改善显着。就平均GLUE得分而言，DeBERTa还优于其他SOTA PLM，即ELECTRAlarge和XLNetlarge)</p>
<p><img src="https://pic3.zhimg.com/80/v2-89f23f1b6c254bf818042f036a097b2a_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>表1：GLUE开发集的比较结果</p>
<p>在所有GLUE任务中，MNLI通常被用作指示性任务，以监控PLM的研究进度。 DeBERTa明显优于MNLI上所有现有的类似大小的PLM，并创造了新的技术水平。</p>
<p>除了GLUE之外，DeBERTa还根据NLU基准的三类进行了评估：(1)问答：SQuAD v1.1(Rajpurkar等，2016)，SQuAD v2.0(Rajpurkar等.,2018)，RACE(Lai) 等(2017)，ReCoRD(Zhang等，2018)和SWAG(Zellers等，2018); (2)自然语言推论：MNLI(Williams等，2018); (3)NER：CoNLL-2003。 为了进行比较，我们包括ALBERT xxlarge(Lan等人，2019)3和Megatron(Shoeybi等人，2019)，它们具有三种不同的模型尺寸，分别表示为Megatron336M，Megatron1.3B和Megatron3.9B，使用 与RoBERTa相同的数据集。 请注意，Megatron336M具有与上述其他模型相似的模型。 </p>
<p><strong>ALBERTxxlarge的隐藏维度是DeBERTa的4倍，计算开销大约是DeBERTa的4倍。</strong></p>
<p>T5(Raffel等人，2020年)具有更多参数(11B)。 Raffel等人(2020)仅报告了T5的测试结果，无法与其他模型进行比较。</p>
<p>我们将结果汇总在表2中。与具有类似模型尺寸(即BERT，RoBERTa，XLNet，ALBERTlarge和Megatron336M)的先前SOTA PLM相比，DeBERTa在所有七个任务中均表现出卓越的性能。 以RACE基准测试为例，DeBERTa的表现明显优于XLNet + 1.4％(86.8％对85.4％)。 尽管Megatron1.3B的体积是DeBERTa的三倍，但DeBERTa在四个基准测试中的三个中均胜过它。 </p>
<p><img src="https://pic1.zhimg.com/80/v2-c6c38a45faac35d86e43556df3932bf8_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>表2：关于MNLI in&#x2F;out-domain，SQuAD v1.1，SQuAD v2.0，RACE，ReCoRD，SWAG和CoNLL 2003 NER开发集的结果。 请注意，文献中缺少的结果以“-”表示。 </p>
<h5 id="5-1-2-基本模型的性能"><a href="#5-1-2-基本模型的性能" class="headerlink" title="5.1.2 基本模型的性能"></a>5.1.2 基本模型的性能</h5><p>我们的基本模型预训练设置与大型模型的设置相似。 基本模型结构遵循BERT基本模型的结构，即L&#x3D;12，H&#x3D;768，A&#x3D;12。我们使用带有64个V100 GPU的4个DGX-2来训练基本模型。 完成一次预训练，批次大小为2048的1M训练步骤需要10天。我们使用相同的78G数据集训练DeBERTa，并将其与通过160G文本数据训练的RoBERTa和XLNet进行比较。</p>
<p>我们在表3中总结了基本模型的结果。在所有三个任务中，DeBERTa始终以比大型模型更大的幅度胜过RoBERTa和XLNet。 例如，在MNLI-m上，DeBERTabase比RoBERTabase获得+ 1.2％(88.8％对87.6％)，比XLNetbase获得+2％(88.8％对86.8％)的提升。 </p>
<p><img src="https://pic2.zhimg.com/80/v2-814e0f7c5fe777eb37fe7e6b6e1f013d_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>表3：关于MNLIin&#x2F;out-domain (m &#x2F; mm)，SQuAD v1.1和v2.0开发集的结果。</p>
<h5 id="5-2-生成任务的主要结果"><a href="#5-2-生成任务的主要结果" class="headerlink" title="5.2 生成任务的主要结果"></a><strong>5.2 生成任务的主要结果</strong></h5><p>我们进一步使用Wikitext103对DeBERTa的自回归语言模型(ARLM)任务进行了评估(Merity等，2016)。 为此，我们训练了DeBERTa的新版本，称为DeBERTa-MT。 与UniLM中一样，它使用MLM和ARLM(自回归语言模型)任务进行了联合预训练(Dong等人，2019)。 预训练的超参数遵循DeBERTabase的方法，但我们使用较少的训练步骤(200k)。 为了进行比较，我们使用RoBERTa作为基准，并包括GPT-2和Transformer-XL作为其他参考。DeBERTa-AP是DeBERTa的一种变体，其中绝对位置嵌入作为RoBERTa集成在输入层中。 为了公平地比较，所有这些模型都是在类似环境中预训练的基本模型。 </p>
<p>表4总结了Wikitext-103上的结果。 我们看到DeBERTabase在开发和测试数据上都获得了较低的困惑，而使用MLM和ARLM的联合训练进一步降低了困惑。 DeBERTa-AP不如DeBERTa表明， 将单词的绝对位置在解码层合并比在输入层中将绝对嵌入合并更为有效。</p>
<p><img src="https://pic3.zhimg.com/80/v2-b174b33d1e4b76e3bf465eb4755181a6_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>表4：语言模型Wikitext-103上的困惑度(越低越好)</p>
<h5 id="5-3-模型分析"><a href="#5-3-模型分析" class="headerlink" title="5.3 模型分析"></a>5.3 模型分析</h5><p>在本节中，我们首先进行消融研究，以量化DeBERTa中引入的不同组件的相对贡献。 然后，我们研究收敛性以表征模型训练效率。 我们使用基本模型设置运行实验进行分析：在配置16个V-100 GPU的DGX-2机器上，使用Wikipedia + Bookcorpus数据集在7天内对批次大小为256的1M步骤进行了预训练。 由于篇幅所限，我们在附录A.7中可视化了DeBERTa和RoBERTa的不同注意力模式。 </p>
<h5 id="5-3-1-消融研究"><a href="#5-3-1-消融研究" class="headerlink" title="5.3.1 消融研究"></a>5.3.1 消融研究</h5><p>为了验证我们的实验设置，我们从零开始对RoBERTa基础模型进行了预训练。 重新训练的RoBERTa模型表示为RoBERTa-ReImpbase。 为了研究DeBERTa中不同组件的相对贡献，我们开发了三种变体：</p>
<p><strong>•-EMD是不带EMD的DeBERTa基础模型。</strong></p>
<p><strong>•-C2P是不带内容到位置(方程4中的(c))的DeBERTa基础模型。</strong></p>
<p><strong>•-P2C是不带位置到内容的模型(等式4中的(b))。 由于XLNet也使用相对位置偏差，因此该模型接近XLNet加EMD。</strong> </p>
<p>表5总结了四个基准数据集的结果。 首先，RoBERTa-ReImp在所有基准数据集上的表现均与RoBERTa相似，证明我们的设置合理。 其次，我们看到删除DeBERTa中的任何一个组件都会导致性能下降。 例如，删除EMD(-EMD)导致RACE损失1.4％(71.7％vs. 70.3％)，SQuAD v1.1损失0.3％(92.1％vs. 91.8％)，1.2％(82.5％vs. 82.5％)。 在SQuAD v2.0上为81.3％)，在MNLI-m &#x2F; mm上分别为0.2％(86.3％对86.1％)和0.1％(86.2％对86.1％)。 同样，删除内容到位置或位置到内容会导致所有基准测试的性能下降。 如预期的那样，删除两个组件会导致更大的性能损失。 </p>
<p><img src="https://pic2.zhimg.com/80/v2-c53132c24a039a4a9e0040cd513e33dd_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>表5：DeBERTa基本模型的消融研究。 </p>
<h5 id="5-3-2-预训练效率"><a href="#5-3-2-预训练效率" class="headerlink" title="5.3.2 预训练效率"></a>5.3.2 预训练效率</h5><p>为了研究模型预训练的效率，我们使用预训练步数，绘制了微调模型在下游任务上的性能。 如图1所示，对于RoBERTa-ReImpbase和DeBERTabase，我们每150K预训练步存储一个checkpoint，然后对两个代表性的下游任务MNLI和SQuAD v2.0进行微调，然后报告准确性和F1 Score分别。 作为参考，我们还报告了原始RoBERTabase(Liu等，2019c)和XLNetbase(Yang等，2019)的最终模型性能。 结果表明，在预训练过程中，DeBERTabase始终优于RoBERTa-ReImpbase。 </p>
<p><img src="https://pic4.zhimg.com/80/v2-75471d06a310aae6de068191cb25c9cf_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>图1：DeBERTa与MNLI和SQuAD v2.0开发集上的的预训练性能曲线。</p>
<h5 id="5-4-最多可扩展至15亿个参数"><a href="#5-4-最多可扩展至15亿个参数" class="headerlink" title="5.4 最多可扩展至15亿个参数"></a>5.4 最多可扩展至15亿个参数</h5><p>更大的预训练模型已显示出更好的泛化结果(Raffel等人，2020； Brown等人，2020； Shoeybi等人，2019)。 因此，我们构建了具有15亿个参数的DeBERTa的较大版本，称为DeBERTa1.5B。 该模型包括48层，hidden size为1,536和24个attention heads 5。DeBERTa1.5B在总计160G的预训练数据集上进行了训练，类似于Liu等人(2019c)，具有使用数据集构造的128K新的单词表。</p>
<p>为了训练DeBERTa1.5B，我们按以下方式优化模型架构。 首先，在所有注意力层中，我们分别将相对位置嵌入Wk,r，Wq,r的投影矩阵与Wk,c，Wq,c共享，以减少模型参数的数量。 我们在表12中基于基本模型的消融研究表明，投影矩阵共享在保持模型性能的同时减小了模型大小。其次，在第一个Transformer层旁边添加了卷积层，以诱导子词编码及其语法的n元语法知识，输出汇总后再馈送到下一个Transformer层。 </p>
<p>表6报告了SuperGLUE的测试结果(Wang等，2019a)，这是最受欢迎的NLU基准之一。 SuperGLUE包含大量NLU任务，包括问答(Clark等人，2019; Khashabi等人，2018a; Zhang等人，2018)，自然语言推理(Dagan等人，2006; Bar-Haim等人) 等人，2006年； Giampiccolo等人，2007年； Bentivogli等人，2009年)，词义歧义消除(Pilehvar和Camacho-Collados，2019年)和Reasoning(Levesque等人，2011年； Roemmele等人，2011年) 。 自2019年发布以来，全局顶级研究团队一直在开发大型PLM，这些驱动力推动了SuperGLUE的显着性能改进。 </p>
<p><img src="https://pic2.zhimg.com/80/v2-6078bcf3aa15fa47fe8dc85bec89fcb5_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>表6：使用SuperGLUE评估服务器对SuperGLUE测试集结果评分。 所有结果均于2021年1月6日从<a target="_blank" rel="noopener" href="https://super.gluebenchmark.com获得./">https://super.gluebenchmark.com获得。</a> </p>
<p>由于将DeBERTa缩放到更大的模型而带来的显着性能提升，使得单个DeBERTa1.5B在2020年12月29日的macro-average得分(89.9比89.8)方面首次超过了SuperGLUE的人类性能，并且组合的DeBERTa模型 (DeBERTaEnsemble)截至2021年1月6日在SuperGLUE基准排名中名列前茅，比人类基线高出许多(90.3对89.8)。 </p>
<p>与由110亿个参数组成的T5相比，具有15亿参数的DeBERTa在训练和维护方面更加高效，并且更易于压缩和部署到各种设置的应用程序。 </p>
<h4 id="六、结论"><a href="#六、结论" class="headerlink" title="六、结论"></a>六、结论</h4><p>本文介绍了一种新的模型架构DeBERTa(注意力解耦的增强解码的BERT)，它使用两种新颖的技术改进了BERT和RoBERTa模型。 首先是解耦的注意力机制，其中每个单词分别使用两个编码其内容和位置的向量表示，单词间的注意力权重分别使用解耦的矩阵计算其内容和相对位置。 其次是增强的mask解码器，它在解码层中合并了绝对位置，以在模型预训练中预测被mask的token。 此外，一种新的虚拟对抗训练方法用于微调，以改善模型对下游任务的概括。 </p>
<p>我们通过全面的实证研究表明，这些技术显着提高了模型预训练的效率和下游任务的性能。 具有15亿个参数的DeBERTa模型就macro-average得分而言，首次超过了SuperGLUE基准的人类表现。 </p>
<p>DeBERTa在SuperGLUE上超越人类的表现标志着通向通用AI的重要里程碑。 尽管在SuperGLUE上取得了令人鼓舞的结果，但该模型绝不能达到NLU的人文水平。 人类非常善于利用从不同任务中学到的知识来解决一项新任务，而无需或几乎没有特定任务的演示。 这被称为组合的综合，即对熟悉的组合的(子任务或基本的解决问题的技能)进行新组合的(新任务)的能力。 展望未来，值得探索如何使DeBERTa以更明确的方式组成合成结构，允许结合自然语言的神经和符号计算，去做类似于人类所做的事情。</p>
<h4 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h4><h5 id="A-1-数据集"><a href="#A-1-数据集" class="headerlink" title="A.1 数据集"></a>A.1 数据集</h5><p>GLUE. 通用语言理解评估(GLUE)基准是九个自然语言理解(NLU)任务的集合。 如表7所示，它包括问答(Rajpurkar等，2016)，语言可接受性(Warstadt等，2018)，情感分析(Socher等，2013)，文本相似度(Cer等，2017)。 )，语义检测(Dolan＆Brockett，2005)和自然语言推理(NLI)(Dagan等，2006; Bar-Haim等，2006; Giampiccolo等，2007; Bentivogli等，2009; Levesque等人，2012; Williams等人，2018)。 任务的多样性使GLUE非常适合评估NLU模型的泛化性和鲁棒性。 </p>
<p><img src="https://pic1.zhimg.com/80/v2-e2403e560093d5e2e924527e34c8178c_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>表7：NLP应用程序基准测试的摘要信息。 </p>
<p>SuperGLUE。 SuperGLUE是GLUE基准的扩展，但难度更大，它是八个NLU任务的集合。 它涵盖了各种任务，包括问答(Zhang等人，2018; Clark等人，2019; Khashabi等人，2018b)，自然语言推理(Dagan等人，2006; Bar-Haim等人， 2006; Giampiccolo等人，2007; Bentivogli等人，2009; De Marneffe等人，2019)，共指解决方案(Levesque等人，2012)和词义歧义消除(Pilehvar＆Camacho-Collados，2019)。</p>
<p>RACE是从中文的英语考试中收集的大规模机器阅读理解数据集，旨在为中学生和高中生设计(Lai等人，2017)。 </p>
<p>SQuAD v1.1 &#x2F; v2.0是斯坦福问答数据集(SQuAD)v1.1和v2.0(Rajpurkar等人，2016; 2018)是流行的机器阅读理解基准。 他们的文章来自大约500篇Wikipedia文章，而问题和答案是通过众源获得的。 SQuAD v2.0数据集包含有关相同段落的无法回答的问题。</p>
<p>SWAG是一个大规模的对抗性数据集，用于扎实的常识推理任务，它统一了自然语言推理和基于身体的推理(Zellers等人，2018)。 SWAG包含有关真实情况的113k个多项选择题。</p>
<p>CoNLL 2003是一个英语数据集，包含来自多种来源的文本。 它有4种命名实体。</p>
<h5 id="A-2-预训练数据集"><a href="#A-2-预训练数据集" class="headerlink" title="A.2 预训练数据集"></a>A.2 预训练数据集</h5><p>对于DeBERTa预训练，我们使用Wikipedia(英语Wikipedia dump7; 12GB)，BookCorpus(Zhu等人，2015)8(6GB)，OPENWEBTEXT(公开Reddit内容(Gokaslan＆ Cohen，2019); 38GB)和STORIES9(CommonCrawl的子集(Trinh＆Le，2018); 31GB)。 重复数据删除后的总数据大小(Shoeybi et al.,2019)约为78GB。 对于预训练，我们还将采样5％的训练数据作为验证集，以监控训练过程。 表8比较了不同预训练模型中使用的数据集。 </p>
<p><img src="https://pic1.zhimg.com/80/v2-167ce39fd9e587e0e1aa8428bfcc577c_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>表8：预训练数据的比较。 </p>
<h5 id="A-3-实施细节"><a href="#A-3-实施细节" class="headerlink" title="A.3 实施细节"></a>A.3 实施细节</h5><p>在RoBERTa(Liu等人，2019c)之后，我们采用了动态数据批次。 我们还包括跨度masking(Joshi等人，2020年)作为跨度最大为3的另一种masking策略。 我们在表9中列出了预训练的详细超参数。对于预训练，我们使用<strong>Adam</strong>(Kingma＆Ba，2014)作为具有权重衰减的优化器(Loshchilov＆Hutter，2018)。 对于微调，使用<strong>RAdam</strong>(Liu等人，2019a)在某些任务上可以获得更好，更可靠的结果，例如 在CoLA，RTE和RACE中，我们使用Adam(Kingma和Ba，2014)作为优化程序进行公平比较。 对于微调，我们使用超参数搜索过程训练每个任务，在DGX-2节点上，每次运行大约需要1-2小时。 表10列出了所有超参数。模型的选择基于特定任务开发集的性能。 </p>
<p><img src="https://pic2.zhimg.com/80/v2-db74fe0fc9c828e5f01f637c983df5a1_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>表9：用于预训练DeBERTa的超参数。 </p>
<p><img src="https://pic1.zhimg.com/80/v2-7650ac35b74b03a0bc24916337ac8eb0_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>表10：用于对下行任务进行DeBERTa微调的超参数。 </p>
<p>我们的代码基于Huggingface Transformers10，FairSeq11和Megatron(Shoeybi et al.,2019)。</p>
<h5 id="A-4-处理长序列输入"><a href="#A-4-处理长序列输入" class="headerlink" title="A.4 处理长序列输入"></a>A.4 处理长序列输入</h5><p>在具有相对位置偏差的情况下，我们选择按公式3截断与k的最大相对距离。因此，在每一层中，每个token最多可以直接关注2(k-1)token及其自身 。 通过堆叠Transformer层，第l层中的每个token最多可以隐式关注(2k-1)l token。 以DeBERTalarge为例，理论上k&#x3D;512，L&#x3D;24，可以处理的最大序列长度为24,528。 这是我们设计选择的副产品，我们发现它对于RACE任务是有益的。 表11显示了对RACE任务的长序列影响的比较。 </p>
<p><img src="https://pic4.zhimg.com/80/v2-c879a2d53c165e2f2f52adba838d9cbb_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>表11：使用DeBERTa处理RACE任务的长序列输入的效果</p>
<p>长序列处理是一个活跃的研究领域。 已有很多研究将Transformer体系结构扩展以用于长序列处理(Beltagy等，2020; Kitaev等，2019; Child等，2019; Dai等，2019)。 我们未来的研究方向之一是扩展DeBERTa以处理极长的序列。 </p>
<h5 id="A-5-模型复杂度"><a href="#A-5-模型复杂度" class="headerlink" title="A.5 模型复杂度"></a>A.5 模型复杂度</h5><p>通过解耦的注意力机制，我们引入了三组附加参数 Wq,r,Wk,r∈Rd×d 和 P∈R2k×d 。 模型参数的总增加量为 2L×d2+2k×d 。 对于大型模型(d&#x3D;1024，L&#x3D;24，k&#x3D; 512)，这大约相当于49M的附加参数，增加了13％。 对于基本模型(d&#x3D;768，L&#x3D;12，k&#x3D;512)，这相当于增加了14M的参数，增加了12％。 但是，<strong>通过在内容和位置嵌入之间共享投影矩阵</strong>，即Wq,r&#x3D;Wq,c，Wk,r&#x3D;Wk,c，DeBERTa的参数数与RoBERTa相同。 我们在基本模型上进行的实验表明，结果几乎与表12相同。我们计划将来使用大型模型来验证结果。</p>
<p><img src="https://pic4.zhimg.com/80/v2-d575185cc9701956d68eaae75d678183_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>表12：DeBERTa基本模型中其他参数的消融研究。</p>
<p>由于计算了额外的位置到内容和内容到位置的注意力分数，因此额外的计算复杂度为O(Nkd)。 与BERT或RoBERTa相比，这将使计算开销增加30％。 与也使用相对位置嵌入的XLNet相比，计算开销增加了约15％。 通过融合注意力计算内核的进一步优化可以显着降低这种额外的开销。 对于EM D，由于预训练中的解码器仅重建被mask的token，因此不会对未mask的token引入额外的计算开销。 在mask了15％token且仅使用两个解码器层的情况下，额外开销为0.15x2&#x2F;L，这导致EMD中基本模型(L&#x3D;12)的额外计算开销仅为3％，大型模型(L&#x3D;24)的额外计算开销为2％。</p>
<h5 id="A-6-增强mask解码器的其他详细信息"><a href="#A-6-增强mask解码器的其他详细信息" class="headerlink" title="A.6 增强mask解码器的其他详细信息"></a>A.6 增强mask解码器的其他详细信息</h5><p>EMD的结构如图2b所示。 EMD有两个输入(即I，H)。 H表示来自先前的transformer层的隐藏状态，并且I可以是用于解码的任何必要信息，例如，H，绝对位置嵌入或从先前的EMD层输出。 n表示n个EMD堆叠层，其中每个EMD层的输出将是下一个EMD层的输入I，最后一个EMD层的输出将直接馈送到语言模型头。 n层可以共享相同的权重。 在我们的实验中，我们对n&#x3D;2层共享相同的权重，以减少参数的数量，并使用绝对位置嵌入作为第一个EMD层的I。 当I&#x3D;H和n&#x3D;1时，EMD与BERT解码器层相同。 但是，EMD更通用，更灵活，因为它可以使用各种类型的输入信息进行解码。 </p>
<p><img src="https://pic1.zhimg.com/80/v2-a36ba37e414623286f2d0f4f9ac1118c_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>图2：解码层比较。</p>
<h5 id="A-7-注意力模式"><a href="#A-7-注意力模式" class="headerlink" title="A.7 注意力模式"></a>A.7 注意力模式</h5><p>为了直观展示DeBERTa与RoBERTa的不同之处，我们在图3中介绍了RoBERTa，DeBERTa和三个DeBERTa变体的注意力模式(在最后的self-attention层)。我们观察到两个差异。首先，RoBERTa对于token的自动出现具有明显的对角线效果。但是这种效果在DeBERTa中不是很明显。这可以归因于EMD的使用，其中绝对位置嵌入被添加到内容的隐藏状态作为query向量，这由DeBERTa-EMD的注意力模式所验证，在该模式中，对角线效果更明显。原始的DeBERTa。其次，我们在RoBERTa的注意力模式中观察到垂直条纹，这主要是由频繁出现的特征性单词或token(例如“a”，“the”和标点符号)引起的。对于DeBERTa，该区域仅出现在代表[CLS] token的第一列中。我们推测，由于[CLS]的特征向量经常被用作下游任务中整个输入序列的上下文表示，因此希望优先强调[CLS]。我们还观察到，垂直条纹效应在三个DeBERTa变体的模式中非常明显。 </p>
<p>我们另外提供三个样本，以说明DeBERTa和RoBERTa的不同注意力模式, 见图4和图5。</p>
<p><img src="https://pic3.zhimg.com/80/v2-5b93b4fba82292482360c60a720a3cda_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>图3：DeBERTa，RoBERTa和DeBERTa变体之间的最后一层注意力模式的比较(即分别没有EMD，C2P和P2C的DeBERTa)。</p>
<p><img src="https://pic2.zhimg.com/80/v2-684680f2e478c7fb65379f1705e5d6bd_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>图4：DeBERTa和RoBERTa之间最后一层的注意力模式比较。 </p>
<p><img src="https://pic1.zhimg.com/80/v2-00b37fc4b513928b66e0ea01568bd160_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>图5：DeBERTa及其变体(即分别没有EMD，C2P和P2C的DeBERTa)之间最后一层注意力模式的比较。 </p>
<h5 id="A-8-微调的差异"><a href="#A-8-微调的差异" class="headerlink" title="A.8 微调的差异"></a>A.8 微调的差异</h5><p>考虑到微调的不同运行的差异，在我们的实验中，我们始终遵循Liu等人(2019c)通过平均五个具有不同随机初始化种子的运行，并且在比较结果时执行显着性测试。 如表13所示，DeBERTabase的性能明显优于RoBERTabase(pvalue&lt;0.05)。 </p>
<p><img src="https://pic4.zhimg.com/80/v2-be8245bb4278c6cc5eea49b979341303_1440w.webp" srcset="/blog/img/loading.gif" lazyload alt="img"></p>
<p>表13：在MNLI匹配的SQuAD v1.1和DeBERTa和RoBERTa的比较。</p>
<h2 id="M2M1000"><a href="#M2M1000" class="headerlink" title="M2M1000"></a>M2M1000</h2><h2 id="LUKE"><a href="#LUKE" class="headerlink" title="LUKE"></a>LUKE</h2><h2 id="clip"><a href="#clip" class="headerlink" title="clip"></a>clip</h2>
                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/blog/categories/%E8%AE%BA%E6%96%87/" class="category-chain-item">论文</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/blog/tags/%E8%AE%BA%E6%96%87%E3%80%81%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%81nlp%E3%80%81transformer/">#论文、深度学习、nlp、transformer</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>经典论文</div>
      <div>https://yoonalis.github.io/blog/2023/12/01/经典论文/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Azure</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2023年12月1日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/blog/2023/12/01/HuggingFace%20NLP%20Tasks/" title="HuggingFace NLP Tasks">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">HuggingFace NLP Tasks</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/blog/2023/11/29/Huggingface%20%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/" title="Huggingface 快速上手">
                        <span class="hidden-mobile">Huggingface 快速上手</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/blog/js/events.js" ></script>
<script  src="/blog/js/plugins.js" ></script>


  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/blog/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/blog/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  
<script src="//cdn.jsdelivr.net/gh/bynotes/texiao/source/js/caidai.js"></script>
<script src="//cdn.jsdelivr.net/gh/bynotes/texiao/source/js/love.js"></script>



<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/blog/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
