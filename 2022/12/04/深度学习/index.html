

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/blog/img/favicon.jpg">
  <link rel="icon" href="/blog/img/favicon.jpg">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Azure">
  <meta name="keywords" content="">
  
    <meta name="description" content="深度学习">
<meta property="og:type" content="article">
<meta property="og:title" content="论文选题">
<meta property="og:url" content="https://yoonalis.github.io/blog/2022/12/04/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="Azure&#39;s blog">
<meta property="og:description" content="深度学习">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://yoonalis.github.io/blog/img/ai.jpg">
<meta property="article:published_time" content="2022-12-04T13:39:40.000Z">
<meta property="article:modified_time" content="2022-12-04T14:10:32.674Z">
<meta property="article:author" content="Azure">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="论文">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://yoonalis.github.io/blog/img/ai.jpg">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>论文选题 - Azure&#39;s blog</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hint.css@2/hint.min.css" />

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/blog/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/blog/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/blog/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"yoonalis.github.io","root":"/blog/","version":"1.9.3","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/blog/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/blog/js/utils.js" ></script>
  <script  src="/blog/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/blog/">
      <strong>Azure</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog/album/">
                <i class="iconfont icon-images"></i>
                album
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog/links/">
                <i class="iconfont icon-link-fill"></i>
                友链
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/blog/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="论文选题"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2022-12-04 21:39" pubdate>
          2022年12月4日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          <!-- compatible with older versions-->
          58k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          <!-- compatible with older versions-->
          486 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">论文选题</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="论文选题"><a href="#论文选题" class="headerlink" title="论文选题"></a>论文选题</h1><p>pick one</p>
<h2 id="深度学习相关"><a href="#深度学习相关" class="headerlink" title="深度学习相关"></a>深度学习相关</h2><h3 id="1-框架"><a href="#1-框架" class="headerlink" title="1. 框架"></a>1. 框架</h3><p>tensorflow</p>
<p>pytorch</p>
<h3 id="2-必备知识点"><a href="#2-必备知识点" class="headerlink" title="2. 必备知识点"></a>2. 必备知识点</h3><h4 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h4><h5 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播"></a>正向传播</h5><p>一个常见的结构：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_06-34-17-20221204%2021:56:32.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_06-34-17"></p>
<p>输入层:比如一张图像</p>
<p>从输入层到隐藏层:H&#x3D;X*W1+b1</p>
<p>隐藏层到输出层:Y&#x3D;H*W2+b2</p>
<p>通过上述两个线性方程的计算，我们就能得到最终的输出Y了，但是如果你还对线性代数的计算有印象的话，应该会知道：一系列线性方程的运算最终都可以用一个线性方程表示。也就是说，上述两个式子联立后可以用一个线性方程表达。对于两次神经网络是这样，就算网络深度加到100层，也依然是这样。这样的话神经网络就失去了意义。</p>
<p>所以这里要对网络注入灵魂：<strong>激活层</strong>。</p>
<p>简而言之，激活层是为矩阵运算的结果添加非线性的。常用的激活函数有三种，分别是阶跃函数、Sigmoid和ReLU。不要被奇怪的函数名吓到，其实它们的形式都很简单，如下图：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_06-39-19-20221204%2021:56:48.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_06-39-19"></p>
<p>阶跃函数：当输入小于等于0时，输出0；当输入大于0时，输出1。</p>
<p>Sigmoid：当输入趋近于正无穷&#x2F;负无穷时，输出无限接近于1&#x2F;0。</p>
<p>ReLU：当输入小于0时，输出0；当输入大于0时，输出等于输入。</p>
<p>其中，阶跃函数输出值是跳变的，且只有二值，较少使用；Sigmoid函数在当x的绝对值较大时，曲线的斜率变化很小（梯度消失），并且计算较复杂；ReLU是当前较为常用的激活函数。</p>
<p>需要注意的是，每个隐藏层计算（矩阵线性运算）之后，都需要加一层激活层，要不然该层线性计算是没有意义的。</p>
<p>softmax:对输出结果正规化处理,简单来说分三步进行：（1）以e为底对所有元素求指数幂；（2）将所有指数幂求和；（3）分别将这些指数幂与该和做商。</p>
<p>交叉熵损失（Cross Entropy Error）：一种直观的解决方法，是用1减去Softmax输出的概率，不过更为常用且巧妙的方法是，求<strong>对数的负数</strong>。</p>
<p>还是用90%举例，对数的负数就是：-log0.9&#x3D;0.046</p>
<p><strong>我们训练神经网络的目的，就是尽可能地减少这个“交叉熵损失”。</strong></p>
<p>一句话复习一下：<strong>神经网络的传播都是形如Y&#x3D;WX+b的矩阵运算；为了给矩阵运算加入非线性，需要在隐藏层中加入激活层；输出层结果需要经过Softmax层处理为概率值，并通过交叉熵损失来量化当前网络的优劣。</strong></p>
<h5 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h5><p>反向传播就是一个<strong>参数优化</strong>的过程，优化对象就是网络中的所有W和b（因为其他所有参数都是确定的）。</p>
<p>神经网络的神奇之处，就在于它可以自动做W和b的优化，在深度学习中，参数的数量有时会上亿，不过其优化的原理和我们这个两层神经网络是一样的。</p>
<p>常用方法：</p>
<p>梯度下降法</p>
<p>迭代法</p>
<p> 反向传播算法（Backpropagation）是目前用来训练人工神经网络（Artificial Neural Network，ANN）的最常用且最有效的算法。其主要思想是：<br>（1）将训练集数据输入到ANN的输入层，经过隐藏层，最后达到输出层并输出结果，这是ANN的前向传播过程；<br>（2）由于ANN的输出结果与实际结果有误差，则计算估计值与实际值之间的误差，并将该误差从输出层向隐藏层反向传播，直至传播到输入层；<br>（3）在反向传播的过程中，根据误差调整各种参数的值；不断迭代上述过程，直至收敛。</p>
<h4 id="cnn：卷积神经网络"><a href="#cnn：卷积神经网络" class="headerlink" title="cnn：卷积神经网络"></a>cnn：卷积神经网络</h4><p>如果用全连接神经网络处理大尺寸图像具有三个明显的缺点：</p>
<p>（1）首先将图像展开为向量会丢失空间信息；</p>
<p>（2）其次参数过多效率低下，训练困难；</p>
<p>（3）同时大量的参数也很快会导致网络过拟合。</p>
<p>而使用卷积神经网络可以很好地解决上面的三个问题。</p>
<p>卷积神经网络的各层中的神经元是3维排列的：宽度、高度和深度。其中的宽度和高度是很好理解的，因为本身卷积就是一个二维模板，但是在卷积神经网络中的深度指的是<strong>激活数据体</strong>的第三个维度，而不是整个网络的深度，整个网络的深度指的是网络的层数。举个例子来理解什么是宽度，高度和深度，假如使用CIFAR-10中的图像是作为卷积神经网络的输入，该<strong>输入数据体</strong>的维度是32x32x3（宽度，高度和深度）。<strong>我们将看到，层中的神经元将只与前一层中的一小块区域连接，而不是采取全连接方式。</strong>对于用来分类CIFAR-10中的图像的卷积网络，其最后的输出层的维度是1x1x10，因为在卷积神经网络结构的最后部分将会把全尺寸的图像压缩为包含分类评分的一个向量，<strong>向量是在深度方向排列的</strong>。下面是全连接神经网络与卷积神经网络的对比：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_07-04-52-20221204%2021:57:09.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_07-04-52"></p>
<p>典型的 CNN 由3个部分构成：</p>
<ol>
<li>卷积层</li>
<li>池化层</li>
<li>全连接层</li>
</ol>
<p>卷积层负责提取图像中的局部特征；池化层用来大幅降低参数量级(降维)；全连接层类似传统神经网络的部分，用来输出想要的结果。</p>
<p>卷积层的运算过程如下图，用一个卷积核扫完整张图片：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/f144f-2019-06-19-juanji-20221204%2021:58:52.gif" srcset="/blog/img/loading.gif" lazyload alt="f144f-2019-06-19-juanji"></p>
<p>在具体应用中，往往有多个卷积核，可以认为，每个卷积核代表了一种图像模式，如果某个图像块与此卷积核卷积出的值大，则认为此图像块十分接近于此卷积核。如果我们设计了6个卷积核，可以理解：我们认为这个图像上有6种底层纹理模式，也就是我们用6中基础模式就能描绘出一副图像。</p>
<p>池化层简单说就是下采样，他可以大大降低数据的维度。其过程如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/3fd53-2019-06-19-chihua-20221204%2021:59:11.gif" srcset="/blog/img/loading.gif" lazyload alt="3fd53-2019-06-19-chihua"></p>
<p>上图中，我们可以看到，原始图片是20×20的，我们对其进行下采样，采样窗口为10×10，最终将其下采样成为一个2×2大小的特征图。</p>
<p>之所以这么做的原因，是因为即使做完了卷积，图像仍然很大（因为卷积核比较小），所以为了降低数据维度，就进行下采样。</p>
<p>总结：池化层相比卷积层可以更有效的降低数据维度，这么做不但可以大大减少运算量，还可以有效的避免过拟合。</p>
<p>经过卷积层和池化层处理过的数据输入到全连接层，得到最终想要的结果。</p>
<p>经过卷积层和池化层降维过的数据，全连接层才能”跑得动”，不然数据量太大，计算成本高，效率低下。</p>
<p>典型的 CNN 并非只是上面提到的3层结构，而是多层结构，例如 LeNet-5 的结构就是卷积层 – 池化层- 卷积层 – 池化层 – 卷积层 – 全连接层</p>
<h4 id="gnn：图神经网络"><a href="#gnn：图神经网络" class="headerlink" title="gnn：图神经网络"></a>gnn：图神经网络</h4><p>图(graph)是一种数据结构，图神经网络(Graph Neural Network)应该是深度学习在图结构数据上的一些模型、方法和应用。常见的图结构由节点(node)和边(edge)构成，节点包含了实体(entity)信息，边包含实体间的关系(relation)信息。现在许多学习任务都需要**处理图结构的数据，比如物理系统建模(physics system)、学习分子指纹(molecular fingerprints)、蛋白质接口预测(protein interface)以及疾病分类(classify diseases)**，这些都需要模型能够从图结构的输入中学习相关的知识。</p>
<p>首先，标准的神经网络比如CNN和RNN不能够适当地处理图结构输入，因为它们都需要节点的特征按照一定的顺序进行排列，但是，<strong>对于图结构而言，并没有天然的顺序而言</strong>，如果使用顺序来完整地表达图的话，那么就需要将图分解成所有可能的序列，然后对序列进行建模，显然，这种方式非常的冗余以及计算量非常大，与此相反，<strong>GNN采用在每个节点上分别传播(propagate)的方式进行学习</strong>，由此忽略了节点的顺序，相当于GNN的输出会随着输入的不同而不同。</p>
<p>另外，<strong>图结构的边表示节点之间的依存关系</strong>，然而，传统的神经网络中，依存关系是通过节点特征表达出来的，也就是说，传统的神经网络不是显式地表达中这种依存关系，而是通过不同节点特征来间接地表达节点之间的关系。通常来说，<strong>GNN通过邻居节点的加权求和来更新节点的隐藏状态</strong>。</p>
<p>最后，就是<strong>对于高级的人工智能来说，推理是一个非常重要的研究主题</strong>，人类大脑的推理过程基本上都是基于图的方式，这个图是从日常的生活经历中学习得到的。GNN尝试从非结构化数据比如情景图片和故事文本中产生结构化的图，并通过这些图来生成更高级的AI系统。</p>
<p><strong>论文对GNN的模型分类如下：</strong></p>
<ul>
<li>图卷积网络(Graph convolutional networks)和图注意力网络(graph attention networks)，因为涉及到传播步骤(propagation step)。</li>
<li>图的空域网络(spatial-temporal networks)，因为该模型通常用在动态图(dynamic graph)上。</li>
<li>图的自编码(auto-encoder)，因为该模型通常使用无监督学习(unsupervised)的方式。</li>
<li>图生成网络(generative networks)，因为是生成式网络。</li>
</ul>
<blockquote>
<p>更多资料可参考 [这里][<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/76025331]">https://zhuanlan.zhihu.com/p/76025331]</a></p>
</blockquote>
<h4 id="rnn：递归神经网络"><a href="#rnn：递归神经网络" class="headerlink" title="rnn：递归神经网络"></a>rnn：递归神经网络</h4><p>适合序列数据 – 一串相互依赖的数据流的场景</p>
<p>典型的集中序列数据：</p>
<ol>
<li>文章里的文字内容</li>
<li>语音里的音频内容</li>
<li>股票市场中的价格走势</li>
<li>……</li>
</ol>
<p>RNN 跟传统神经网络最大的区别在于每次都会将前一次的输出结果，带到下一次的隐藏层中，一起训练。如下图所示：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/f0116-2019-07-02-rnn-1-20221204%2021:59:34.gif" srcset="/blog/img/loading.gif" lazyload alt="f0116-2019-07-02-rnn-1"></p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_07-54-59-20221204%2021:59:44.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_07-54-59"></p>
<p>&#x3D;&#x3D;应用场景：文本生成　机器翻译　语音识别　生成图像描述　视频标记&#x3D;&#x3D;</p>
<h4 id="lstm：长短期记忆递归神经网络"><a href="#lstm：长短期记忆递归神经网络" class="headerlink" title="lstm：长短期记忆递归神经网络"></a>lstm：长短期记忆递归神经网络</h4><p>RNN 是一种死板的逻辑，越晚的输入影响越大，越早的输入影响越小，且无法改变这个逻辑。</p>
<p>LSTM从被设计之初就被用于解决一般递归神经网络中普遍存在的<strong>长期依赖问题</strong>，使用LSTM可以有效的传递和表达长时间序列中的信息并且不会导致长时间前的有用信息被忽略（遗忘）。与此同时，LSTM还可以解决RNN中的梯度消失&#x2F;爆炸问题。</p>
<p>Gated Recurrent Unit – GRU 是 LSTM 的一个变体。他保留了 LSTM 划重点，遗忘不重要信息的特点，在long-term 传播的时候也不会被丢失。</p>
<p>GRU 主要是在 LSTM 的模型上做了一些简化和调整，在训练数据集比较大的情况下可以节省很多时间。</p>
<p>详细实现可参考 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/123857569">这里</a></p>
<h4 id="分类-回归-聚类-降维-有监督-无监督"><a href="#分类-回归-聚类-降维-有监督-无监督" class="headerlink" title="分类\回归\聚类\降维\有监督\无监督"></a>分类\回归\聚类\降维\有监督\无监督</h4><p>1，给定一个样本特征 , 我们希望预测其对应的属性值 , 如果是离散的, 那么这就是一个分类问题，反之，如果 是连续的实数, 这就是一个回归问题。<br>2，如果给定一组样本特征 , 我们没有对应的属性值 , 而是想发掘这组样本在二维空间的分布, 比如分析哪些样本靠的更近，哪些样本之间离得很远, 这就是属于聚类问题。<br>3，如果我们想用维数更低的子空间来表示原来高维的特征空间, 那么这就是降维问题。</p>
<h4 id="元学习"><a href="#元学习" class="headerlink" title="元学习"></a>元学习</h4><p>通常在机器学习里，我们会使用某个场景的大量数据来训练模型；然而当场景发生改变，模型就需要重新训练。但是对于人类而言，一个小朋友成长过程中会见过许多物体的照片，某一天，当Ta（第一次）仅仅看了几张狗的照片，就可以很好地对狗和其他物体进行区分。</p>
<p>元学习Meta Learning，含义为学会学习，即learn to learn，就是带着这种对人类这种“学习能力”的期望诞生的。Meta Learning希望使得模型获取一种“学会学习”的能力，使其可以在获取已有“知识”的基础上快速学习新的任务。</p>
<p>需要注意的是，虽然同样有“预训练”的意思在里面，但是元学习的内核区别于迁移学习（Transfer Learning）。</p>
<p>详细研究可参考 [李宏毅视频][<a target="_blank" rel="noopener" href="https://www.youtube.com/@HungyiLeeNTU/search?query=meta%5D">https://www.youtube.com/@HungyiLeeNTU/search?query=meta]</a></p>
<h4 id="restnet"><a href="#restnet" class="headerlink" title="restnet"></a>restnet</h4><p>卷积神经网络在图像分类领域具有非常广泛的应用，从理论上讲，越深的网络结构，其拟合能力应该越强，如16层的VGG16的拟合能力要强于5层的LeNet。然而，何恺明等人通过实验发现，当网络的深度达到一定程度时，网络的性能不升反降，并且这种性能的下降不是由过拟合引起的，因为深度网络的训练误差和测试误差都比浅层网络高，如图所示。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_08-24-48-20221204%2021:59:56.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_08-24-48"></p>
<p>为改善上述问题，何恺明等人提出了深度残差学习框架。</p>
<p>常规的神经网络，是将卷积层、全连接层等结构按照一定的顺序简单地连接到一起，每层结构仅接受来自上一层的信息，并在本层处理后传递给下一层。当网络层次加深后，这种单一的连接方式会导致神经网络性能退化。所谓的残差学习，就是在上述的单一连接方式的基础上，加入了“<strong>短连接</strong>”（shortcut connections），如图所示。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_08-25-42-20221204%2022:00:04.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_08-25-42"></p>
<p>短连接能跨越几个层，将输入x直接映射到输出端（类似于电路中的短路），与输出相加。这样做导致的直接结果就是，在神经网络中加入上述的结构不再会导致神经网络的退化，考虑最坏的情况也不过是F(x)为0，这相当于网络没有加深，与之前一致。但是如果加入的层有效，就能使网络的性能得到提升。从数学的角度来看，上述结构的输入为x，输出H(x)为：</p>
<p>H(x)&#x3D;F(x)+xH(x)&#x3D;F(x)+x</p>
<p>其中F(x)称为残差函数，变换上式可得：</p>
<p>F(x)&#x3D;H(x)−xF(x)&#x3D;H(x)−x</p>
<p>显然，该结构中加入的短连接是没有参数的，因此学习H(x)的参数与学习F(x)的参数本质上是一样的，但是将函数优化为0或者在0附近显然更容易。因此，残差学习，学习的就是残差函数F(x)的参数。</p>
<p>论文原文中给出了两种ResNet的基本单元结构，其中左边的单元用于较浅的网络，如ResNet18、ResNet34，右边的单元则用于较深的网络，如ResNet50、ResNet101等。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_08-32-49-20221204%2022:00:13.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_08-32-49"></p>
<p>首先，无论是左边还是右边的单元，都是由卷积层和激活函数构成的。左边的单元由2个大小为3x3的卷积层与2个ReLU激活函数构成，右边的单元由2个1x1、1个3x3的卷积层和3个ReLU激活函数构成。至于卷积核的的通道（channel）数，则由单元的具体位置决定。通常卷积神经网络在提取图像特征的过程种，卷积核都会从“大卷积核、低通道数量“层层递进到到”小卷积核、多通道数量“。因此，在实际应用的过程种，短连接跨越的卷积层的通道数量有所改变是非常常见的。原论文中给出了直接连接、填零连接和映射连接三种具体的短连接形式。当实际单元的输入通道数与输出相同时，如上图左，则短连接<strong>直接连接</strong>即可。若输入通道数与输出不相同，如上图右，输入的维度为64，输出却是256，此时输入x无法与F(x)直接相加。<strong>填零连接</strong>就是将多出的维度全部填充0后再进行相加，这个方法不会引入额外的参数。<strong>映射连接</strong>则是通过矩阵变换，利用大小为1x1的卷积层扩展输入的维度后再进行相加。短连接跨越通道数不同的卷积层时，卷积执行的步长为2，这刚好缩小特征矩阵的大小，并增加了特征的通道数。</p>
<p>原论文中给出了如下5个具体的ResNet网络，这些网络都由上述的基本单元构成。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_08-35-51-20221204%2022:00:22.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_08-35-51"></p>
<h3 id="3-opencv：跨平台计算机视觉库"><a href="#3-opencv：跨平台计算机视觉库" class="headerlink" title="3.opencv：跨平台计算机视觉库"></a>3.opencv：跨平台计算机视觉库</h3><p><a target="_blank" rel="noopener" href="https://opencv.org/">官网</a></p>
<h4 id="图像基本操作"><a href="#图像基本操作" class="headerlink" title="图像基本操作"></a>图像基本操作</h4><h4 id="阈值与平滑处理"><a href="#阈值与平滑处理" class="headerlink" title="阈值与平滑处理"></a>阈值与平滑处理</h4><h4 id="图像形态学操作"><a href="#图像形态学操作" class="headerlink" title="图像形态学操作"></a>图像形态学操作</h4><h4 id="图像梯度计算"><a href="#图像梯度计算" class="headerlink" title="图像梯度计算"></a>图像梯度计算</h4><h4 id="边缘检测"><a href="#边缘检测" class="headerlink" title="边缘检测"></a>边缘检测</h4><h4 id="图像金字塔与轮廓检测"><a href="#图像金字塔与轮廓检测" class="headerlink" title="图像金字塔与轮廓检测"></a>图像金字塔与轮廓检测</h4><h4 id="直方图与傅里叶变换"><a href="#直方图与傅里叶变换" class="headerlink" title="直方图与傅里叶变换"></a>直方图与傅里叶变换</h4><h4 id="图像特征"><a href="#图像特征" class="headerlink" title="图像特征"></a>图像特征</h4><h4 id="相关实践：数字识别、OCR识别、全景图像拼接、车位识别、答题卡判卷、背景建模、光流估计、目标追踪"><a href="#相关实践：数字识别、OCR识别、全景图像拼接、车位识别、答题卡判卷、背景建模、光流估计、目标追踪" class="headerlink" title="相关实践：数字识别、OCR识别、全景图像拼接、车位识别、答题卡判卷、背景建模、光流估计、目标追踪"></a>相关实践：数字识别、OCR识别、全景图像拼接、车位识别、答题卡判卷、背景建模、光流估计、目标追踪</h4><h3 id="4-物体检测：yolo"><a href="#4-物体检测：yolo" class="headerlink" title="4. 物体检测：yolo"></a>4. 物体检测：yolo</h3><p>YOLO系列是one-stage且是基于<a target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6&spm=1001.2101.3001.7020">深度</a>学习的回归方法，而R-CNN、Fast-RCNN、Faster-RCNN等是two-stage且是基于深度学习的分类方法。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/pjreddie/darknet">YOLO官网</a></p>
<h4 id="YOLO-v1"><a href="#YOLO-v1" class="headerlink" title="YOLO v1"></a>YOLO v1</h4><p>论文地址：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1506.02640">https://arxiv.org/abs/1506.02640</a></p>
<p>官方代码：<a target="_blank" rel="noopener" href="https://github.com/pjreddie/darknet">https://github.com/pjreddie/darknet</a></p>
<p>YOLOv1的核心思想：</p>
<p>YOLOv1的核心思想就是利用整张图作为网络的输入，直接在输出层回归bounding box的位置和bounding box所属的类别。</p>
<p>实现方法：</p>
<ul>
<li>将一幅图像分成SxS个网格(grid cell)，如果某个object的中心落在这个网格中，则这个网格就负责预测这个object。</li>
<li>每个网格要预测B个bounding box，每个bounding box除了要回归自身的位置之外，还要附带预测一个confidence值。这个confidence代表了所预测的box中含有object的置信度和这个box预测的有多准两重信息，其值是这样计算的：$P_r(Object) * IOU_{pred}^{truth}$，该表达式含义：如果有object落在一个grid cell里，则第一项取1，否则取0。 第二项是预测的bounding box和实际的groundtruth之间的IoU值。 </li>
<li>每个bounding box要预测(x, y, w, h)和confidence共5个值，每个网格还要预测一个类别信息，记为C类。则SxS个网格，每个网格要预测B个bounding box还要预测C个categories。输出就是S x S x (5*B+C)的一个tensor。</li>
</ul>
<p>注意：class信息是针对每个网格的，confidence信息是针对每个bounding box的。</p>
<p><strong>举例</strong></p>
<p>在PASCAL VOC中，图像输入为448x448像素，取S&#x3D;7，B&#x3D;2，一共有20个类别(C&#x3D;20)。则输出就是7x7x(2x5+20)的一个tensor。整个网络结构如下图所示：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_18-44-35-20221204%2022:00:36.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_18-44-35"></p>
<ul>
<li>在test的时候，每个网格预测的class信息和bounding box预测的confidence信息相乘，就得到每个bounding box的class-specific confidence score，得到每个box的class-specific confidence score以后，设置阈值，滤掉得分低的boxes，对保留的boxes进行NMS处理，就得到最终的检测结果。</li>
</ul>
<p>注意：<br>由于输出层为全连接层，因此在检测时，YOLOv1模型的输入只支持与训练图像相同的输入分辨率。<br>虽然每个格子可以预测B个bounding box，但是最终只选择IOU最高的bounding box作为物体检测输出，即每个格子最多只预测出一个物体。当物体占画面比例较小，如图像中包含畜群或鸟群时，每个格子包含多个物体，但却只能检测出其中一个。</p>
<p><strong>简单的概括就是：</strong></p>
<ul>
<li>给个一个输入图像，首先将图像划分成7*7的网格；</li>
<li>对于每个网格，我们都预测2个边框（包括每个边框是目标的置信度以及每个边框区域在多个类别上的概率）；</li>
<li>根据上一步可以预测出7<em>7</em>2个目标窗口，然后根据阈值去除可能性比较低的目标窗口，最后NMS去除冗余窗口即可。</li>
</ul>
<p>YOLO的缺点</p>
<ul>
<li>YOLO对相互靠的很近的物体和很小的群体检测效果不好，这是因为一个网格中只预测了两个框，并且只属于一类；</li>
<li>同一类物体出现的新的不常见的长宽比和其他情况时，泛化能力偏弱；</li>
<li>由于损失函数的问题，定位误差是影响检测效果的主要原因。尤其是大小物体的处理上，还有待加强。</li>
</ul>
<h4 id="YOLO-v2"><a href="#YOLO-v2" class="headerlink" title="YOLO v2"></a>YOLO v2</h4><p>论文地址：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1612.08242">https://arxiv.org/abs/1612.08242</a></p>
<p>官方代码：<a target="_blank" rel="noopener" href="http://pjreddie.com/darknet/yolo/">http://pjreddie.com/darknet/yolo/</a></p>
<p>YOLOv2相对v1版本，在继续保持处理速度的基础上，从预测<strong>更准确（Better）</strong>，<strong>速度更快（Faster）</strong>，<strong>识别对象更多（Stronger）</strong>这三个方面进行了改进。其中识别更多对象也就是扩展到能够检测9000种不同对象，称之为<strong>YOLO9000</strong>。</p>
<p>文章提出了一种新的训练方法–联合训练算法，这种算法可以把这两种的数据集混合到一起。使用一种分层的观点对物体进行分类，用巨量的分类数据集数据来扩充检测数据集，从而把两种不同的数据集混合起来。联合训练算法的基本思路就是：同时在检测数据集和分类数据集上训练物体检测器（Object Detectors ），用检测数据集的数据学习物体的准确位置，用分类数据集的数据来增加分类的类别量、提升健壮性。</p>
<p>YOLO9000就是使用联合训练算法训练出来的，他拥有9000类的分类信息，这些分类信息学习自ImageNet分类数据集，而物体位置检测则学习自COCO检测数据集。</p>
<p>YOLOv1有很多缺点，作者希望改进的方向是:<strong>改善recall，提升定位的准确度</strong>，同时保持分类的准确度。</p>
<h5 id="改进点"><a href="#改进点" class="headerlink" title="改进点"></a>改进点</h5><p><strong>Batch Normalization</strong><br>批量归一化有助于解决反向传播过程中的梯度消失和梯度爆炸问题，降低对一些超参数（比如学习率、网络参数的大小范围、激活函数的选择）的敏感性，并且每个batch分别进行归一化的时候，起到了一定的正则化效果（YOLOv2不再使用dropout），从而能够获得更好的收敛速度和收敛效果。</p>
<p>使用Batch Normalization对网络进行优化，让网络提高了收敛性，同时还消除了对其他形式的正则化（regularization）的依赖。通过对YOLOv2的每一个卷积层增加Batch Normalization，最终使得mAP提高了2%，同时还使model正则化。使用Batch Normalization可以从model中去掉Dropout，而不会产生过拟合。</p>
<p><strong>High resolution classifier</strong><br>用于图像分类的训练样本很多，而标注了边框的用于训练目标检测的样本相比而言就少了很多，因为标注边框的人工成本比较高。所以目标检测模型通常都先用图像分类样本训练卷积层，提取图像特征，但这引出另一个问题，就是图像分类样本的分辨率不是很高。所以YOLOv1使用ImageNet的图像分类样本采用 224<em>224 作为输入，来训练CNN卷积层。然后在训练目标检测时，检测用的图像样本采用更高分辨率的 448</em>448 像素图像作为输入，但这样不一致的输入分辨率肯定会对模型性能有一定影响。</p>
<p>所以YOLOv2在采用 224<em>224 图像进行分类模型预训练后，再采用 448</em>448 高分辨率样本对分类模型进行微调（10个epoch），使网络特征逐渐适应 448<em>448 的分辨率。然后再使用 448</em>448 的检测样本进行训练，缓解了分辨率突然切换造成的影响，最终通过使用高分辨率，mAP提升了4%。</p>
<p><strong>Convolution with anchor boxes</strong></p>
<p>YOLOv1包含有全连接层，从而能直接预测Bounding Boxes的坐标值。Faster R-CNN算法只用卷积层与Region Proposal Network来预测Anchor Box的偏移值与置信度，而不是直接预测坐标值，YOLOv2作者发现通过预测偏移量而不是坐标值能够简化问题，让神经网络学习起来更容易。</p>
<p>借鉴Faster RCNN的做法，YOLOv2也尝试采用先验框（anchor）。在每个grid预先设定一组不同大小和宽高比的边框，来覆盖整个图像的不同位置和多种尺度，这些先验框作为预定义的候选区在神经网络中将检测其中是否存在对象，以及微调边框的位置。</p>
<p>之前YOLOv1并没有采用先验框，并且每个grid只预测两个bounding box，也就是整个图像只有98个bounding box。YOLOv2如果每个grid采用9个先验框，总共有13<em>13</em>9&#x3D;1521个先验框。所以最终YOLOv2去掉了全连接层，使用Anchor Boxes来预测 Bounding Boxes。作者去掉了网络中一个Pooling层，这让卷积层的输出能有更高的分辨率，同时对网络结构进行收缩让其运行在416<em>416而不是448</em>448。</p>
<p>由于图片中的物体都倾向于出现在图片的中心位置，特别是那种比较大的物体，所以有一个单独位于物体中心的位置用于预测这些物体。YOLOv2的卷积层采用32这个值来下采样图片，所以通过选择416*416用作输入尺寸最终能输出一个13*13的Feature Map。使用Anchor Box会让精确度稍微下降，但用了它能让YOLOv2能预测出大于一千个框，同时recall达到88%，mAP达到69.2%。<br><strong>Dimension clusters</strong><br>之前Anchor Box的尺寸是手动选择的，所以尺寸还有优化的余地。YOLOv2尝试统计出更符合样本中对象尺寸的先验框，这样就可以减少网络微调先验框到实际位置的难度。YOLOv2的做法是对训练集中标注的边框进行K-means聚类分析，以寻找尽可能匹配样本的边框尺寸。如果我们用标准的欧式距离的k-means，尺寸大的框比小框产生更多的错误。因为我们的目的是提高IOU分数，这依赖于Box的大小，所以距离度量的使用：</p>
<p>其中，centroid是聚类时被选作中心的边框，box就是其它边框，d就是两者间的“距离”，IOU越大，“距离”越近。YOLOv2给出的聚类分析结果如下图所示，通过分析实验结果（Figure 2），在model复杂性与high recall之间权衡之后，选择聚类分类数K&#x3D;5。<br>Direct location prediction<br>用Anchor Box的方法，会让model变得不稳定，尤其是在最开始几次迭代的时候。大多数不稳定因素产生自预测Box的(x,y)位置的时候。按照之前YOLOv1的方法，网络不会预测偏移量，而是根据YOLOv1中的网格单元的位置来直接预测坐标，这就让Ground Truth的值介于0到1之间。而为了让网络的结果能落在这一范围内，网络使用一个 Logistic Activation来对于网络预测结果进行限制，让结果介于0到1之间。 网络在每一个网格单元中预测出5个Bounding Boxes，每个Bounding Boxes有五个坐标值tx，ty，tw，th，t0，他们的关系见下图。假设一个网格单元对于图片左上角的偏移量是cx，cy，Bounding Boxes Prior的宽度和高度是pw，ph，那么预测的结果见下图右面的公式：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_18-56-44-20221204%2022:00:49.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_18-56-44"></p>
<p><strong>Fine-Grained Features</strong><br>目标检测面临的一个问题是图像中的需要检测的目标会有大有小，输入图像经过多层网络提取特征，最后输出的特征图中（比如YOLOv2中输入416<em>416经过卷积网络下采样最后输出是13</em>13），较小的对象可能特征已经不明显甚至被忽略掉了。为了更好的检测出一些比较小的对象，最后输出的特征图需要保留一些更细节的信息。于是YOLOv2引入一种称为passthrough层的方法在特征图中保留一些细节信息。具体来说，就是在最后一个pooling之前，特征图的大小是26<em>26</em>512，将其1拆4，直接传递（passthrough）到pooling后（并且又经过一组卷积）的特征图，两者叠加到一起作为输出的特征图。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_18-58-27-20221204%2022:01:01.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_18-58-27"></p>
<p>具体怎样将1个特征图拆成4个特征图，见下图，图中示例的是1个4<em>4拆成4个2</em>2，因为深度不变，所以没有画出来。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_18-58-53-20221204%2022:01:10.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_18-58-53"></p>
<p><strong>Multi-ScaleTraining</strong></p>
<p>作者希望YOLOv2能健壮的运行于不同尺寸的图片之上，所以把这一想法用于训练model中。区别于之前的补全图片的尺寸的方法，YOLOv2每迭代几次都会改变网络参数。每10个Batch，网络会随机地选择一个新的图片尺寸，由于使用了下采样参数是32，所以不同的尺寸大小也选择为32的倍数{320，352…..608}，最小320<em>320，最大608</em>608，网络会自动改变尺寸，并继续训练的过程。这一政策让网络在不同的输入尺寸上都能达到一个很好的预测效果，同一网络能在不同分辨率上进行检测。当输入图片尺寸比较小的时候跑的比较快，输入图片尺寸比较大的时候精度高，所以你可以在YOLOv2的速度和精度上进行权衡。</p>
<p><strong>YOLOv2 Faster</strong></p>
<p>YOLOv1的backbone使用的是GoogleLeNet，速度比VGG-16快，YOLOv1完成一次前向过程只用8.52 billion 运算，而VGG-16要30.69billion，但是YOLOv1精度稍低于VGG-16。</p>
<p>YOLOv2基于一个新的分类model，有点类似与VGG。YOLOv2使用3*3filter，每次Pooling之后都增加一倍Channels的数量。YOLOv2使用Global Average Pooling，使用Batch Normilazation来让训练更稳定，加速收敛，使model规范化。最终的model–Darknet19，有19个卷积层和5个maxpooling层，处理一张图片只需要5.58 billion次运算，在ImageNet上达到72.9%top-1精确度，91.2%top-5精确度。</p>
<p>网络训练在 ImageNet 1000类分类数据集上训练了160epochs，使用随机梯度下降，初始学习率为0.1， polynomial rate decay with a power of 4, weight decay of 0.0005 and momentum of 0.9 。训练期间使用标准的数据扩大方法：随机裁剪、旋转、变换颜色（hue）、变换饱和度（saturation）， 变换曝光度（exposure shifts）。在训练时，把整个网络在更大的448*448分辨率上Fine Turnning 10个 epoches，初始学习率设置为0.001，这种网络达到达到76.5%top-1精确度，93.3%top-5精确度。</p>
<p>网络去掉了最后一个卷积层，而加上了三个3<em>3卷积层，每个卷积层有1024个Filters，每个卷积层紧接着一个1</em>1卷积层。对于VOC数据，网络预测出每个网格单元预测五个Bounding Boxes，每个Bounding Boxes预测5个坐标和20类，所以一共125个Filters，增加了Passthough层来获取前面层的细粒度信息，网络训练了160epoches，初始学习率0.001，数据扩大方法相同，对COCO与VOC数据集的训练对策相同。</p>
<h4 id="YOLO-v3"><a href="#YOLO-v3" class="headerlink" title="YOLO v3"></a>YOLO v3</h4><p>论文地址：<a target="_blank" rel="noopener" href="https://pjreddie.com/media/files/papers/YOLOv3.pdf">https://pjreddie.com/media/files/papers/YOLOv3.pdf</a> </p>
<p>这张图很好的总结了YOLOv3的结构，让我们对YOLO有更加直观的理解。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_19-04-40-20221204%2022:01:21.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_19-04-40"></p>
<p>DBL: 代码中的Darknetconv2d_BN_Leaky，是YOLOv3的基本组件，就是卷积+BN+Leaky relu。<br>resn：n代表数字，有res1，res2, … ,res8等等，表示这个res_block里含有多少个res_unit。不懂resnet请戳这儿</p>
<p>concat：张量拼接；将darknet中间层和后面的某一层的上采样进行拼接。拼接的操作和残差层add的操作是不一样的，拼接会扩充张量的维度，而add只是直接相加不会导致张量维度的改变。</p>
<p><strong>Backbone：darknet-53</strong></p>
<p>为了达到更好的分类效果，作者自己设计训练了darknet-53，在ImageNet数据集上实验发现这个darknet-53，的确很强，相对于ResNet-152和ResNet-101，darknet-53不仅在分类精度上差不多，计算速度还比ResNet-152和ResNet-101强多了，网络层数也比他们少。</p>
<p>YOLOv3使用了darknet-53的前面的52层（没有全连接层），YOLOv3这个网络是一个全卷积网络，大量使用残差的跳层连接，并且为了降低池化带来的梯度负面效果，作者直接摒弃了POOLing，用conv的stride来实现降采样。在这个网络结构中，使用的是步长为2的卷积来进行降采样。</p>
<p>为了加强算法对小目标检测的精确度，YOLOv3中采用类似FPN的upsample和融合做法（最后融合了3个scale，其他两个scale的大小分别是26×26和52×52），在多个scale的feature map上做检测。</p>
<p>作者在3条预测支路采用的也是全卷积的结构，其中最后一个卷积层的卷积核个数是255，是针对COCO数据集的80类：3*(80+4+1)&#x3D;255，3表示一个grid cell包含3个bounding box，4表示框的4个坐标信息，1表示objectness score。</p>
<p>网络中作者进行了三次检测，分别是在32倍降采样，16倍降采样，8倍降采样时进行检测，这样在多尺度的feature map上检测跟SSD有点像。在网络中使用up-sample（上采样）的原因:网络越深的特征表达效果越好，比如在进行16倍降采样检测，如果直接使用第四次下采样的特征来检测，这样就使用了浅层特征，这样效果一般并不好。如果想使用32倍降采样后的特征，但深层特征的大小太小，因此YOLOv3使用了步长为2的up-sample（上采样），把32倍降采样得到的feature map的大小提升一倍，也就成了16倍降采样后的维度。同理8倍采样也是对16倍降采样的特征进行步长为2的上采样，这样就可以使用深层特征进行detection。</p>
<p>作者通过上采样将深层特征提取，其维度是与将要融合的特征层维度相同的（channel不同）。如下图所示，85层将13×13×256的特征上采样得到26×26×256，再将其与61层的特征拼接起来得到26×26×768。为了得到channel255，还需要进行一系列的3×3，1×1卷积操作，这样既可以提高非线性程度增加泛化性能提高网络精度，又能减少参数提高实时性。52×52×255的特征也是类似的过程。</p>
<p><strong>Bounding Box</strong></p>
<p>YOLOv3的Bounding Box由YOLOv2又做出了更好的改进。在YOLOv2和YOLOv3中，都采用了对图像中的object采用k-means聚类。feature map中的每一个cell都会预测3个边界框（bounding box） ，每个bounding box都会预测三个东西：（1）每个框的位置（4个值，中心坐标tx和ty，框的高度bh和宽度bw），（2）一个objectness prediction ，（3）N个类别，coco数据集80类，voc20类。</p>
<p>三次检测，每次对应的感受野不同，32倍降采样的感受野最大，适合检测大的目标，所以在输入为416×416时，每个cell的三个anchor box为(116 ,90); (156 ,198); (373 ,326)。16倍适合一般大小的物体，anchor box为(30,61); (62,45); (59,119)。8倍的感受野最小，适合检测小目标，因此anchor box为(10,13); (16,30); (33,23)。所以当输入为416×416时，实际总共有（52×52+26×26+13×13）×3&#x3D;10647个proposal box。</p>
<p><strong>LOSS Function</strong></p>
<p>YOLOv3重要改变之一：No more softmaxing the classes。YOLOv3现在对图像中检测到的对象执行多标签分类。</p>
<p>logistic回归用于对anchor包围的部分进行一个目标性评分(objectness score)，即这块位置是目标的可能性有多大。这一步是在predict之前进行的，可以去掉不必要anchor，可以减少计算量。</p>
<p>如果模板框不是最佳的即使它超过我们设定的阈值，我们还是不会对它进行predict。不同于Faster R-CNN的是，YOLOv3只会对1个prior进行操作，也就是那个最佳prior。而logistic回归就是用来从9个anchor priors中找到objectness score(目标存在可能性得分)最高的那一个。logistic回归就是用曲线对prior相对于 objectness score映射关系的线性建模。</p>
<ul>
<li>首先，YOLOv3要先build target，因为我们知道正样本是label与anchor box iou大于0.5的组成，所以我们根据label找到对应的anchor box。如何找出label中存放着[image,class,x(归一化),y,w(归一化),h],我们可以用这些坐标在对应13×13 Or 26×26 or 52×52的map中分别于9个anchor算出iou，找到符合要求的，把索引与位置记录好。用记录好的索引位置找到predict的anchor box。</li>
<li>xywh是由均方差来计算loss的，其中预测的xy进行sigmoid来与lable xy求差，label xy是grid cell中心点坐标，其值在0-1之间，所以predict出的xy要sigmoid。</li>
<li>分类用的多类别交叉熵，置信度用的二分类交叉熵。只有正样本才参与class，xywh的loss计算，负样本只参与置信度loss。</li>
</ul>
<h4 id="YOLO-v4"><a href="#YOLO-v4" class="headerlink" title="YOLO v4"></a>YOLO v4</h4><p>YOLOv4: Optimal Speed and Accuracy of Object Detection</p>
<p>论文：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2004.10934">https://arxiv.org/abs/2004.10934</a></p>
<p>代码：<a target="_blank" rel="noopener" href="https://github.com/AlexeyAB/darknet">https://github.com/AlexeyAB/darknet</a></p>
<p>YOLOv4其实是一个结合了大量前人研究技术，加以组合并进行适当创新的算法，实现了速度和精度的完美平衡。可以说有许多技巧可以提高卷积神经网络(CNN)的准确性，但是某些技巧仅适合在某些模型上运行，或者仅在某些问题上运行，或者仅在小型数据集上运行；我们来码一码这篇文章里作者都用了哪些调优手段：加权残差连接(WRC),跨阶段部分连接(CSP),跨小批量标准化(CmBN),自对抗训练(SAT),Mish激活,马赛克数据增强,CmBN,DropBlock正则化,CIoU Loss等等。经过一系列的堆料，终于实现了目前最优的实验结果：43.5％的AP(在Tesla V100上，MS COCO数据集的实时速度约为65FPS)。</p>
<p>先直接上<code>YOLOv4</code>的整体原理图(来源网络)如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_19-17-31-20221204%2022:01:36.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_19-17-31"></p>
<p>我们前面知道在YOLOv3中，特征提取网络使用的是Darknet53，而在YOLOv4中，对Darknet53做了一点改进，借鉴了CSPNet，CSPNet全称是Cross Stage Partial Networks，也就是跨阶段局部网络。CSPNet解决了其他大型卷积神经网络框架Backbone中网络优化的梯度信息重复问题，将梯度的变化从头到尾地集成到特征图中，因此减少了模型的参数量和FLOPS数值，既保证了推理速度和准确率，又减小了模型尺寸。<br>CSPNet实际上是基于Densnet的思想，复制基础层的特征映射图，通过dense block发送副本到下一个阶段，从而将基础层的特征映射图分离出来。这样可以有效缓解梯度消失问题(通过非常深的网络很难去反推丢失信号) ，支持特征传播，鼓励网络重用特征，从而减少网络参数数量。CSPNet思想可以和ResNet、ResNeXt和DenseNet结合，目前主要有CSPResNext50 和CSPDarknet53两种改造Backbone网络。</p>
<p>考虑到几方面的平衡：输入网络分辨率&#x2F;卷积层数量&#x2F;参数数量&#x2F;输出维度。一个模型的分类效果好不见得其检测效果就好，想要检测效果好需要以下几点：</p>
<ul>
<li>更大的网络输入分辨率——用于检测小目标</li>
<li>更深的网络层——能够覆盖更大面积的感受野</li>
<li>更多的参数——更好的检测同一图像内不同size的目标</li>
</ul>
<p>为了增大感受野，作者还使用了<code>SPP-block</code>，使用<code>PANet</code>代替<code>FPN</code>进行参数聚合以适用于不同<code>level</code>的目标检测。</p>
<p><code>SPP-Net</code>全称<code>Spatial Pyramid Pooling Networks</code>，当时主要是用来解决不同尺寸的特征图如何进入全连接层的，直接看下图，下图中对任意尺寸的特征图直接进行固定尺寸的池化，来得到固定数量的特征。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_19-20-50-20221204%2022:01:46.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_19-20-50"></p>
<p>如上图，以3个尺寸的池化为例，对特征图进行一个最大值池化，即一张特征图得取其最大值，得到1<em>d(d是特征图的维度)个特征；对特征图进行网格划分为2x2的网格，然后对每个网格进行最大值池化，那么得到4</em>d个特征；同样，对特征图进行网格划分为4x4个网格，对每个网格进行最大值池化，得到16*d个特征。 接着将每个池化得到的特征合起来即得到固定长度的特征个数（特征图的维度是固定的），接着就可以输入到全连接层中进行训练网络了。用到这里是为了增加感受野。</p>
<p><code>YOLOv4</code>使用<code>PANet</code>(<code>Path Aggregation Network</code>)代替<code>FPN</code>进行参数聚合以适用于不同<code>level</code>的目标检测, <code>PANet</code>论文中融合的时候使用的方法是<code>Addition</code>，<code>YOLOv4</code>算法将融合的方法由加法改为<code>Concatenation</code>。</p>
<p><strong>激活函数</strong></p>
<p>对激活函数的研究一直没有停止过，<code>ReLU</code>还是统治着深度学习的激活函数，不过，这种情况有可能会被<code>Mish</code>改变。<code>Mish</code>是另一个与<code>ReLU</code>和<code>Swish</code>非常相似的激活函数。正如论文所宣称的那样，<code>Mish</code>可以在不同数据集的许多深度网络中胜过它们。公式为：$y &#x3D; x*tanh(ln(1+e^x))$</p>
<p>Mish是一个平滑的曲线，平滑的激活函数允许更好的信息深入神经网络，从而得到更好的准确性和泛化；在负值的时候并不是完全截断，允许比较小的负梯度流入。实验中，随着层深的增加，ReLU激活函数精度迅速下降，而Mish激活函数在训练稳定性、平均准确率(1%-2.8%)、峰值准确率(1.2% - 3.6%)等方面都有全面的提高。如下图：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_19-28-33-20221204%2022:01:57.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_19-28-33"></p>
<p><strong>自对抗训练(SAT)</strong></p>
<p>SAT为一种新型数据增强方式。在第一阶段，神经网络改变原始图像而不是网络权值。通过这种方式，神经网络对其自身进行一种对抗式的攻击，改变原始图像，制造图像上没有目标的假象。在第二阶段，训练神经网络对修改后的图像进行正常的目标检测。</p>
<p>Self-Adversarial Training是在一定程度上抵抗对抗攻击的数据增强技术。CNN计算出Loss, 然后通过反向传播改变图片信息，形成图片上没有目标的假象，然后对修改后的图像进行正常的目标检测。需要注意的是在SAT的反向传播的过程中，是不需要改变网络权值的。 使用对抗生成可以改善学习的决策边界中的薄弱环节，提高模型的鲁棒性。因此这种数据增强方式被越来越多的对象检测框架运用。</p>
<p>YOLO v5</p>
<p>YOLOv4出现之后不久，YOLOv5横空出世。YOLOv5在YOLOv4算法的基础上做了进一步的改进，检测性能得到进一步的提升。虽然YOLOv5算法并没有与YOLOv4算法进行性能比较与分析，但是YOLOv5在COCO数据集上面的测试效果还是挺不错的。大家对YOLOv5算法的创新性半信半疑，有的人对其持肯定态度，有的人对其持否定态度。在我看来，YOLOv5检测算法中还是存在很多可以学习的地方，虽然这些改进思路看来比较简单或者创新点不足，但是它们确定可以提升检测算法的性能。其实工业界往往更喜欢使用这些方法，而不是利用一个超级复杂的算法来获得较高的检测精度。<br>YOLOv5是一种单阶段目标检测算法，该算法在YOLOv4的基础上添加了一些新的改进思路，使其速度与精度都得到了极大的性能提升。主要的改进思路如下所示：</p>
<p>输入端：在模型训练阶段，提出了一些改进思路，主要包括Mosaic数据增强、自适应锚框计算、自适应图片缩放；</p>
<p>基准网络：融合其它检测算法中的一些新思路，主要包括：Focus结构与CSP结构；</p>
<p>Neck网络：目标检测网络在BackBone与最后的Head输出层之间往往会插入一些层，Yolov5中添加了FPN+PAN结构；</p>
<p>Head输出层：输出层的锚框机制与YOLOv4相同，主要改进的是训练时的损失函数GIOU_Loss，以及预测框筛选的DIOU_nms。</p>
<p>网络架构</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_19-30-57-20221204%2022:02:07.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_19-30-57"></p>
<p>上图展示了YOLOv5目标检测算法的整体框图。对于一个目标检测算法而言，我们通常可以将其划分为4个通用的模块，具体包括：输入端、基准网络、Neck网络与Head输出端，对应于上图中的4个红色模块。YOLOv5算法具有4个版本，具体包括：YOLOv5s、YOLOv5m、YOLOv5l、YOLOv5x四种，本文重点讲解YOLOv5s，其它的版本都在该版本的基础上对网络进行加深与加宽。</p>
<ul>
<li>输入端-输入端表示输入的图片。该网络的输入图像大小为608*608，该阶段通常包含一个图像预处理阶段，即将输入图像缩放到网络的输入大小，并进行归一化等操作。在网络训练阶段，YOLOv5使用Mosaic数据增强操作提升模型的训练速度和网络的精度；并提出了一种自适应锚框计算与自适应图片缩放方法。</li>
<li>基准网络-基准网络通常是一些性能优异的分类器种的网络，该模块用来提取一些通用的特征表示。YOLOv5中不仅使用了CSPDarknet53结构，而且使用了Focus结构作为基准网络。</li>
<li>Neck网络-Neck网络通常位于基准网络和头网络的中间位置，利用它可以进一步提升特征的多样性及鲁棒性。虽然YOLOv5同样用到了SPP模块、FPN+PAN模块，但是实现的细节有些不同。</li>
<li>Head输出端-Head用来完成目标检测结果的输出。针对不同的检测算法，输出端的分支个数不尽相同，通常包含一个分类分支和一个回归分支。YOLOv4利用GIOU_Loss来代替Smooth L1 Loss函数，从而进一步提升算法的检测精度。</li>
</ul>
<p>YOLO v5基础组件</p>
<ul>
<li>CBL-CBL模块由Conv+BN+Leaky_relu激活函数组成，如上图中的模块1所示。</li>
<li>Res unit-借鉴ResNet网络中的残差结构，用来构建深层网络，CBM是残差模块中的子模块，如上图中的模块2所示。</li>
<li>CSP1_X-借鉴CSPNet网络结构，该模块由CBL模块、Res unint模块以及卷积层、Concate组成而成，如上图中的模块3所示。</li>
<li>CSP2_X-借鉴CSPNet网络结构，该模块由卷积层和X个Res unint模块Concate组成而成，如上图中的模块4所示。</li>
<li>Focus-如上图中的模块5所示，Focus结构首先将多个slice结果Concat起来，然后将其送入CBL模块中。</li>
<li>SPP-采用1×1、5×5、9×9和13×13的最大池化方式，进行多尺度特征融合，如上图中的模块6所示。</li>
</ul>
<blockquote>
<p>yolo系列参考 [这里][<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_40716944/article/details/114822515?spm=1001.2101.3001.6650.2&amp;utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-2-114822515-blog-105157452.pc_relevant_3mothn_strategy_and_data_recovery&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-2-114822515-blog-105157452.pc_relevant_3mothn_strategy_and_data_recovery&amp;utm_relevant_index=5%5D">https://blog.csdn.net/qq_40716944/article/details/114822515?spm=1001.2101.3001.6650.2&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-2-114822515-blog-105157452.pc_relevant_3mothn_strategy_and_data_recovery&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-2-114822515-blog-105157452.pc_relevant_3mothn_strategy_and_data_recovery&amp;utm_relevant_index=5]</a></p>
</blockquote>
<h3 id="5-图像分割-x2F-语义分割"><a href="#5-图像分割-x2F-语义分割" class="headerlink" title="5. 图像分割&#x2F;语义分割"></a>5. 图像分割&#x2F;语义分割</h3><h4 id="unet"><a href="#unet" class="headerlink" title="unet"></a>unet</h4><p>Unet可以说是<strong>最常用、最简单</strong>的一种分割模型了，它简单、高效、易懂、容易构建、可以从小数据集中训练。</p>
<p>Unet已经是非常老的分割模型了，是2015年《U-Net: Convolutional Networks for Biomedical Image Segmentation》提出的模型，论文链接：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1505.04597">https://arxiv.org/abs/1505.04597</a></p>
<p>在Unet之前，则是更老的FCN网络，FCN是Fully Convolutional Netowkrs的碎屑，不过这个<strong>基本上是一个框架，到现在的分割网络，谁敢说用不到卷积层呢。</strong> 不过FCN网络的准确度较低，不比Unet好用。现在还有Segnet，Mask RCNN，DeepLabv3+等网络，不过今天我先介绍Unet，毕竟一口吃不成胖子。</p>
<p>网络结构</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_19-35-40-20221204%2022:02:18.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_19-35-40"></p>
<p>这个结构就是先对图片进行卷积和池化，在Unet论文中是池化4次，比方说一开始的图片是224x224的，那么就会变成112x112，56x56,28x28,14x14四个不同尺寸的特征。<strong>然后我们对14x14的特征图做上采样或者反卷积，得到28x28的特征图，这个28x28的特征图与之前的28x28的特征图进行通道伤的拼接concat，然后再对拼接之后的特征图做卷积和上采样，得到56x56的特征图，再与之前的56x56的特征拼接，卷积，再上采样，经过四次上采样可以得到一个与输入图像尺寸相同的224x224的预测结果。</strong></p>
<p>其实整体来看，这个也是一个Encoder-Decoder的结构。</p>
<p>Unet网络非常的简单，前半部分就是特征提取，后半部分是上采样。在一些文献中把这种结构叫做<strong>编码器-解码器结构</strong>，由于网络的整体结构是一个大些的英文字母U，所以叫做U-net。</p>
<ul>
<li>Encoder：左半部分，由两个3x3的卷积层（RELU）再加上一个2x2的maxpooling层组成一个下采样的模块（后面代码可以看出）；</li>
<li>Decoder：有半部分，由一个上采样的卷积层（去卷积层）+特征拼接concat+两个3x3的卷积层（ReLU）反复构成（代码中可以看出来）；</li>
</ul>
<p>Unet的好处我感觉是：网络层越深得到的特征图，有着更大的视野域，浅层卷积关注纹理特征，深层网络关注本质的那种特征，所以深层浅层特征都是有格子的意义的；另外一点是通过反卷积得到的更大的尺寸的特征图的边缘，是缺少信息的，毕竟每一次下采样提炼特征的同时，也必然会损失一些边缘特征，而失去的特征并不能从上采样中找回，因此通过特征的拼接，来实现边缘特征的一个找回。</p>
<p>注：大多数医疗影像语义分割任务都会首先用Unet作为baseline</p>
<p>附一个比较清晰的代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">double_conv2d_bn</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,in_channels,out_channels,kernel_size=<span class="hljs-number">3</span>,strides=<span class="hljs-number">1</span>,padding=<span class="hljs-number">1</span></span>):<br>        <span class="hljs-built_in">super</span>(double_conv2d_bn,self).__init__()<br>        self.conv1 = nn.Conv2d(in_channels,out_channels,<br>                               kernel_size=kernel_size,<br>                              stride = strides,padding=padding,bias=<span class="hljs-literal">True</span>)<br>        self.conv2 = nn.Conv2d(out_channels,out_channels,<br>                              kernel_size = kernel_size,<br>                              stride = strides,padding=padding,bias=<span class="hljs-literal">True</span>)<br>        self.bn1 = nn.BatchNorm2d(out_channels)<br>        self.bn2 = nn.BatchNorm2d(out_channels)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        out = F.relu(self.bn1(self.conv1(x)))<br>        out = F.relu(self.bn2(self.conv2(out)))<br>        <span class="hljs-keyword">return</span> out<br>    <br><span class="hljs-keyword">class</span> <span class="hljs-title class_">deconv2d_bn</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,in_channels,out_channels,kernel_size=<span class="hljs-number">2</span>,strides=<span class="hljs-number">2</span></span>):<br>        <span class="hljs-built_in">super</span>(deconv2d_bn,self).__init__()<br>        self.conv1 = nn.ConvTranspose2d(in_channels,out_channels,<br>                                        kernel_size = kernel_size,<br>                                       stride = strides,bias=<span class="hljs-literal">True</span>)<br>        self.bn1 = nn.BatchNorm2d(out_channels)<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        out = F.relu(self.bn1(self.conv1(x)))<br>        <span class="hljs-keyword">return</span> out<br>    <br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Unet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(Unet,self).__init__()<br>        self.layer1_conv = double_conv2d_bn(<span class="hljs-number">1</span>,<span class="hljs-number">8</span>)<br>        self.layer2_conv = double_conv2d_bn(<span class="hljs-number">8</span>,<span class="hljs-number">16</span>)<br>        self.layer3_conv = double_conv2d_bn(<span class="hljs-number">16</span>,<span class="hljs-number">32</span>)<br>        self.layer4_conv = double_conv2d_bn(<span class="hljs-number">32</span>,<span class="hljs-number">64</span>)<br>        self.layer5_conv = double_conv2d_bn(<span class="hljs-number">64</span>,<span class="hljs-number">128</span>)<br>        self.layer6_conv = double_conv2d_bn(<span class="hljs-number">128</span>,<span class="hljs-number">64</span>)<br>        self.layer7_conv = double_conv2d_bn(<span class="hljs-number">64</span>,<span class="hljs-number">32</span>)<br>        self.layer8_conv = double_conv2d_bn(<span class="hljs-number">32</span>,<span class="hljs-number">16</span>)<br>        self.layer9_conv = double_conv2d_bn(<span class="hljs-number">16</span>,<span class="hljs-number">8</span>)<br>        self.layer10_conv = nn.Conv2d(<span class="hljs-number">8</span>,<span class="hljs-number">1</span>,kernel_size=<span class="hljs-number">3</span>,<br>                                     stride=<span class="hljs-number">1</span>,padding=<span class="hljs-number">1</span>,bias=<span class="hljs-literal">True</span>)<br>        <br>        self.deconv1 = deconv2d_bn(<span class="hljs-number">128</span>,<span class="hljs-number">64</span>)<br>        self.deconv2 = deconv2d_bn(<span class="hljs-number">64</span>,<span class="hljs-number">32</span>)<br>        self.deconv3 = deconv2d_bn(<span class="hljs-number">32</span>,<span class="hljs-number">16</span>)<br>        self.deconv4 = deconv2d_bn(<span class="hljs-number">16</span>,<span class="hljs-number">8</span>)<br>        <br>        self.sigmoid = nn.Sigmoid()<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        conv1 = self.layer1_conv(x)<br>        pool1 = F.max_pool2d(conv1,<span class="hljs-number">2</span>)<br>        <br>        conv2 = self.layer2_conv(pool1)<br>        pool2 = F.max_pool2d(conv2,<span class="hljs-number">2</span>)<br>        <br>        conv3 = self.layer3_conv(pool2)<br>        pool3 = F.max_pool2d(conv3,<span class="hljs-number">2</span>)<br>        <br>        conv4 = self.layer4_conv(pool3)<br>        pool4 = F.max_pool2d(conv4,<span class="hljs-number">2</span>)<br>        <br>        conv5 = self.layer5_conv(pool4)<br>        <br>        convt1 = self.deconv1(conv5)<br>        concat1 = torch.cat([convt1,conv4],dim=<span class="hljs-number">1</span>)<br>        conv6 = self.layer6_conv(concat1)<br>        <br>        convt2 = self.deconv2(conv6)<br>        concat2 = torch.cat([convt2,conv3],dim=<span class="hljs-number">1</span>)<br>        conv7 = self.layer7_conv(concat2)<br>        <br>        convt3 = self.deconv3(conv7)<br>        concat3 = torch.cat([convt3,conv2],dim=<span class="hljs-number">1</span>)<br>        conv8 = self.layer8_conv(concat3)<br>        <br>        convt4 = self.deconv4(conv8)<br>        concat4 = torch.cat([convt4,conv1],dim=<span class="hljs-number">1</span>)<br>        conv9 = self.layer9_conv(concat4)<br>        outp = self.layer10_conv(conv9)<br>        outp = self.sigmoid(outp)<br>        <span class="hljs-keyword">return</span> outp<br>    <br><br>model = Unet()<br>inp = torch.rand(<span class="hljs-number">10</span>,<span class="hljs-number">1</span>,<span class="hljs-number">224</span>,<span class="hljs-number">224</span>)<br>outp = model(inp)<br><span class="hljs-built_in">print</span>(outp.shape)<br>==&gt; torch.Size([<span class="hljs-number">10</span>, <span class="hljs-number">1</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>])<br></code></pre></td></tr></table></figure>



<h4 id="u2net"><a href="#u2net" class="headerlink" title="u2net"></a>u2net</h4><p>论文：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2005.09007.pdf">https://arxiv.org/pdf/2005.09007.pdf</a></p>
<p>代码：<a target="_blank" rel="noopener" href="https://codeload.github.com/NathanUA/U-2-Net/zip/master">https://codeload.github.com/NathanUA/U-2-Net/zip/master</a></p>
<p>U2net是基于unet提出的一种新的网络结构，同样基于encode-decode，作者参考FPN，Unet，在此基础之上提出了一种新模块RSU(ReSidual U-blocks) 经过测试，对于分割物体前背景取得了惊人的效果。同样具有较好的实时性，经过测试在P100上前向时间仅为18ms(56fps)。</p>
<p>网络结构</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_20-15-07-20221204%2022:02:32.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_20-15-07"></p>
<p>其实这个网络结构以及把一整个u2net完整的表示出来了，作者提出了一种名为RSU的新模块。对于每个RSU本身就是一个小号的Unet，最后所有的RSU用一种类似FPN的结构连接在一起。类似down-top top-down。通过这种方式来增加多尺度能力。获得了极为优秀的分割结果。</p>
<p>从这个结构可以很容易的看出，所谓的RSU其实就是一个很简单的Unet</p>
<p>作者通过类似FPN的结构，将多个Unet输出结果进行组合。最后进行合并，得到mask，通过多个loss在不同层的表现来进行更新。取得了非常理想的效果。</p>
<p>下面是整个U2net代码细节的介绍：</p>
<p>由于这个代码写的相对比较。。嗯。写的比较清晰，所以和以往的介绍不一样，我以代码注释的形式介绍好了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">U2NET</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,in_ch=<span class="hljs-number">3</span>,out_ch=<span class="hljs-number">1</span></span>):<br>        <span class="hljs-built_in">super</span>(U2NET,self).__init__()<br>        self.stage1 = RSU7(in_ch,<span class="hljs-number">32</span>,<span class="hljs-number">64</span>)<br>		<span class="hljs-comment">#对于每一个RSU来说，本质其实就是一个Unet，多个下采样多个上采样</span><br><br>        self.pool12 = nn.MaxPool2d(<span class="hljs-number">2</span>,stride=<span class="hljs-number">2</span>,ceil_mode=<span class="hljs-literal">True</span>)<br>        self.stage2 = RSU6(<span class="hljs-number">64</span>,<span class="hljs-number">32</span>,<span class="hljs-number">128</span>)<br>        self.pool23 = nn.MaxPool2d(<span class="hljs-number">2</span>,stride=<span class="hljs-number">2</span>,ceil_mode=<span class="hljs-literal">True</span>)<br>        self.stage3 = RSU5(<span class="hljs-number">128</span>,<span class="hljs-number">64</span>,<span class="hljs-number">256</span>)<br>        self.pool34 = nn.MaxPool2d(<span class="hljs-number">2</span>,stride=<span class="hljs-number">2</span>,ceil_mode=<span class="hljs-literal">True</span>)<br>        self.stage4 = RSU4(<span class="hljs-number">256</span>,<span class="hljs-number">128</span>,<span class="hljs-number">512</span>)<br>        self.pool45 = nn.MaxPool2d(<span class="hljs-number">2</span>,stride=<span class="hljs-number">2</span>,ceil_mode=<span class="hljs-literal">True</span>)<br>        self.stage5 = RSU4F(<span class="hljs-number">512</span>,<span class="hljs-number">256</span>,<span class="hljs-number">512</span>)<br>        self.pool56 = nn.MaxPool2d(<span class="hljs-number">2</span>,stride=<span class="hljs-number">2</span>,ceil_mode=<span class="hljs-literal">True</span>)<br>        self.stage6 = RSU4F(<span class="hljs-number">512</span>,<span class="hljs-number">256</span>,<span class="hljs-number">512</span>)<br>        <span class="hljs-comment"># decoder</span><br>        self.stage5d = RSU4F(<span class="hljs-number">1024</span>,<span class="hljs-number">256</span>,<span class="hljs-number">512</span>)<br>        self.stage4d = RSU4(<span class="hljs-number">1024</span>,<span class="hljs-number">128</span>,<span class="hljs-number">256</span>)<br>        self.stage3d = RSU5(<span class="hljs-number">512</span>,<span class="hljs-number">64</span>,<span class="hljs-number">128</span>)<br>        self.stage2d = RSU6(<span class="hljs-number">256</span>,<span class="hljs-number">32</span>,<span class="hljs-number">64</span>)<br>        self.stage1d = RSU7(<span class="hljs-number">128</span>,<span class="hljs-number">16</span>,<span class="hljs-number">64</span>)<br>        self.side1 = nn.Conv2d(<span class="hljs-number">64</span>,out_ch,<span class="hljs-number">3</span>,padding=<span class="hljs-number">1</span>)<br>        self.side2 = nn.Conv2d(<span class="hljs-number">64</span>,out_ch,<span class="hljs-number">3</span>,padding=<span class="hljs-number">1</span>)<br>        self.side3 = nn.Conv2d(<span class="hljs-number">128</span>,out_ch,<span class="hljs-number">3</span>,padding=<span class="hljs-number">1</span>)<br>        self.side4 = nn.Conv2d(<span class="hljs-number">256</span>,out_ch,<span class="hljs-number">3</span>,padding=<span class="hljs-number">1</span>)<br>        self.side5 = nn.Conv2d(<span class="hljs-number">512</span>,out_ch,<span class="hljs-number">3</span>,padding=<span class="hljs-number">1</span>)<br>        self.side6 = nn.Conv2d(<span class="hljs-number">512</span>,out_ch,<span class="hljs-number">3</span>,padding=<span class="hljs-number">1</span>)<br>        self.outconv = nn.Conv2d(<span class="hljs-number">6</span>,out_ch,<span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br><br>        hx = x<br>        <span class="hljs-comment">#stage 1</span><br>        hx1 = self.stage1(hx)<br>        <span class="hljs-comment">#通过一个个Unet得到相应的mask</span><br>        hx = self.pool12(hx1)<br>        <span class="hljs-comment">#stage 2</span><br>        hx2 = self.stage2(hx)<br>        hx = self.pool23(hx2)<br>        <span class="hljs-comment">#stage 3</span><br>        hx3 = self.stage3(hx)<br>        hx = self.pool34(hx3)<br>        <span class="hljs-comment">#stage 4</span><br>        hx4 = self.stage4(hx)<br>        hx = self.pool45(hx4)<br>        <span class="hljs-comment">#stage 5</span><br>        hx5 = self.stage5(hx)<br>        hx = self.pool56(hx5)<br>        <span class="hljs-comment">#stage 6</span><br>        hx6 = self.stage6(hx)<br>        hx6up = _upsample_like(hx6,hx5)<br>        <span class="hljs-comment">#-------------------- decoder --------------------</span><br>        hx5d = self.stage5d(torch.cat((hx6up,hx5),<span class="hljs-number">1</span>))<br>        <span class="hljs-comment">#这里类似FPN。每个block的输出结果和上一个（下一个block）结果做融合（cat），然后输出。</span><br>        hx5dup = _upsample_like(hx5d,hx4)<br>        <span class="hljs-comment">#由于每个block做了下采样，为了resize到原图，需要做一个上采样，这里作者直接用的双线性插值做的上采样</span><br>        hx4d = self.stage4d(torch.cat((hx5dup,hx4),<span class="hljs-number">1</span>))<br>        hx4dup = _upsample_like(hx4d,hx3)<br>        hx3d = self.stage3d(torch.cat((hx4dup,hx3),<span class="hljs-number">1</span>))<br>        hx3dup = _upsample_like(hx3d,hx2)<br>        hx2d = self.stage2d(torch.cat((hx3dup,hx2),<span class="hljs-number">1</span>))<br>        hx2dup = _upsample_like(hx2d,hx1)<br>        hx1d = self.stage1d(torch.cat((hx2dup,hx1),<span class="hljs-number">1</span>))<br><br><br>        <span class="hljs-comment">#side output</span><br>        d1 = self.side1(hx1d)<br>		<span class="hljs-comment">#这里本质就是把每一个block输出结果，转换成WxHx1的mask最后过一个sigmod就可以得到每个block输出的概率图。</span><br>        d2 = self.side2(hx2d)<br>        d2 = _upsample_like(d2,d1)<br><br>        d3 = self.side3(hx3d)<br>        d3 = _upsample_like(d3,d1)<br><br>        d4 = self.side4(hx4d)<br>        d4 = _upsample_like(d4,d1)<br><br>        d5 = self.side5(hx5d)<br>        d5 = _upsample_like(d5,d1)<br><br>        d6 = self.side6(hx6)<br>        d6 = _upsample_like(d6,d1)<br><br>        d0 = self.outconv(torch.cat((d1,d2,d3,d4,d5,d6),<span class="hljs-number">1</span>))<br>		<span class="hljs-comment">#6个blovk cat一起之后做特征融合，然后再做输出，结果就是d0的结果，其他的输出都是为了计算loss</span><br>        <span class="hljs-keyword">return</span> F.sigmoid(d0), F.sigmoid(d1), F.sigmoid(d2), F.sigmoid(d3), F.sigmoid(d4), F.sigmoid(d5), F.sigmoid(d6)<br><br></code></pre></td></tr></table></figure>

<p>具体的对于每个RSU来说</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">RSU7</span>(nn.Module):<span class="hljs-comment">#UNet07DRES(nn.Module):</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_ch=<span class="hljs-number">3</span>, mid_ch=<span class="hljs-number">12</span>, out_ch=<span class="hljs-number">3</span></span>):<br>        <span class="hljs-built_in">super</span>(RSU7,self).__init__()<br><br>        self.rebnconvin = REBNCONV(in_ch,out_ch,dirate=<span class="hljs-number">1</span>)<br><br>        self.rebnconv1 = REBNCONV(out_ch,mid_ch,dirate=<span class="hljs-number">1</span>)<br>        self.pool1 = nn.MaxPool2d(<span class="hljs-number">2</span>,stride=<span class="hljs-number">2</span>,ceil_mode=<span class="hljs-literal">True</span>)<br><br>        self.rebnconv2 = REBNCONV(mid_ch,mid_ch,dirate=<span class="hljs-number">1</span>)<br>        self.pool2 = nn.MaxPool2d(<span class="hljs-number">2</span>,stride=<span class="hljs-number">2</span>,ceil_mode=<span class="hljs-literal">True</span>)<br><br>        self.rebnconv3 = REBNCONV(mid_ch,mid_ch,dirate=<span class="hljs-number">1</span>)<br>        self.pool3 = nn.MaxPool2d(<span class="hljs-number">2</span>,stride=<span class="hljs-number">2</span>,ceil_mode=<span class="hljs-literal">True</span>)<br><br>        self.rebnconv4 = REBNCONV(mid_ch,mid_ch,dirate=<span class="hljs-number">1</span>)<br>        self.pool4 = nn.MaxPool2d(<span class="hljs-number">2</span>,stride=<span class="hljs-number">2</span>,ceil_mode=<span class="hljs-literal">True</span>)<br><br>        self.rebnconv5 = REBNCONV(mid_ch,mid_ch,dirate=<span class="hljs-number">1</span>)<br>        self.pool5 = nn.MaxPool2d(<span class="hljs-number">2</span>,stride=<span class="hljs-number">2</span>,ceil_mode=<span class="hljs-literal">True</span>)<br><br>        self.rebnconv6 = REBNCONV(mid_ch,mid_ch,dirate=<span class="hljs-number">1</span>)<br><br>        self.rebnconv7 = REBNCONV(mid_ch,mid_ch,dirate=<span class="hljs-number">2</span>)<br><br>        self.rebnconv6d = REBNCONV(mid_ch*<span class="hljs-number">2</span>,mid_ch,dirate=<span class="hljs-number">1</span>)<br>        self.rebnconv5d = REBNCONV(mid_ch*<span class="hljs-number">2</span>,mid_ch,dirate=<span class="hljs-number">1</span>)<br>        self.rebnconv4d = REBNCONV(mid_ch*<span class="hljs-number">2</span>,mid_ch,dirate=<span class="hljs-number">1</span>)<br>        self.rebnconv3d = REBNCONV(mid_ch*<span class="hljs-number">2</span>,mid_ch,dirate=<span class="hljs-number">1</span>)<br>        self.rebnconv2d = REBNCONV(mid_ch*<span class="hljs-number">2</span>,mid_ch,dirate=<span class="hljs-number">1</span>)<br>        self.rebnconv1d = REBNCONV(mid_ch*<span class="hljs-number">2</span>,out_ch,dirate=<span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br><br>        hx = x<br>        hxin = self.rebnconvin(hx)<br><br>        hx1 = self.rebnconv1(hxin)<br>        hx = self.pool1(hx1)<br><br>        hx2 = self.rebnconv2(hx)<br>        hx = self.pool2(hx2)<br><br>        hx3 = self.rebnconv3(hx)<br>        hx = self.pool3(hx3)<br><br>        hx4 = self.rebnconv4(hx)<br>        hx = self.pool4(hx4)<br><br>        hx5 = self.rebnconv5(hx)<br>        hx = self.pool5(hx5)<br><br>        hx6 = self.rebnconv6(hx)<br><br>        hx7 = self.rebnconv7(hx6)<br><br>        hx6d =  self.rebnconv6d(torch.cat((hx7,hx6),<span class="hljs-number">1</span>))<br>        hx6dup = _upsample_like(hx6d,hx5)<br>		<span class="hljs-comment">#双线性差值做的上采样</span><br>        hx5d =  self.rebnconv5d(torch.cat((hx6dup,hx5),<span class="hljs-number">1</span>))<br>        hx5dup = _upsample_like(hx5d,hx4)<br><br>        hx4d = self.rebnconv4d(torch.cat((hx5dup,hx4),<span class="hljs-number">1</span>))<br>        hx4dup = _upsample_like(hx4d,hx3)<br><br>        hx3d = self.rebnconv3d(torch.cat((hx4dup,hx3),<span class="hljs-number">1</span>))<br>        hx3dup = _upsample_like(hx3d,hx2)<br><br>        hx2d = self.rebnconv2d(torch.cat((hx3dup,hx2),<span class="hljs-number">1</span>))<br>        hx2dup = _upsample_like(hx2d,hx1)<br><br>        hx1d = self.rebnconv1d(torch.cat((hx2dup,hx1),<span class="hljs-number">1</span>))<br><br>        <span class="hljs-keyword">return</span> hx1d + hxin<br></code></pre></td></tr></table></figure>

<p>对应结构图本质其实是这样的：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_20-17-55-20221204%2022:02:46.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_20-17-55"></p>
<p>RSU和ResNet的残差结构其实非常的相似。只不过将权重层换成了Unet而已。</p>
<p>最后我们来看下loss：</p>
<p>由于作者把U2net分成了多个block，每个block输出一个loss，那么最后整个模型的loss本质其实就是7个loss相加（6个block输出结果加1个特征融合后的结果）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">muti_bce_loss_fusion</span>(<span class="hljs-params">d0, d1, d2, d3, d4, d5, d6, labels_v</span>):<br><br>	loss0 = bce_loss(d0,labels_v)<br>	loss1 = bce_loss(d1,labels_v)<br>	loss2 = bce_loss(d2,labels_v)<br>	loss3 = bce_loss(d3,labels_v)<br>	loss4 = bce_loss(d4,labels_v)<br>	loss5 = bce_loss(d5,labels_v)<br>	loss6 = bce_loss(d6,labels_v)<br><br>	loss = loss0 + loss1 + loss2 + loss3 + loss4 + loss5 + loss6<br>	<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;l0: %3f, l1: %3f, l2: %3f, l3: %3f, l4: %3f, l5: %3f, l6: %3f\n&quot;</span>%(loss0.data[<span class="hljs-number">0</span>],loss1.data[<span class="hljs-number">0</span>],loss2.data[<span class="hljs-number">0</span>],loss3.data[<span class="hljs-number">0</span>],loss4.data[<span class="hljs-number">0</span>],loss5.data[<span class="hljs-number">0</span>],loss6.data[<span class="hljs-number">0</span>]))<br><br>	<span class="hljs-keyword">return</span> loss0, loss<br><br><span class="hljs-comment">#每个mask计算二值交叉熵最后相加</span><br>d0, d1, d2, d3, d4, d5, d6 = net(inputs_v)<br>        loss2, loss = muti_bce_loss_fusion(d0, d1, d2, d3, d4, d5, d6, labels_v)<br><br>        loss.backward()<br>        optimizer.step()<br></code></pre></td></tr></table></figure>

<p>可以说整个代码清晰简单好懂。但是效果确实好，我觉得好的分割检测模型就该这样，大道至简。</p>
<p>分割结果如下图所示，在我的实际使用中发现，如果只是前背景分割不涉及语义分割，在几个项目中有极为优秀的表现，即使背景很复杂，同时实时性非常优秀（p100前向能做到18ms）。</p>
<h4 id="deeplab"><a href="#deeplab" class="headerlink" title="deeplab"></a>deeplab</h4><p>DeepLab系列一共有四篇文章，分别对应DeepLab V1、DeepLab V2、DeepLab V3和DeepLab V3+。</p>
<p><strong>DeepLab V1</strong></p>
<p>论文题目：<a href="https://link.juejin.cn/?target=https://arxiv.org/abs/1606.00915">Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs</a></p>
<p>开源代码：<a href="https://link.juejin.cn/?target=https://github.com/TheLegendAli/DeepLab-Context">TheLegendAli&#x2F;DeepLab-Context</a></p>
<p><strong>DeepLab V2</strong></p>
<p>论文题目：<a href="https://link.juejin.cn/?target=https://arxiv.org/abs/1606.00915">DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</a></p>
<p>开源代码：<a href="https://link.juejin.cn/?target=https://github.com/DrSleep/tensorflow-deeplab-resnet">DrSleep&#x2F;tensorflow-deeplab-resnet</a></p>
<p><strong>DeepLab V3</strong></p>
<p>论文题目：<a href="https://link.juejin.cn/?target=https://arxiv.org/abs/1706.05587">Rethinking Atrous Convolution for Semantic Image Segmentation</a></p>
<p>开源代码：<a href="https://link.juejin.cn/?target=https://github.com/eveningdong/DeepLabV3-Tensorflow">leonndong&#x2F;DeepLabV3-Tensorflow</a></p>
<p><strong>DeepLab V3+</strong></p>
<p>论文题目：<a href="https://link.juejin.cn/?target=https://arxiv.org/abs/1802.02611">Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</a></p>
<p>开源代码：<a href="https://link.juejin.cn/?target=https://github.com/jfzhang95/pytorch-deeplab-xception">jfzhang95&#x2F;pytorch-deeplab-xception</a></p>
<p>图像分割CNN是根据classification这种high-level semantics改编的，但CNN做语义分割时精准度不够，根本原因是 DCNNs 的高级特征的平移不变性，即高层次特征映射，根源于重复的池化和下采样会丢失localization信息，即无法对像素点精确定位语义（low-level semantics）。</p>
<p>针对下采样或池化降低分辨率，DeepLab采用了空洞卷积代替池化操作来扩展感受野，获取更多的上下文信息。同时DeepLab v1v2 结合了深度卷积神经网络（DCNNs）和概率图模型（DenseCRFs）的方法。DeepLab v2提出了串行的ASPP模块，ASPP增强了网络在多尺度下多类别分割时的鲁棒性， 使用不同的采样比例与感受野提取输入特征，能在多个尺度上捕获目标与上下文信息，虽然大大扩展了卷积核的感受野，但<strong>随着感受野越来越接近图像大小，会退化为1x1卷积。</strong></p>
<p>为了解决这个问题，<strong>DeepLab v3改进了ASPP空洞卷积空间金字塔池化层，不同的dilation卷积并行操作，然后归一尺寸后求和</strong>。ASPP模块借鉴PSPNet思想，通过不同采样率的空洞卷积并行采样，捕捉图像不同尺度的上下文信息。</p>
<p>DeepLab v3+通过添加一个简单而有效的解码器模块扩展DeepLab v3以优化分割结果，在PASCAL VOC 2012数据集和Cityscapes数据集中分别取得了89%和82.1%的MIOU。</p>
<p><strong>语义分割面临的主要挑战</strong></p>
<p><strong>分辨率</strong> 连续的池化或下采样操作会导致图像的分辨率大幅度下降，从而损失了原始信息， 且在上采样过程中难以恢复。因此，越来越多的网络都在试图减少分辨率的损失， 比如使用空洞卷积，或者用步长为2的卷积操作代替池化。</p>
<p><strong>多尺度特征</strong> 同一张图片中不同大小物体的分割精度不同，因为不同尺度卷积核对不同大小物体的分割效果不同。在分辨率较小的情况下，小物体的位置信息经常被丢失，通过设置不同参数的卷积层或池化层， 提取到不同尺度的特征图。将这些特征图送入网络做融合，对于整个网络性能的提升很大。但是由于图像金字塔的多尺度输入，造成计算时保存了大量的梯度，从而导致对硬件的要求很高。</p>
<h5 id="deeplab-v1"><a href="#deeplab-v1" class="headerlink" title="deeplab v1"></a>deeplab v1</h5><p>DeepLab v1是结合了深度卷积神经网络（DCNNs）和概率图模型（DenseCRFs）的方法</p>
<p>深度卷积神经网络（DCNNs）采用FCN思想，修改VGG16网络，得到 coarse score map并插值到原图像大小，使用Atrous convolution得到更dense且感受野不变的feature map</p>
<p>概率图模型（DenseCRFs）借用fully connected CRF对从DCNNs得到的分割结果进行细节上的refine。</p>
<p>DeepLab v1：VGG16+空洞卷积+CRF对边缘分割结果进行后处理。针对下采样或池化降低分辨率，DeepLab采用了空洞卷积来扩展感受野，获取更多的上下文信息。同时，采用完全连接的条件随机场（CRF）提高模型捕获细节的能力。</p>
<p><strong>网络结构</strong></p>
<p>把全连接层（fc6、fc7、fc8）改成卷积层（端到端训练） 2.把最后两个池化层（pool4、pool5）的步长2改成1（保证feature的分辨率下降到原图的1&#x2F;8）。 3.把最后三个卷积层（conv5_1、conv5_2、conv5_3）的dilate rate设置为2，且第一个全连接层的dilate rate设置为4（保持感受野）。 4.把最后一个全连接层fc8的通道数从1000改为21（分类数为21）。 5.第一个全连接层fc6， 通道数从4096变为1024， 卷积核大小从7x7变为3x3，后续实验中发现此处的dilate rate为12时（LargeFOV），效果最好。</p>
<p>网络变形： DeepLab-MSc：类似FCN，加入特征融合 DeepLab-7×7：替换全连接的卷积核大小为7× 7 DeepLab-4×4：替换全连接的卷积核大小为4× 4 DeepLab-LargeFOV：替换全连接的卷积核大小为3×3，空洞率为12</p>
<p>损失函数：交叉熵 + softmax 优化器：SGD + momentum 0.9 batchsize：20 学习率：10^−3（每经过2000个epoch，学习率 * 0.1）</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_20-29-10-20221204%2022:02:59.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_20-29-10"></p>
<h5 id="deeplab-v2"><a href="#deeplab-v2" class="headerlink" title="deeplab v2"></a>deeplab v2</h5><p>VGG16&#x2F;ResNet+串行的ASPP模块+CRF对边缘分割结果进行后处理。添加了ASPP空洞卷积空间金字塔池化层，通过不同的dilation卷积串行操作，来取代导致浅层特征损失的池化操作，大大扩大了感受野。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_20-29-29-20221204%2022:03:13.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_20-29-29"></p>
<p>空洞卷积：稠密映射，标准3×3卷积（rate为1），感受野为3；空洞卷积（rate为2），卷积核尺寸为5x5，感受野为7；空洞卷积（rate为4），卷积核尺寸为9x9，感受野为15。</p>
<p><strong>Network&amp;ASPP</strong></p>
<p>ASPP模块构成——&gt;DeepLab v1到DeepLab v2的进化——&gt;基于VGG16的DeepLab v2在v1的基础上做了进一步调整（FC6-FC8替换为ASPP）</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_20-32-20-20221204%2022:03:22.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_20-32-20"></p>
<p><strong>实验设置</strong></p>
<p>损失函数：交叉熵 + softmax 优化器：SGD + momentum 0.9 Batchsize：20 学习率策略：step：10^−3（每经过2000个epoch，学习率 * 0.1）</p>
<p>网络变形： LargeFOV：3×3卷积 + rate&#x3D;12(DeepLab v1最好结果) ASPP-S：r &#x3D; 2, 4, 8, 12 ASPP-L：r &#x3D; 6, 12, 18, 24</p>
<h5 id="deeplab-v3"><a href="#deeplab-v3" class="headerlink" title="deeplab v3"></a>deeplab v3</h5><p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_20-32-58-20221204%2022:03:54.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_20-32-58"></p>
<p>Deeplab v3：ResNet+改进后的并行ASPP模块。随着采样率的增大，有效滤波器权重的数量（应用于有效特征的权重而不是padding补充的0）变少，在空洞率接近特征映射大小的极端情况下，3×3滤波器不是捕获整个图像上下文，而是退化为简单的1×1卷积（只有中心滤波器权重是有效的）。因此，v3使用了并行ASPP模块，最后一个分支拼接全局池化模块来捕获全局上下文信息。</p>
<p><strong>语义分割常用特征提取框架</strong></p>
<ol>
<li>图像金字塔：从输入图像入手，将不同尺度的图像分别送入网络进行特征提取，后期再融合。</li>
<li>编解码结构：编码器部分利用下采样进行特征提取，解码器部分利用上采样还原特征图尺寸。 </li>
<li>深度网络vs空洞卷积：经典分类算法利用连续下采样提取特征，而空洞卷积是利用不同的采样率。 </li>
<li>空间金字塔结构：除ASPP外，仍有其他网络使用了该思想，如SPPNet、PSPNet等。</li>
</ol>
<p><strong>网络结构</strong></p>
<p>经典分类算法网络架构，如ResNet ——&gt; DeepLab v3空洞卷积串行网络结构 ——&gt; DeepLab v3空洞卷积并行网络结构（调整了ASPP模块）</p>
<p><strong>实验设置</strong></p>
<p>裁剪尺寸：裁剪图片至513x513（为了更好的拟合空洞率） 学习率策略：采用poly策略，原理同v2</p>
<p>BN层策略：当output_stride&#x3D;16时，batchsize&#x3D;16，同时BN层做参数衰减decay&#x3D;0.9997。在增强的数据集上，以初始学习率0.007训练30K后，冻结BN层参数。当output_stride&#x3D;8时，batchsize&#x3D;8，使用初始学习率0.001训练30K。</p>
<h5 id="deeplab-v3-1"><a href="#deeplab-v3-1" class="headerlink" title="deeplab v3+"></a>deeplab v3+</h5><p>DeepLabv3+的核心是通过添加一个简单而有效的解码器模块来恢复对象边界（沿着对象边界来细化分割结果），扩展了DeepLab v3。以Xcepition&#x2F;ResNet为骨架，采用深度可分离卷积进行编码，在多尺度特征提取ASPP模块后再接一个简单的解码器模块。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_20-35-03-20221204%2022:04:02.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_20-35-03"></p>
<p><strong>补充知识：深度可分离卷积</strong></p>
<p>标准卷积：标准输入图片尺寸为12×12×3，用1个5×5×3的卷积核进行卷积操作，会得到8×8×1的输出； 用256个5×5×3的卷积核进行卷积操作，会得到8×8×256的输出。 参数计算：256×5×5×3 &#x3D; 19200</p>
<p>分组卷积：组卷积是对输入特征图进行分组，每组分别进行卷积。 假设输入特征图的尺寸为C<em>H</em>W (12× 5×5)，输出特征图的数量为N (6)个，如果设定要分成G (3)个groups，则每组的输入特征图数量为C&#x2F;G (4)，每 组 的 输出特征图数量为N&#x2F;G (2)，每个卷积核的尺寸为(C&#x2F;G)<em>K</em>K (4×5×5)，卷积核的总数仍为N (6)个，每组的卷积核数量为N&#x2F;G (2)，每个卷积核只与其同组的输入特征图进行卷积，卷积核的总参数量为N*(C&#x2F;G)<em>K</em>K，可见，总参数量减少为原来的1&#x2F;G。</p>
<p>深度可分离卷积是组卷积的一种极端情况，也就是输入有多少个通道，对应的分组就有多少个组，即分组的组数&#x3D;输入特征图的通道数。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_20-36-24-20221204%2022:04:14.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_20-36-24"></p>
<p>深度可分离卷积 &#x3D; 深度卷积 + 逐点卷积</p>
<p>深度卷积：每个5×5×1的卷积核对应输入图像中的一个通道，得到三个8×8×1的输出， 拼接后得到8×8×3的结果</p>
<p>逐点卷积：设置256个1×1×3的卷积核，对深度卷积的输出再进行卷积操作，最终得到 8×8×256的输出</p>
<p>参数计算：*深度卷积参数 &#x3D; 5×5×3 &#x3D; 75 逐点卷积参数 &#x3D; 256×1×1×3 &#x3D; 768 总参数 &#x3D; 75 + 768 &#x3D; 843 &lt;&lt; 19200</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_20-37-18-20221204%2022:04:24.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_20-37-18"></p>
<p>网络结构</p>
<p>编码器：</p>
<ol>
<li>使用DeepLab v3作为编码器结构，输出与输入尺寸之比16(output_stride &#x3D; 16)。</li>
<li>ASPP：一个1×1卷积 + 三个3×3卷积(rate &#x3D; {6, 12, 18}) + 全局平均池化。</li>
</ol>
<p>解码器：</p>
<ol>
<li>先把encoder的结果上采样4倍（双线性插值），然后与编码器中相对应尺寸的特征图进行拼接融合，再进行3x3的卷积， 最后上采样4倍得到最终结果</li>
<li>融合低层次信息前，先进行1x1的卷积， 目的是降低通道数。</li>
</ol>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_20-38-00-20221204%2022:04:35.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_20-38-00"></p>
<p>DeepLab v3+对Xception进行了微调：</p>
<ol>
<li><p>更深的Xception结构，原始middle flow迭代8次，微调后迭代16次。 </p>
</li>
<li><p>所有max pooling结构被stride&#x3D;2的深度可分离卷积替代。 </p>
</li>
<li><p>每个3x3的depthwise convolution（结合了空洞卷积）后都跟BN和Relu。</p>
</li>
</ol>
<h5 id="deeplab总结"><a href="#deeplab总结" class="headerlink" title="deeplab总结"></a>deeplab总结</h5><p>v1：修改经典分类网络(VGG16)，将空洞卷积应用于模型中，试图解决分辨率过低及提取多尺度特征问题，用CRF做后处理（VGG16+空洞卷积+CRF对边缘分割结果进行后处理）</p>
<p>v2：设计ASPP模块，将空洞卷积的性能发挥到最大，沿用VGG16作为主网络，尝试使用ResNet-101进行对比实验，用CRF做后处理（VGG16&#x2F;ResNet+串行的ASPP模块+CRF对边缘分割结果进行后处理）</p>
<p>v3：以ResNet为主网络，设计了一种串行和一种并行的DCNN网络，微调ASPP模块，取消CRF做后处理（ResNet+改进后的并行ASPP模块）</p>
<p>v3+：以ResNet或Xception为主网络，结合编解码结构设计了一种新的算法模型，以v3作为编码器结构，另行设计了解码器结构，取消CRF做后处理（ResNet&#x2F;Xception+并行的ASPP模块+编码器结构）</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_20-39-38-20221204%2022:04:44.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_20-39-38"></p>
<h4 id="maskrcnn"><a href="#maskrcnn" class="headerlink" title="maskrcnn"></a>maskrcnn</h4><h3 id="6-行为识别：对于视频序列"><a href="#6-行为识别：对于视频序列" class="headerlink" title="6. 行为识别：对于视频序列"></a>6. 行为识别：对于视频序列</h3><h4 id="slowfast"><a href="#slowfast" class="headerlink" title="slowfast"></a>slowfast</h4><p>SlowFast是Facebook在2019年ICCV的一篇视频识别论文，受到灵长类动物的视网膜神经细胞种类的启发（大约80%的细胞（P-cells）以低频运作，可以识别细节信息；而大约20%的细胞（M-cells）则以高频运作，对时间的变化敏感）。作者<strong>提出了一种新的快慢网络SlowFast架构，来实现两个分支分别对时间与空间维度进行处理分析</strong>。在Kinetics-400动作识别benchmark与AVA动作检测benchmark中得到了state-of-art的结果。</p>
<p>SlowFast算法整体由两个卷积分支组成：</p>
<ul>
<li><strong>Slow分支</strong>：较少的帧数以及较大的通道数学习空间语义信息。</li>
<li><strong>Fast分支</strong>：较大的帧数以及较少的通道数学习运动信息</li>
</ul>
<p>计算量与通道数的平方成正比，Fast分支由于通道数较少，其比较轻量化，仅仅占用整体20%的计算量。</p>
<p>Slow分支使用一个较大的步长τ来采集视频帧，通常设置τ为16，如果针对帧率为30的视频，这意味着大约1秒可以采集2帧，即T&#x3D;2。slow分支通道数为D，</p>
<p>Fast分支使用一个较小的步长来采集视频帧，步长为τ&#x2F;α，其中α通常设置为8，因此针对帧率为30的视频，1秒可以采集15帧（αT）。<strong>Fast分支通过使用较小的通道数（β D）来保持轻量化，β通常设置为的⅛</strong>。</p>
<p>Slow通道和Fast通道都使用3D卷积的RestNet模型。在每个分支的末端，SlowFast执行全局平均池化，然后concat两个通道的特征进行类别预测。</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_20-44-41-20221204%2022:05:02.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_20-44-41"></p>
<p>上图是一个SlowFast网络结构。卷积核的尺寸记作{T×S², C} ，其中T、S和C分别表示时序temporal, 空间spatial和频道Channel的数目。stride记作{temporal stride, spatial stride ^ 2}。 α &#x3D; 8 ，β &#x3D; 1&#x2F;8。τ &#x3D; 16。绿色表示高一些的时序分辨率，Fast通道中的橙色表示较少的通道。</p>
<p><strong>侧向连接</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_20-47-29-20221204%2022:05:12.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_20-47-29"></p>
<p>Slow与Fast分支提取的特征需要进行融合，如上图，SlowFast采用的是将Fast分支的特征通过侧向连接（Lateral connections）送入Slow分支进行混合。但是两个分支的特征维度是不一致的（Fast分支是{αT, S², βC} 而Slow分支是 {T, S², αβC}），因此SlowFast需要对Fast分支的结果进行数据变换。</p>
<p>论文给出了三种进行数据变换的技术思路，其中第三个思路在实践中最有效。</p>
<ol>
<li>Time-to-channel：将{αT, S², βC} 的特征变形为 {T , S², αβC}之后就行融合，就是说把α帧压入一帧</li>
<li>Time-strided采样：简单地每隔α帧进行采样，{αT , S², βC} 就变换为 {T , S², βC}</li>
<li>Time-strided卷积: 用一个5×1×1的3d卷积， 输出通道为2βC，步长为 α.</li>
</ol>
<p>这三种方案的实验结果如下图所示，Time-Strided卷积的效果最好。</p>
<p>此外作者还尝试了双向侧链接，即将Slow分支结果也送入Fast分支，但是对性能没有改善。</p>
<h4 id="3d卷积"><a href="#3d卷积" class="headerlink" title="3d卷积"></a>3d卷积</h4><p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_20-58-44-20221204%2022:05:26.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_20-58-44"></p>
<p>a)和b)分别为2D卷积用于单通道图像和多通道图像的情况（多通道图像可以指同一张图片的3个颜色通道，也可以指多张堆叠在一起的图片）。对于每一个滤波器（卷积核），输出一张二维的特征图，多通道的信息被完全压缩了。输出的通道数由滤波器个数决定，有几个滤波器，输出就有几个通道。</p>
<p>而c)中3D卷积的输出仍然为3D的特征图。</p>
<p>现在考虑一个视频段输入，其大小为 c∗l∗h∗w ，其中c为图像通道(一般为3)，l为视频序列的长度，h和w分别为视频的宽与高。进行一次kernel_size为3∗3∗3，stride为1，padding&#x3D;‘same’，n_filters&#x3D;K的3D 卷积后，输出的大小为K∗l∗h∗w。池化同理。</p>
<p><strong>卷积核的维度</strong></p>
<p>卷积核的维度指的的进行滑窗操作的维度，而滑窗操作不在channel维度上进行，不管有几个channel，它们都共享同一个滑窗位置（虽然2D多channel卷积的时候每个channel上的卷积核权重是独立的，但滑窗位置是共享的）。所以在讨论卷积核维度的时候，是不把channel维加进去的。</p>
<p>2D conv的卷积核就是(c, k_h, k_w)，因此，对于RGB图像做2D卷积，卷积核可以是conv2D(3,3) 而不该是conv3D(3,3,3)</p>
<p>3D conv的卷积核就是(c, k_d, k_h, k_w)，其中k_d就是多出来的第三维，根据具体应用，在视频中就是时间维，在CT图像中就是层数维</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_21-03-07-20221204%2022:05:35.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_21-03-07"></p>
<p>针对单通道，与2D卷积不同之处在于，输入图像多了一个 depth 维度，故输入大小为(1, depth, height, width)，卷积核也多了一个k_d维度，因此卷积核在输入3D图像的空间维度（height,width和depth维度）上均进行滑窗操作，每次滑窗与 (k_d, k_h, k_w) 窗口内的values进行相关操作，得到输出3D图像中的一个value，最终输出一个3D的特征图。</p>
<p>这里的3D不是通道导致的，而是深度（多层切片，多帧视频），因此，虽然输入和卷积核和输出都是3D的，但都可以是单通道的。</p>
<p>针对多通道，输入大小为(3, depth, height, width)，与2D多通道卷积的操作类似，对于每次滑窗，卷积核同时与3个channels上的 (k_d, k_h, k_w) 窗口内的所有values进行相关操作，得到输出3D图像中的一个value。</p>
<p>由于3D卷积中的卷积核是3D的，因此在每个channel下使用的是同样的参数，权重共享。不同于2D多通道下的卷积核，后者在每一个channel使用的权重是一样的，不同的通道权重可能不一样。</p>
<h4 id="实践：视频异常检测"><a href="#实践：视频异常检测" class="headerlink" title="实践：视频异常检测"></a>实践：视频异常检测</h4><h3 id="7-transformer"><a href="#7-transformer" class="headerlink" title="7. transformer"></a>7. transformer</h3><h4 id="在视觉的应用vit"><a href="#在视觉的应用vit" class="headerlink" title="在视觉的应用vit"></a>在视觉的应用vit</h4><h5 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h5><p>重点介绍ViT原理，同时简单介绍三篇相关论文，这四篇论文的源码见 <a href="https://link.zhihu.com/?target=https://github.com/google-research/vision_transformer">https://github.com/google-research/vision_transformer</a></p>
<p><a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2010.11929">arXiv:2010.11929</a>：An image is worth 16x16 words: Transformers for image recognition at scale（ViT大法，一般人没钱做的工作）</p>
<p><a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2105.01601">arXiv:2105.01601</a>：MLP-Mixer: An all-MLP Architecture for Vision （用MLPs替代self-attention可以得到和ViT同样好的结果）</p>
<p><a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2106.01548">arXiv:2106.01548</a>：When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations （不使用大规模预训练和强数据增强ViT是否依然可以表现优秀）</p>
<p><a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2106.10270">arXiv:2106.10270</a>：How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers （通过大量实验，总共训练了超过5w个ViT，教你如何训练自己的ViT模型，以及数据增广和模型正则化什么时候有用）</p>
<p>有关transformer结构和原理，大家可以参考：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/410258597">Transformer解析</a></p>
<p>本文重点介绍ViT原理，同时简单介绍三篇相关论文，这四篇论文的源码见 <a href="https://link.zhihu.com/?target=https://github.com/google-research/vision_transformer">https://github.com/google-research/vision_transformer</a></p>
<p><a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2010.11929">arXiv:2010.11929</a>：An image is worth 16x16 words: Transformers for image recognition at scale（ViT大法，一般人没钱做的工作）</p>
<p><a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2105.01601">arXiv:2105.01601</a>：MLP-Mixer: An all-MLP Architecture for Vision （用MLPs替代self-attention可以得到和ViT同样好的结果）</p>
<p><a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2106.01548">arXiv:2106.01548</a>：When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations （不使用大规模预训练和强数据增强ViT是否依然可以表现优秀）</p>
<p><a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2106.10270">arXiv:2106.10270</a>：How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers （通过大量实验，总共训练了超过5w个ViT，教你如何训练自己的ViT模型，以及数据增广和模型正则化什么时候有用）</p>
<p>有关transformer结构和原理，大家可以参考：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/410258597">Transformer解析</a></p>
<h5 id="vit"><a href="#vit" class="headerlink" title="vit"></a>vit</h5><p>ViT是2020年Google团队提出的将Transformer应用在图像分类的模型，虽然不是第一篇将transformer应用在视觉任务的论文，但是因为其模型“简单”且效果好，可扩展性强（scalable，模型越大效果越好），成为了transformer在CV领域应用的里程碑著作，也引爆了后续相关研究</p>
<p>把最重要的说在最前面，ViT原论文中最核心的结论是，当拥有足够多的数据进行预训练的时候，ViT的表现就会超过CNN，突破transformer缺少归纳偏置的限制，可以在下游任务中获得较好的迁移效果</p>
<p>但是当训练数据集不够大的时候，ViT的表现通常比同等大小的ResNets要差一些，因为Transformer和CNN相比缺少归纳偏置（inductive bias），即一种先验知识，提前做好的假设。CNN具有两种归纳偏置，一种是局部性（locality&#x2F;two-dimensional neighborhood structure），即图片上相邻的区域具有相似的特征；一种是平移不变形（translation equivariance）， f(g(x))&#x3D;g(f(x)) ，其中g代表卷积操作，f代表平移操作。当CNN具有以上两种归纳偏置，就有了很多先验信息，需要相对少的数据就可以学习一个比较好的模型</p>
<p>ViT将输入图片分为多个patch（16x16），再将每个patch投影为固定长度的向量送入Transformer，后续encoder的操作和原始Transformer中完全相同。但是因为对图片分类，因此在输入序列中加入一个特殊的token，该token对应的输出即为最后的类别预测</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_21-11-40-20221204%2022:05:53.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_21-11-40"></p>
<p>按照上面的流程图，一个ViT block可以分为以下几个步骤</p>
<p>(1) patch embedding：例如输入图片大小为224x224，将图片分为固定大小的patch，patch大小为16x16，则每张图像会生成224x224&#x2F;16x16&#x3D;196个patch，即输入序列长度为<strong>196</strong>，每个patch维度16x16x3&#x3D;<strong>768</strong>，线性投射层的维度为768xN (N&#x3D;768)，因此输入通过线性投射层之后的维度依然为196x768，即一共有196个token，每个token的维度是768。这里还需要加上一个特殊字符cls，因此最终的维度是<strong>197x768</strong>。到目前为止，已经通过patch embedding将一个视觉问题转化为了一个seq2seq问题</p>
<p>(2) positional encoding（standard learnable 1D position embeddings）：ViT同样需要加入位置编码，位置编码可以理解为一张表，表一共有N行，N的大小和输入序列长度相同，每一行代表一个向量，向量的维度和输入序列embedding的维度相同（768）。注意位置编码的操作是sum，而不是concat。加入位置编码信息之后，维度依然是<strong>197x768</strong></p>
<p>(3) LN&#x2F;multi-head attention&#x2F;LN：LN输出维度依然是197x768。多头自注意力时，先将输入映射到q，k，v，如果只有一个头，qkv的维度都是197x768，如果有12个头（768&#x2F;12&#x3D;64），则qkv的维度是197x64，一共有12组qkv，最后再将12组qkv的输出拼接起来，输出维度是197x768，然后在过一层LN，维度依然是<strong>197x768</strong></p>
<p>(4) MLP：将维度放大再缩小回去，197x768放大为197x3072，再缩小变为<strong>197x768</strong></p>
<p>一个block之后维度依然和输入相同，都是197x768，因此可以堆叠多个block。最后会将特殊字符cls对应的输出 zL0 作为encoder的最终输出 ，代表最终的image presentation（另一种做法是不加cls字符，对所有的tokens的输出做一个平均），后面接一个MLP进行图片分类</p>
<p>关于image presentation</p>
<p>是否可以直接使用average pooling得到最终的image presentation，而不加特殊字符cls，通过实验表明，同样可以使用average pooling，原文ViT是为了尽可能是模型结构接近原始的Transformer，所以采用了类似于BERT的做法，加入特殊字符</p>
<p>关于positional encoding</p>
<p>1-D 位置编码：例如3x3共9个patch，patch编码为1到9</p>
<p>2-D 位置编码：patch编码为11,12,13,21,22,23,31,32,33，即同时考虑X和Y轴的信息，每个轴的编码维度是D&#x2F;2</p>
<p>实际实验结果表明，不管使用哪种位置编码方式，模型的精度都很接近，甚至不适用位置编码，模型的性能损失也没有特别大。原因可能是ViT是作用在image patch上的，而不是image pixel，对网络来说这些patch之间的相对位置信息很容易理解，所以使用什么方式的位置编码影像都不大</p>
<p>关于CNN+Transformer</p>
<p>既然CNN具有归纳偏置的特性，Transformer又具有很强全局归纳建模能力，使用CNN+Transformer的混合模型是不是可以得到更好的效果呢？将224x224图片送入CNN得到16x16的特征图，拉成一个向量，长度为196，后续操作和ViT相同</p>
<p>关于输入图片大小</p>
<p>通常在一个很大的数据集上预训练ViT，然后在下游任务相对小的数据集上微调，已有研究表明在分辨率更高的图片上微调比在在分辨率更低的图片上预训练效果更好（It is often beneficial to fine-tune at higher resolution than pre-training）（参考<em>2019-NIPS-Fixing the train test resolution discrepancy</em>）</p>
<p>当输入图片分辨率发生变化，输入序列的长度也发生变化，虽然ViT可以处理任意长度的序列，但是预训练好的位置编码无法再使用（例如原来是3x3，一种9个patch，每个patch的位置编码都是有明确意义的，如果patch数量变多，位置信息就会发生变化），一种做法是使用插值算法，扩大位置编码表。但是如果序列长度变化过大，插值操作会损失模型性能，这是ViT在微调时的一种局限性</p>
<p>当在很大的数据集上预训练时，ViT性能超越CNN。</p>
<h5 id="MLPs"><a href="#MLPs" class="headerlink" title="MLPs"></a>MLPs</h5><p>ViT作者团队出品，在CNN和Transformer大火的背景下，舍弃了卷积和注意力机制，提出了MLP-Mixer，一个完全基于MLPs的结构，其MLPs有两种类型，分别是<strong>channel-mixing MLPs</strong>和<strong>token-mixing MLPs</strong>，前者独立作用于image patches（融合通道信息），后者跨image patches作用（融合空间信息）。实验结果表明该结构和SOTA方法同样出色，证明了convolution和attention不是必要操作，如果将其替换为简单的MLP，模型依然可以完美work</p>
<p>mixer结构</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_21-18-33-20221204%2022:06:06.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_21-18-33"></p>
<p>类似于ViT，首先进行patch embedding操作，一个Mixer Layer中包含了channel-mixing MLPs和token-mixing MLPs，但是Mixer不适用positional encoding，因为token-mixing MLPs对输入tokens的顺序非常敏感</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_21-23-06-20221204%2022:06:17.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_21-23-06"></p>
<p>token-mixing MLPs：允许信息在空间维度交互，独立作用于每一个channel，作用于列，融合不同token的特征</p>
<p>channel-mixing MLPs：允许信息在通道交互，独立作用于每一个token，作用于行，融合不同channel的特征</p>
<p>输入image的分辨率为 H×W ，patch的分辨率为 P×P，则patch的数量 S&#x3D;HW&#x2F;P2 ，所有的patch拉直后线性投影到维度 C ，则得到Mixer Layer的输入 X∈RS×C 。token-mixing MLPs作用于 X 的列，特征维度不发生变化（ RS→RS ），channel-mixing MLPs作用于 X 的行，特征维度同样不发生变化（ RC→RC ）。每一个MLP包含两个全连接层和一个非线性激活（GELU），一个Mixer layers的公式如下，计算复杂度和输入patches的数量成线性关系（ViT是平方关系）</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_21-24-02-20221204%2022:06:25.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_21-24-02"></p>
<p>需要注意token-mixing MLPs共享参数，channel-mixing MLPs同样共享参数，因此避免了当输入特征维度增加（ C 变大）或者输入序列长度增加（ S 变大）时，模型参数量急剧增加的情况，极大减少了内存消耗</p>
<p>模型最后接global average pooling+a linear classifier</p>
<p><strong>实验结果</strong></p>
<p>当在大规模数据集上预训练（100million images），Mixer可以接近CNNs和Transformers的SOTA表现，在ImageNet上达到87.94%的top-1 accuracy；当在更小规模数据集上预训练时（10million），结合一些regularization techniques，Mixer可以接近ViT的性能，但是稍逊于CNN</p>
<h5 id="without-Pre-training-or-Strong-Data-Augmentations"><a href="#without-Pre-training-or-Strong-Data-Augmentations" class="headerlink" title="without Pre-training or Strong Data Augmentations"></a>without Pre-training or Strong Data Augmentations</h5><p>ViTs和MLPs的相关研究目前大多都非常依赖海量数据，在大规模数据集上的预训练以及强数据增广都是基本操作，但是在模型实际优化过程中依然存在诸多困难，比如对初始化和学习率非常敏感。因此作者从损失几何（loss geometry&#x2F;loss landscape geometry）的角度探究ViTs和MLP-Mixers（从损失几何的角度说白了就是用SAM方法对loss做平滑），<strong>旨在让模型摆脱对大规模预训练以及强数据增广的依赖</strong>，提升模型在训练阶段对数据的利用效率，以及推理阶段的泛华能力（intending to improve the models’ data efficiency at training and generalization at inference）</p>
<p>通过可视化和 Hessian 发现了收敛模型极其尖锐的局部最小值（sharp local minima of the converged models），因此使用最近提出的锐度感知优化器（sharpness-aware optimizer&#x2F;sharpness-aware minimizer，<strong>SAM</strong>）提高平滑度（promote smoothness），得到更加平滑的损失函数（much flatter loss landspace&#x2F; smoothed loss landspace），大大提升了ViTs和MLPs在多个任务上的准确度和鲁棒性，包括监督、对抗、对比、迁移学习等（使用简单的 Inception 式预处理，ViT-B&#x2F;16 和 Mixer-B&#x2F;16 在 ImageNet 上的top-1准确率分别提升了5.3% 和11.0%）</p>
<p>改进的平滑度归因于前几层中较稀疏的激活神经元（the improved smoothness attributes to sparser active&#x2F;activated neurons in the first few layers）。在没有大规模预训练或强数据增强的情况下，在 ImageNet 上从头开始训练时，所得 ViT 的性能优于类似大小和吞吐量（throughput）的 ResNet。还拥有更敏锐的注意力图（more perceptive attention maps）</p>
<h5 id="how-to-train-your-vit"><a href="#how-to-train-your-vit" class="headerlink" title="how to train your vit"></a>how to train your vit</h5><p>ViT在很多视觉任务上都展现了相当优秀的性能，但是和CNN相比，缺少归纳偏置让ViT应用于小数据集时非常依赖模型正则化（model regularization）和数据增广（data augmentation）（把模型正则化和数据增广合起来简称AugReg）</p>
<p>作者使用系统性的实证研究方法（systematic empirical study），探究训练数据量、AugReg、模型大小、计算成本的interpaly（相互影响&#x2F;影响），说白了就是做了大量实验，用实验结果说明问题，总共训练了超过50000个ViT模型，结果发布在了 <a href="https://link.zhihu.com/?target=https://github.com/rwightman/pytorch-image-models">https://github.com/rwightman/pytorch-image-models</a> 和 <a href="https://link.zhihu.com/?target=https://github.com/google-research/vision_">https://github.com/google-research/vision_ transformer</a></p>
<p>实验表明，当增加计算成本（Improved compute&#x2F;Increased compute budget），即让模型训练更长时间已达到一定的性能，同时使用AugReg会带来意想不到的效果：在ImageNet-21k（14million）上训练的ViT模型，和在JFT-300M上训练的ViT模型相比拥有更好的性能。同时大量实验也揭示了各类techniques的对模型性能的影响，以及什么时候AugReg对模型性能有益&#x2F;什么时候无益</p>
<p>作者还对ViT迁移学习进行了深入分析。结论是<strong>即使下游数据似乎与预训练数据只有微弱的关联，迁移学习仍然是最佳选择。</strong>作者分析还表明，对于迁移学习来说，训练数据更多的模型和数据增强更多的模型相比较（among similarly performing pre-trained models），前者可能是更好的选择，下游任务性能表现更好。该研究的意义在哪，当我们的计算成本有限时，可以通过本研究的结论选择一种方式，更高效的优化ViT模型</p>
<h4 id="swintransformer"><a href="#swintransformer" class="headerlink" title="swintransformer"></a>swintransformer</h4><p>“Swin Transformer: Hierarchical Vision Transformer using Shifted Windows” 是MSRA今年三月份上传到arXiv的一篇论文，论文中提出了一种新型的Transformer架构(Swin Transformer)，其利用滑动窗口和分层结构使得Swin Transformer成为了机器视觉领域新的Backbone，在图像分类、目标检测、语义分割等多种机器视觉任务中达到了SOTA水平。</p>
<p>Paper：<a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2103.14030v1?ref=hackernoon.com">https://arxiv.org/abs/2103.14030v1?ref=hackernoon.comarxiv.org/abs/2103.14030v1?ref=hackernoon.com</a></p>
<p>Transformer应用在机器视觉领域遇到的问题：</p>
<ol>
<li>图片的scale变化非常大，非固定标准</li>
<li>相较于文本信息，图片有更大的分辨率，Transformer的计算复杂度是token数量的平方(图1,每个token都要与其他token计算QK值)，如果将每个像素值算作一个token，其计算量非常大，不利于在多种机器视觉任务中的应用</li>
</ol>
<p>Swin Transformer 解决以上问题的方法</p>
<ol>
<li>通过与CNN相似的分层结构来处理图片，使得模型能够灵活处理不同尺度的图片</li>
</ol>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_21-31-45-20221204%2022:06:38.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_21-31-45"></p>
<p>其中涉及到窗口self-attention计算，不重叠窗口间的联系，mask的计算，相对位置编码的计算。</p>
<ol start="2">
<li>Swin Transformer 采用的是window self-attention，降低了计算复杂度。</li>
</ol>
<p>假设一张图片共有 h∗w 个patches(每个patches是原图4*4像素区域），每个窗口包括 M∗M 个patches，根据图1，</p>
<p>原始Transformer self-attention计算复杂度 &#x3D; $(hw)^2$</p>
<p>在Swin Transformer中采用的是window self-attention，其计算复杂度为窗口计算复杂度*窗口数量，窗口数量&#x3D; $h∗w&#x2F;M^2$ ,窗口计算复杂度&#x3D; $(M2)^2$ ，</p>
<p>Swin Transformer self-attention计算复杂度 &#x3D; $h∗w&#x2F;M^2∗(M^2)^2&#x3D;M^2∗(hw)$</p>
<p>计算复杂度由patches数量的平方关系降低到线性关系。</p>
<p>论文的总体结构图</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_21-34-43-20221204%2022:06:50.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_21-34-43"></p>
<p>程序的逻辑框架图</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_21-35-11-20221204%2022:07:00.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_21-35-11"></p>
<p>程序主要分为三个部分：</p>
<ol>
<li><p>图像处理：将RGB像素分辨率的图像转化为patches分辨率图像并根据具体模型大小改变输入通道数(图5)。</p>
</li>
<li><p>Swin Transformer stage：实现window self-attention及shifted window self-attention，通过Patch Merging实现分层结构，降低计算复杂度并能够处理不同尺度的图片。图片中*2代表此处有两个Swin Transformer Block，模型大小不同对应的Swin Transformer Block个数不同(图5)。</p>
</li>
<li><p>不同视觉任务输出：不同任务有不同的输出，此处以图像分类任务为例进行说明，所以Part2输出根据任务要求转换成对应的输出。</p>
</li>
</ol>
<p><strong>论文和程序中都要注意三个不同的概念，分别是resolution&#x2F; patches&#x2F; windows</strong></p>
<p><strong>resolution:</strong> 输入图片的分辨率是像素分辨率，程序Part 1 输入图片是像素分辨率，但是Part 2程序中对应的H&#x2F;W是patches 分辨率，不是像素分辨率。</p>
<p><strong>patches:</strong> 图像4<em>4像素区域称为一个patch，分类任务输入图像像素分辨率是224</em>224，patch_size &#x3D; 4，所以patches__resolution &#x3D; 56*56</p>
<p><strong>windows:</strong> 窗口大小由patches定义的，不是像素定义的，论文及程序中window_size &#x3D; 7，说明一个window有7*7&#x3D;49个patches</p>
<h4 id="基于transformer的detr目标检测"><a href="#基于transformer的detr目标检测" class="headerlink" title="基于transformer的detr目标检测"></a>基于transformer的detr目标检测</h4><p><a href="https://link.zhihu.com/?target=https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460205.pdf">DETR</a>的全称是DEtection TRansformer，是Facebook提出的基于Transformer的端到端目标检测网络，发表于ECCV2020，代码已开源：</p>
<p>作为Transformer用在目标检测领域的开山之作，DETR是CV领域学习Transformer绕不过的一道坎。前人栽树后人乘凉，学习一些经典的思路和代码对自己的提升也是巨大的。</p>
<p>DETR的思路和传统的目标检测的本质思路有相似之处，但表现方式很不一样。传统的方法比如Anchor-based方法本质上是对预定义的密集anchors进行类别的分类和边框系数的回归。DETR则是将目标检测视为一个集合预测问题（集合和anchors的作用类似）。由于Transformer本质上是一个序列转换的作用，因此，可以将DETR视为一个从图像序列到一个集合序列的转换过程。该集合实际上就是一个<strong>可学习的位置编码</strong>（文章中也称为object queries或者output positional encoding，代码中叫作query_embed）。</p>
<p>网络结构</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_21-37-34-20221204%2022:07:10.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_21-37-34"></p>
<p>DETR使用的Transformer结构和原始版本稍有不同：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/Snipaste_2022-12-04_21-38-20-20221204%2022:07:19.png" srcset="/blog/img/loading.gif" lazyload alt="Snipaste_2022-12-04_21-38-20"></p>
<p>详情了解：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/348060767">这里</a></p>
<h3 id="8-3d点云"><a href="#8-3d点云" class="headerlink" title="8. 3d点云"></a>8. 3d点云</h3><h4 id="pointnet"><a href="#pointnet" class="headerlink" title="pointnet"></a>pointnet</h4><h4 id="pointnet-1"><a href="#pointnet-1" class="headerlink" title="pointnet++"></a>pointnet++</h4><h4 id="点云补全pf-net"><a href="#点云补全pf-net" class="headerlink" title="点云补全pf-net"></a>点云补全pf-net</h4><h4 id="点云配准"><a href="#点云配准" class="headerlink" title="点云配准"></a>点云配准</h4><h3 id="9-目标追踪与姿态估计"><a href="#9-目标追踪与姿态估计" class="headerlink" title="9. 目标追踪与姿态估计"></a>9. 目标追踪与姿态估计</h3><blockquote>
<p> YOLO系列 + Deepsort系列 + Openpose系列</p>
</blockquote>
<h4 id="姿态估计openpose"><a href="#姿态估计openpose" class="headerlink" title="姿态估计openpose"></a>姿态估计openpose</h4><p>得到人体各个关键点位置，将他们按顺序进行拼接</p>
<p>难点：遮挡、匹配</p>
<p>COCO数据集提供17个关键点（openpose为18个关键点：脖子：左肩膀右肩膀做平均）</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220920212233033-20220920%2021:22:33.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220920212233033"></p>
<p>Top-down方法：先检测得到所有人的框（目标检测），再对每一个框进行姿态估计输出结果。优点：准；缺点：依赖目标检测、非极大值抑制会让重叠目标或小目标丢失、算法复杂度和人数成正比，不适合实时</p>
<p><strong>Multi-Person Pose Estimation using Part Affinity Fields</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220920214803080-20220920%2021:48:03.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220920214803080"></p>
<p>获得关键点：通过热度图得到每一个关键点的预测结果（每个关键点单独预测，18个关键点需要18张特征图）</p>
<p>拼接：找到最合适的拼接方向PAF（19个拼接方式，19*2个特征图）</p>
<blockquote>
<p>在标签中，设计PAF来表示关键点连接向量</p>
<p>PAF标签定义：将两个关键点的V向量做成单位向量，在手臂上任何一点P的向量与V一致，论文中给定了距离阈值</p>
<img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220920220704638-20220920%2022:07:04.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220920220704638"/>

<img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220920220955266-20220920%2022:09:55.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220920220955266" style="zoom:50%;" />

<p>求$X_{j_1}$和$X_{j_2}$间各点的PAF在险段上投影的积分，线段上各点的PAF方向如果与线段的方向越接近权值就越大</p>
<p>找每个头和每个左肩膀投影的得分值，找到最大的，就是最好最合适的方向</p>
</blockquote>
<p>匹配方法：固定成二分，可以直接套用匈牙利算法，每个点先与其他一个点做匹配</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220921140114594-20220921%2014:01:14.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220921140114594"></p>
<p><strong>整体框架：</strong></p>
<ul>
<li>分别经过两个分支得到各自预测结果</li>
<li>基于各分支结果组成骨架</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220921140403694-20220921%2014:04:03.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220921140403694"></p>
<p>最早：Convolutional Pose Machines:</p>
<ul>
<li>为OpenPose后面的工作奠定了基础，也可以当作基础框架</li>
<li>通过多个stage来不断优化关键点位置（stage1预测完全错误，2，3在纠正）</li>
<li>stage越多相当于层数越深，模型感受野越大，姿态估计需要更大感受野</li>
<li>每个stage都要加损失函数，需要中间过程也做得好</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220921140654630-20220921%2014:06:54.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220921140654630"></p>
<p>openpose的做法：</p>
<ul>
<li>两个网络结构分别搞定：1.关键点预测，2.姿势的‘亲和力’向量</li>
<li>多个stage，相当于纠正的过程，不断调整预测结果</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220921141931643-20220921%2014:19:31.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220921141931643"></p>
<blockquote>
<p>代码：</p>
<p>预存：Realtime_Multi-Person_Pose_Estimation</p>
<p>github相关：（关键词：openpose）</p>
<p>Hzzone pytorch-openpose：<a target="_blank" rel="noopener" href="https://github.com/Hzzone/pytorch-openpose">https://github.com/Hzzone/pytorch-openpose</a></p>
<p>c++版的，star更多所以还是放在这里吧：<a target="_blank" rel="noopener" href="https://github.com/CMU-Perceptual-Computing-Lab/openpose">https://github.com/CMU-Perceptual-Computing-Lab/openpose</a></p>
</blockquote>
<h4 id="deepsort"><a href="#deepsort" class="headerlink" title="deepsort"></a>deepsort</h4><p>卡尔曼滤波</p>
<ul>
<li><p>根据已知信息估计最优位置</p>
</li>
<li><p>本质是优化估计算法</p>
</li>
<li><p>例如估计人在下一帧的位置</p>
</li>
<li><p>基于估计值和观测值进行综合（如下一帧预测值和下一帧检测值）</p>
</li>
<li><p>卡尔曼增益的目的是让最优估计值的方差更小，相当于一个权重值，怎么利用估计和观测决定了卡尔曼滤波的核心作用</p>
</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220921153305685-20220921%2015:33:05.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220921153305685"></p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220921153606066-20220921%2015:36:06.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220921153606066"></p>
<ul>
<li>两大核心模块：prediction和update</li>
<li>预测阶段：预测状态估计值及其协方差（在单状态中，协方差矩阵就是其方差，他是预测状态中的不确定性的度量&#x2F;噪声导致的）</li>
<li>更新阶段：基于预测值更新参数，预测完需要根据观测值来修正，修正后的状态值去估计下一帧</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220921154801512-20220921%2015:48:01.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220921154801512"></p>
<p>追踪问题需要考虑的状态：</p>
<ul>
<li><p>均值（Mean）：8维向量表示为 x &#x3D; [cx, cy, r, h, vx, vy, vr, vh]</p>
</li>
<li><p>中心坐标（cx, cy），宽高比r，高h，以及各自的速度变化值组成</p>
</li>
<li><p>协方差矩阵：表示目标位置信息的不确定性，由8*8的矩阵表示</p>
</li>
<li><p>追踪过程也要分为两个阶段</p>
</li>
<li><p>每一个track都要预测下一时刻的状态，并基于检测到的结果来修正（匀速、线性）</p>
</li>
</ul>
<p>匈牙利算法：</p>
<ul>
<li>在完成匹配的同时最小化代价矩阵</li>
<li>算法过程：</li>
<li>对于矩阵的每一行，减去其中最小的元素</li>
<li>对于矩阵的每一列，减去其中最小的元素</li>
<li>（3）用最少的水平线或垂直线覆盖矩阵中所有的0</li>
<li>如果线的数量&#x3D;N，则找到了最优分配，算法结束，否则下一步</li>
<li>找到没有被任何线覆盖的最小元素，每个没被线覆盖的行减去这个元素，每个被线覆盖的列加上这个元素，返回（3）</li>
</ul>
<blockquote>
<p>调用：sklearn:linear_assignment(); scipy:linear_sum_assignment()</p>
</blockquote>
<ul>
<li>运动信息匹配（卡尔曼估计）、外观匹配（ReID）、IOU匹配（BBOX）</li>
</ul>
<p>ReID特征：</p>
<ul>
<li>追踪人所以用到了ReID，如果追踪其他目标需要自己训练</li>
<li>对输入的bbox进行特征提取，返回128维特征</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220921170526712-20220921%2017:05:26.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220921170526712"></p>
<ul>
<li>根据当前检测的所有bbox与当前所有track，先得到其所有ReID特征</li>
<li>当前每一个track均存了一个特征序列（就是每一次匹配会保留一份特征）</li>
<li>例如一个track有5份128维向量，选其与每个bbox余弦距离最小的作为输入</li>
<li>track保存的特征数量是有上限的，默认参数是100个</li>
</ul>
<p>追踪任务基本流程：</p>
<ul>
<li>目标检测+追踪（对检测的bbox提取各项特征后进行匹配）</li>
</ul>
<p>前身：sort算法（只对位置和运动信息进行匹配，不关注里面的内容）</p>
<ol>
<li><p>卡尔曼预测与更新</p>
</li>
<li><p>匈牙利匹配返回结果</p>
</li>
</ol>
<ul>
<li>将预测后的tracks和当前帧中的detections进行匹配（IOU匹配）</li>
<li>没有REID等深度学习特征</li>
</ul>
<p><strong>deepsort算法</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220921192135436-20220921%2019:21:35.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220921192135436"></p>
<ul>
<li>级联匹配:代价函数由运动特征(卡尔曼预测)与ReID特征计算的距离组成</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220921192412849-20220921%2019:24:12.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220921192412849"></p>
<p><strong>追踪流程拆解：</strong></p>
<ul>
<li>第一帧：检测得到10个bbox，此时没有track</li>
<li>对每一帧的检测结果，都会经过NMS和置信度阈值来筛选</li>
<li>不会执行任何的deepsort追踪操作</li>
<li>对当前10个检测结果分别初始化track</li>
<li>第二帧：检测得到11个bbox，如何跟上一帧初始化的track匹配呢？</li>
<li>进行IOU匹配（级联匹配需要对confirmed track，现在还没有）</li>
<li>此时匹配到10个track，注意匹配到的更新卡夫曼参数</li>
<li>还有一个bbox没有匹配到，为其新建一个bbox</li>
<li>第三帧：检测得到12个bbox，当前还没有confirm的track（命中3次）</li>
<li>前面创建了11个track要继续与12个bbox进行IOU匹配</li>
<li>其中一个track没有匹配上（此时是非confirm的，将其删除）</li>
<li>有10个track已经命中3次了，将其状态更改为confirm</li>
<li>第四帧：检测得到14个bbox，不仅要做IOU还要做级联匹配</li>
<li>对前面10个confirm的track进行级联匹配（优先），然后再IOU</li>
<li>只有confirm的track才会可视化在输出结果中</li>
<li>注意每次匹配到的track一定要更新其卡夫曼参数</li>
</ul>
<p><strong>追踪流程拆解总结：</strong></p>
<ul>
<li>检测得到当前帧的bbox（其实追踪好坏主要取决于检测结果）</li>
<li>track分为：confirmed和unconfirmed，待遇不同</li>
<li>对于confirmed要先进行级联匹配，优先级高，连续70帧没被匹配上会被删除</li>
<li>代价矩阵包括ReID特征构建的余弦距离与运动信息构建的马氏距离</li>
<li>对于级联匹配玩剩下的和当前是unconfirmed与剩下的bbox进行IOU匹配</li>
<li>经过级联与IOU匹配后就得到了所有匹配结果，同时要更新track参数</li>
<li>对于匹配成功的track，连续命中三次以上才能转换成confirmed</li>
<li>对于没有匹配成功的track，如果它是unconfirmed则删除</li>
</ul>
<blockquote>
<p>代码：</p>
<p>Yolov5_DeepSort_Pytorch</p>
<p>github地址：</p>
<p>mikel-brostrom &#x2F; Yolov5_DeepSort_Pytorch：<a target="_blank" rel="noopener" href="https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet">https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet</a></p>
</blockquote>
<h3 id="10-行人重识别"><a href="#10-行人重识别" class="headerlink" title="10. 行人重识别"></a>10. 行人重识别</h3><h4 id="基于注意力机制的reld"><a href="#基于注意力机制的reld" class="headerlink" title="基于注意力机制的reld"></a>基于注意力机制的reld</h4><blockquote>
<p>来源论文：Relation-Aware Global Attention，2020，CVPR</p>
</blockquote>
<p>主要贡献：</p>
<ol>
<li><p>对channel做权重</p>
</li>
<li><p>对空间做权重</p>
</li>
</ol>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220922145630351-20220922%2014:56:30.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220922145630351"></p>
<blockquote>
<p>代码：</p>
<p>github地址：</p>
<p>microsoft &#x2F; Relation-Aware-Global-Attention-Networks：<a target="_blank" rel="noopener" href="https://github.com/microsoft/Relation-Aware-Global-Attention-Networks">https://github.com/microsoft/Relation-Aware-Global-Attention-Networks</a></p>
<p>以resnet50为基础、</p>
<p>入口：main_imgreid.py，先写参数</p>
</blockquote>
<h4 id="基于attention的行人重识别"><a href="#基于attention的行人重识别" class="headerlink" title="基于attention的行人重识别"></a>基于attention的行人重识别</h4><h4 id="基于行人局部特征融合"><a href="#基于行人局部特征融合" class="headerlink" title="基于行人局部特征融合"></a>基于行人局部特征融合</h4><blockquote>
<p> 论文来源：Relation Network for Person Re-identification</p>
</blockquote>
<p>整体流程：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220922161312974-20220922%2016:13:13.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220922161312974"></p>
<p>流程解析：</p>
<ol>
<li>先对整体进行特征提取：现将数据resize，输入到resnet</li>
<li>将特征图分块，直接在h纬度进行截取</li>
<li>计算GCP特征：avg-max</li>
</ol>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220922161615425-20220922%2016:16:15.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220922161615425"></p>
<ol start="4">
<li>one vs rest：有点类似attention</li>
</ol>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220922161825106-20220922%2016:18:25.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220922161825106"></p>
<ol start="5">
<li>损失函数：依然是分类损失和triple loss，在六个点各计算了两种损失</li>
</ol>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220922162634214-20220922%2016:26:34.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220922162634214"></p>
<blockquote>
<p>代码：Relation Network for Person Re-identification</p>
<p>github:<a target="_blank" rel="noopener" href="https://github.com/cvlab-yonsei/RRID">https://github.com/cvlab-yonsei/RRID</a></p>
</blockquote>
<h4 id="基于图模型"><a href="#基于图模型" class="headerlink" title="基于图模型"></a>基于图模型</h4><h4 id="基于拓扑图：针对遮挡现象"><a href="#基于拓扑图：针对遮挡现象" class="headerlink" title="基于拓扑图：针对遮挡现象"></a>基于拓扑图：针对遮挡现象</h4><blockquote>
<p>来源论文：Learning Relation and Topology for Occluded Person Re-Identification</p>
</blockquote>
<p>提出了三阶段的模型，重点解决遮蔽现象的局部特征：</p>
<ol>
<li>关键点局部特征提取</li>
<li>图卷积融合关键点特征</li>
<li>基于图匹配的方式来计算相似度并训练模型</li>
</ol>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220922171534923-20220922%2017:15:35.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220922171534923"></p>
<p>一阶段：关键点局部特征提取</p>
<ul>
<li>选择一个pose estimation即可</li>
<li>得到的是各个关键点的热度图信息</li>
<li>通过热度图得到原始特征图的局部特征</li>
<li>同样也加了多个损失（local和global）</li>
<li>local和global的都要进行训练</li>
<li>全局特征是global average pooling得到</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220922190549304-20220922%2019:05:49.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220922190549304"></p>
<p>二阶段：局部特征关系整合（图卷积）</p>
<ul>
<li><p>通过加入关系更好的利用局部特征</p>
</li>
<li><p>先初始化邻接矩阵来进行图卷积（局部特征关系整合：得到邻接矩阵A来指导每个关键点特征如何跟其他关键点特征 进行计算，并且A矩阵也要进行学习）</p>
</li>
<li><p>邻接矩阵在学习过程中更新（顶点数：13个局部+1全局）（利用差异特征来学习邻接矩阵A）（用A来指导如何利用不同关键点的特征进行组合，最终再与输入的局部特征进行整合。）</p>
</li>
<li><p>更新完后与设定的模型mask。设定的模型没有的边直接删除</p>
</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220922194657966-20220922%2020:04:12.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220922194657966"></p>
<p>三阶段：图匹配</p>
<ul>
<li>回到AP、AN问题</li>
<li>输入两张图像（经过了前两阶段后的结果）</li>
<li>计算他们之间的相似度（其实输入是两组，第一组A,P; 第二组，A,N; ）</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220922201207388-20220922%2020:12:07.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220922201207388"></p>
<ul>
<li>图匹配就是要一个相似度矩阵U</li>
<li>例如14*14，表示两个图之间的关系</li>
<li>这里面是一个交叉的过程(cross)，分别交叉来得到各自匹配的特征结果</li>
<li>引入新的损失函数：验证损失</li>
<li>就是sigmoid(emb1, emb2)的结果</li>
</ul>
<p>每个阶段都有损失，前两个阶段的local和global，后一个阶段的Classification Loss&#x2F;Triplet Loss</p>
<blockquote>
<p>代码：HOReID</p>
</blockquote>
<h3 id="11-gan生成对抗网络"><a href="#11-gan生成对抗网络" class="headerlink" title="11. gan生成对抗网络"></a>11. gan生成对抗网络</h3><p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220922204254337-20220922%2020:42:54.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220922204254337"></p>
<blockquote>
<p>代码：gan</p>
</blockquote>
<p>主要是生成器、判别器、损失</p>
<h4 id="cyclegan-图像合成"><a href="#cyclegan-图像合成" class="headerlink" title="cyclegan 图像合成"></a>cyclegan 图像合成</h4><ul>
<li>junyanz &#x2F; pytorch-CycleGAN-and-pix2pix：<a target="_blank" rel="noopener" href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix">https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix</a></li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220922214545626-20220922%2021:45:45.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220922214545626"></p>
<h4 id="stargan"><a href="#stargan" class="headerlink" title="stargan"></a>stargan</h4><p>Cycle-Gan中如果要生成一种效果，需要训练一组配对的G和D(2+2)，如果需要多种效果，stargan可以用一个模型完成，stargan已经迭代到了第二版本</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220923143240344-20220923%2014:32:40.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220923143240344"></p>
<ul>
<li>G额外接受了编码信息（one-hot）</li>
<li>D也同样接受，只需一个D与G</li>
<li>整体流程类似cycle-gan，但只用一个G就可以生成效果</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220923143818459-20220923%2014:38:18.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220923143818459"></p>
<p><strong>整体流程：</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220923144903516-20220923%2014:49:03.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220923144903516"></p>
<p>详细展示：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220923145546095-20220923%2014:55:46.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220923145546095"></p>
<p>不仅是图像，也可以用于声音领域：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220923145636573-20220923%2014:56:36.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220923145636573"></p>
<p>v2版本</p>
<ul>
<li>多了mapping notwork和style encoder</li>
</ul>
<p>整体网络模型</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220923150316715-20220923%2015:03:16.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220923150316715"></p>
<p>Style reconstruction（编码器训练）</p>
<ul>
<li>风格编码器损失函数：$L_{sty} &#x3D; E_{x,\tilde y,z}[||\tilde s - E_{\tilde y}(G(x,\tilde s))||_1]$</li>
<li>其中 $\tilde s$ 是由mapping网络所得到的结果</li>
<li>相当于先给G一组风格向量，让它去生成，再对结果的结果进行编码</li>
<li>让编码后得到的风格向量与输入的mapping网络向量越接近越好</li>
</ul>
<p>Style diversification（多样化训练）</p>
<ul>
<li>结果得生成一些，多样化损失：$L_{ds} &#x3D; E_{x,\tilde y,z_1,z_2}[||G(x,\tilde s_1)-G(x,\tilde s_2||1]$</li>
<li>两组随机向量$z_1$，$z_2$通过mapping网络可以得到$s_1$与$s_2$：$\tilde s_i &#x3D; F_{\tilde y}(z_i) \text{for}\ i \in {1,2}$ </li>
<li>为了结果丰富，要使得当下计算结果越大越好，求损失时去负号</li>
</ul>
<p>cycle loss</p>
<ul>
<li>转换的只是特色，主体要保留：$L_{cyc} &#x3D; E_{x,y,\tilde y,z}[||x - G(G(x,\tilde s),\tilde s||_1]$</li>
<li>其中：$\tilde s &#x3D; E_y(x)$也就是原始图像本身的风格向量</li>
<li>训练时可以保证乳香不至于变得太离谱</li>
<li>这个损失函数其实与cycle gan基本是一样的</li>
</ul>
<blockquote>
<p>代码：stargan-v2</p>
<p>github：<a target="_blank" rel="noopener" href="https://github.com/clovaai/stargan-v2">https://github.com/clovaai/stargan-v2</a></p>
</blockquote>
<h4 id="starganvc2-变声器"><a href="#starganvc2-变声器" class="headerlink" title="starganvc2 变声器"></a>starganvc2 变声器</h4><blockquote>
<p>来源论文：StarGAN-VC2: Rethinking Conditional Methods for StarGAN-Based Voice Conversion</p>
<p>相关资料：<a target="_blank" rel="noopener" href="https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/stargan-vc2/index.html">https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/stargan-vc2/index.html</a></p>
</blockquote>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220923204726970-20220923%2020:47:27.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220923204726970"></p>
<p>类似变声器，转换语音特征的同时留下语音内容</p>
<p>VC：Voice Conversion</p>
<ul>
<li>构建变声器：思想类似stargan，细节完全不同</li>
<li>需要输入：1. 声音数据；2. 标签编码</li>
<li>整体来说还是GAN模型，主要解决数据特征提取，网络模型定义</li>
<li>Stargan-vc2是升级版，前身还有cyclegan-vc和stargan-vc</li>
</ul>
<p>输入数据</p>
<ul>
<li>VCC2016和VCC2018（这个数据比较小，也可以用其他的）</li>
<li>4个人的声音数据，相当于4个domain，他们之间相互转换</li>
<li>论文中选择的特征为：MCEPs；log F0；APs</li>
<li>输入特征为：batchsize*1*35*128（35是特征个数，128是指定特征纬度）</li>
<li>频率：每秒钟波峰所发生的数目称之为信号的频率，用单位千赫兹(kHz)表示</li>
<li>0.1毫秒完成4.8次采样，则1秒48000次采样，采样率48KHZ</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220923211127864-20220923%2021:11:27.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220923211127864"></p>
<p>预处理</p>
<ul>
<li>16KHZ重采样（经验值，与论文一致）</li>
<li>预加重：补偿高频信号，让高频信号权重更大一些，因为它信息多</li>
<li>分帧：类似时间窗口，得到多个特征段</li>
<li>论文中没有详细介绍预处理内容，源码中按照常用套路来做</li>
</ul>
<p>特征汇总</p>
<ul>
<li>基频特征(F0)：声音可以分解成不同频率的正弦波，其中最低的那个</li>
<li>频谱包络：语音是一个时序信号，如采样频率为16kHz的音频文件（每秒包含16000个采样点）分帧后得到了多个子序列，然后对每个子序列进行傅立叶变换操作，就得到了频率-振幅图（也就是描述频率-振幅图变化趋势的）</li>
<li>Aperiodic参数：基于F0与频谱包络计算得到</li>
</ul>
<p>MFCC</p>
<ul>
<li><p>梅尔倒谱系数：</p>
<p>流程：连续语音–预加重–加窗分帧–FFT（傅立叶变换）–MEL滤波器组–对数运算–DCT</p>
</li>
<li><p>通俗解释：FFT之后就把语音转换到频域，MEL滤波器变换后相当于得到更符合人类听觉的效果</p>
<img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220923215302181-20220923%2021:53:02.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220923215302181" style="zoom:50%;" />
</li>
<li><p>最后DCT相当于提取每一帧的包络（这里面特征多）</p>
</li>
</ul>
<p>网络架构</p>
<ul>
<li>生成器：输入就是提取好的特征，输出也就是特征</li>
<li>编码-解码的过程，其中引入了IN和GLU单元</li>
</ul>
<p>Instance Normalization</p>
<ul>
<li>变声期改变风格，不改变内容</li>
<li>编码时要留住原始内容，就要去掉声音中特性的部分</li>
<li>解码时要放大特性，需要再处理解码特征</li>
<li>Instance Normalization和Adaptive Instance Normalization</li>
<li>IN：使当前特征更平稳更平缓</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220923220524738-20220923%2022:05:24.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220923220524738"></p>
<ul>
<li>Adain：解码时要突出特色</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220923221322759-20220923%2022:13:22.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220923221322759"></p>
<p>小细节</p>
<ul>
<li>上采样与下采样：都是老路子，stride &#x3D; 2来下采样，反卷积来上采样</li>
<li>PixelShuffle(Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network)：Pixelshuffle会为 (∗ , r^2 x C, H, W) 的Tensor给reshape成 (∗ , C, rH, rW)</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220923221947540-20220923%2022:19:47.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220923221947540"></p>
<p>判别器</p>
<ul>
<li>GSP：global sum pooling：一个特征图压缩成一个点，batch*512*h*w压缩成batch*512</li>
<li>标签通过embedding编码成512维特征（batch*512），内积得到batch个判别结果</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220923222013672-20220923%2022:20:13.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220923222013672"></p>
<p>损失：对抗损失、分类损失、循环一致性损失</p>
<blockquote>
<p>代码：pytorch-StarGAN-VC2-master</p>
</blockquote>
<h4 id="超分辨率重构"><a href="#超分辨率重构" class="headerlink" title="超分辨率重构"></a>超分辨率重构</h4><blockquote>
<p>来源论文：Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.pdf">https://openaccess.thecvf.com/content_cvpr_2017/papers/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.pdf</a></p>
</blockquote>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220924163001559-20220924%2016:30:01.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220924163001559"></p>
<blockquote>
<p>代码：srgan</p>
<p><a target="_blank" rel="noopener" href="https://github.com/tensorlayer/srgan">https://github.com/tensorlayer/srgan</a></p>
</blockquote>
<h4 id="基于gan的图像补全"><a href="#基于gan的图像补全" class="headerlink" title="基于gan的图像补全"></a>基于gan的图像补全</h4><blockquote>
<p>来源论文：Globally and Locally Consistent Image Completion</p>
<p><a target="_blank" rel="noopener" href="http://iizuka.cs.tsukuba.ac.jp/projects/completion/data/completion_sig2017.pdf">http://iizuka.cs.tsukuba.ac.jp/projects/completion/data/completion_sig2017.pdf</a></p>
</blockquote>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220924165154555-20220924%2016:51:54.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220924165154555"></p>
<p>网络结构图：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220924165236234-20220924%2016:52:36.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220924165236234"></p>
<p>特征分两部分提取：局部的和全局的，进行一致性表达</p>
<p>整个网络由三大块组成：</p>
<ul>
<li>修复网络：把有空白的图像修复成正常的，完全由全卷积组成，不使用池化层</li>
<li>全局判别器：帮助训练</li>
<li>局部判别器：帮助训练</li>
</ul>
<blockquote>
<p>代码：基于GAN的图像补全实战&#x2F;glcic</p>
<p>其他相关：<a target="_blank" rel="noopener" href="https://github.com/otenim/GLCIC-PyTorch">https://github.com/otenim/GLCIC-PyTorch</a></p>
</blockquote>
<h3 id="12-强化学习"><a href="#12-强化学习" class="headerlink" title="12. 强化学习"></a>12. 强化学习</h3><p>强化学习就是不断与环境交互，需要大量的模拟数据来训练，训练就是不断尝试的过程，设置奖励机制，中间过程也可以设置奖励。</p>
<ul>
<li>机器人要不断与环境交互</li>
<li>互动就是得到下一步的指示（Action）</li>
<li>按照指示走下去势必会改变环境</li>
<li>继续与环境交互得到新的指示，并结合奖励机制来学习要不要这样做</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220927140839750-20220927%2014:08:39.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220927140839750"></p>
<p>如何通过状态得到当前合适的行为？</p>
<ul>
<li>通过神经网络，输入为state，输出为action</li>
<li>主要问题：如何训练网络？不是分类或回归任务了</li>
</ul>
<p>深度学习和强化学习的区别</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220927142616581-20220927%2014:26:16.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220927142616581"></p>
<h4 id="ppo算法"><a href="#ppo算法" class="headerlink" title="ppo算法"></a>ppo算法</h4><blockquote>
<p>Proximal Policy Optimization</p>
<p>关注目标函数定义方法、如何实际进行求解</p>
</blockquote>
<p>网络的输入与输出</p>
<ul>
<li>一次游戏的记录结果</li>
<li>包括了每一步的状态与行动 (trajectory) ：$\tau &#x3D; {s_1,a_1,s_2,a_2,s_3,…,s_T,a_T }$</li>
</ul>
<p>游戏的行为如何产生？</p>
<ul>
<li>其中$p(s_{t+1} | s_t,a_t)$是游戏自带的，$p_\theta(a_t|s_t)$是模型输出的结果，其中奖励是由当前第一步的action和state共同决定的（游戏提供的规则）</li>
<li>当前模型恰好得到了如下的游戏记录：</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220927144811119-20220927%2014:48:11.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220927144811119" style="zoom:40%;" />

<p>需要优化的是网络中的权重参数</p>
<ul>
<li><p>找到最优的 $\theta$ ，使得总奖励最大：$\theta ^\star &#x3D; \arg \max \limits_\theta E_{\tau - p_\theta(\tau)}[\sum \limits_t r(s_t,a_t)]$</p>
</li>
<li><p>加入期望，即便相同的 $\theta$ 得到的action也可能不同</p>
</li>
<li><p>大数定律：穷举以接近：（ i 表示玩了多少次，t 表示一次多少steps）</p>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220927150533422-20220927%2015:05:33.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220927150533422" style="zoom:33%;" />

<ul>
<li>计算梯度：$\pi_\theta (\tau)$ 表示当前序列可能性，$r(\tau)$ 作为奖励</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220927151416552-20220927%2015:14:16.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220927151416552" style="zoom:33%;" />

<img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220927151114134-20220927%2015:11:14.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220927151114134" style="zoom:33%;" />

<ul>
<li>化简到最终要求解的梯度：更新参数：$\theta \leftarrow \theta + \alpha ▼_\theta J(\theta)$（梯度上升）</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220927152016136-20220927%2015:20:16.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220927152016136" style="zoom:33%;" />

<p>铁三角训练模型：</p>
<img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220927152447107-20220927%2015:24:47.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220927152447107" style="zoom:50%;" />

<p>如何获得这么多游戏数据？</p>
<ul>
<li><p>持续游戏，并记录每场游戏的情况（$\tau ^1:(s_1^1,a_1^1) (s_2^1,a_2^1)，\tau^2:(s_1^2,a_1^2)(s_2^2,a_2^2)$）和奖励（$R(\tau^1)， R(\tau^2)$）</p>
</li>
<li><p>把数据带入最终要求的梯度公式求解即可</p>
</li>
</ul>
<p>改进：</p>
<ul>
<li><p>baseline方法：总的奖励与 $\theta$ 无关，可以看作一个权重项，可以做归一化，但有些游戏以奖励为主，惩罚较少，此时可以对总的奖励进行一个去均值操作：$R(\tau ^n) - b$ ，其中，$b &#x3D; 1&#x2F;N\sum^N_{i&#x3D;1}r(\tau)（减去平均均值即去均值）</p>
</li>
<li><p>On policy和Off policy：</p>
<ul>
<li>On policy：就是训练数据由当前agent与不断环境交互得到</li>
<li>Off policy：找一个 $\bar{\pi}(\tau)$ 去替代 $\pi_\theta(\tau)$</li>
<li>使用On policy策略训练太慢，做一批数据只是迭代一次</li>
</ul>
</li>
<li><p>Importance Sampling</p>
<ul>
<li>梯度策略要求我们不断产生样本数据：</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220927160851782-20220927%2016:08:51.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220927160851782" style="zoom:33%;" />

<ul>
<li>用另一种分布代替当前的分布：从 P 这个分布中不断采样 X ，再把 X 带入到 f(x) 中，再求 f(x) 的期望值，P’ 上场，将P换下来</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220927160958859-20220927%2016:09:58.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220927160958859" style="zoom:33%;" />

<ul>
<li><p>要满足的要求：P 与 P‘ 要尽量接近</p>
</li>
<li><p>从 P’ 中sample出数据供 $\theta$进行训练（这一批数据可以训练多次）</p>
<img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220927161622056-20220927%2016:16:22.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220927161622056" style="zoom:33%;" />
</li>
<li><p>条件限制：$\pi_{\theta’}$ 和 $\pi_\theta$ 要相近（KL-divergence）：$D_{KL}(\pi_{\theta’}||\pi_\theta) &#x3D; E_{\pi_{\theta’}}[\log \pi_\theta - \log \pi_{\theta’}]$</p>
</li>
<li><p>直观解释：$||\theta’ - \theta||^2 \le \epsilon$，实际中是二者经过网络的预测结果尽可能差不多</p>
</li>
<li><p>实际中可以直接拿训练模型的前一次迭代参数，PPO2的限制条件如下：</p>
<img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220927164713758-20220927%2016:47:13.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220927164713758" style="zoom:33%;" />
</li>
<li><p>总结：用前一步迭代结果的 $\theta$ 产生大量数据，供当前 $\theta$ 进行学习</p>
</li>
</ul>
</li>
</ul>
<blockquote>
<p>代码：PPO-PyTorch</p>
</blockquote>
<p>ActionCritic组合</p>
<ul>
<li><p>让模型与当前的水平匹配，将 $R(\tau^n)-b$ 更新为 $R(\tau^n)-value$，value是critic网络要学习的结果</p>
</li>
<li><p>在PPO2版本中，限制好范围，例如 ε 取0.2；A&gt;0 是好事，此时 $p_\theta (a_t|s_t)$ 要越大越好，但有上界!</p>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220927193416972-20220927%2019:34:17.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220927193416972" style="zoom:50%;" />



<h4 id="q-learning和dqn算法"><a href="#q-learning和dqn算法" class="headerlink" title="q-learning和dqn算法"></a>q-learning和dqn算法</h4><p>眼前顺势奖励 + 记忆经验奖励</p>
<ul>
<li>瞬时奖励：做了一个动作就能获得的奖励</li>
<li>记忆经验奖励：按照训练时的经验，上一个动作发生后，接下来怎么做才能获得更大的奖励</li>
<li>DQN就是用神经网络来预测</li>
</ul>
<p>Q-learning</p>
<ul>
<li>收集数据：${(s_i,a_i,s_i’,r_i)}$，也就是玩游戏的记录，状态、动作、下个状态、奖励</li>
<li>令目标等于：$y_i \leftarrow r(s_i,a_i) + \gamma \max_{a_i’}Q_\empty (s_i’,s_i’)$</li>
<li>目标函数：$\arg \max_\empty 1&#x2F;2 \sum_i||Q_\empty(s_i,a_i)-y_i||$</li>
</ul>
<p>以密室逃脱为例</p>
<ul>
<li>设定5是终点，到5会有更大奖励</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220927204326236-20220927%2020:43:26.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220927204326236"></p>
<ul>
<li>初始化Q和R：Q现在看起来是一个空表，要不断进行填充(行为state，列为action)</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220927204414122-20220927%2020:44:14.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220927204414122"></p>
<ul>
<li><p>迭代方式：Q(state, action) &#x3D; R(state, action) + Gamma * Max[A(next state, all actions)]</p>
</li>
<li><p>举例如：Q(1,5) &#x3D; Q(1,5) + 0.8 * Max[Q(5,1), Q(5,4), Q(5,5)] &#x3D; 100 + 0.8 * 0 &#x3D; 100</p>
</li>
<li><p>其中0.8是折扣因子，相当于经验记忆奖励的权重</p>
</li>
<li><p>重复迭代，得到收敛后的模型结果</p>
</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220927205147353-20220927%2020:51:47.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220927205147353"></p>
<p>DQN</p>
<ul>
<li>先玩一阵，得到记录数据，从记录中取一个batch，target与main相同网络结构，损失函数就是回归问题</li>
<li>状态很难穷举时，Q(s,a) 可以用神经网络来做</li>
<li>数据获取：构建一个replay buffers，用的时候去里面取一个batch就行</li>
<li>off policy策略</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220927205433174-20220927%2020:54:33.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220927205433174" style="zoom:50%;" />

<blockquote>
<p>代码：DQN</p>
</blockquote>
<p>改进：Rainbow，加上各种套路（Double-DQN，Dueling-DQN，MultiStep）</p>
<p>Double-DQN</p>
<ul>
<li>随着游戏的进行，期望的Q会越大越大!</li>
<li>引入一个双重保险B：$Q_{\empty A}(s,a) \leftarrow r + \gamma Q_{\empty B}(s’, \arg \underset {a’}\max Q_{\empty A}(s’,a’))$</li>
</ul>
<p>Dueling-DQN</p>
<ul>
<li>让网络有举一反三的能力，而不是一次更新当前的结果</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220927213822044-20220927%2021:38:22.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220927213822044" style="zoom:80%;" />

<ul>
<li>网络结构中多输出一个S值，可以当作偏置项：$Q(s,a) &#x3D; A(s,a) + V(s)$</li>
<li>其实就是让网络能同时更新相同state下不同action可能导致的结果，一般迭代的时候就更新当前状态下，某一个action的结果</li>
<li>限制网络不要只更新A值：让A中所有列的值加起来恒为0</li>
</ul>
<p>MultiStep-DQN</p>
<ul>
<li>将眼界放远，不光看眼前，还要看下N步的结果</li>
<li>MultiStep就是计算Q值的时候选择多个时间步</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20220927215705350-20220927%2021:57:05.png" srcset="/blog/img/loading.gif" lazyload alt="image-20220927215705350" style="zoom:30%;" />

<ul>
<li>同梯度下降中的，随机&#x2F;批量，这个就是小批量</li>
</ul>
<p>Continuous actions</p>
<ul>
<li>针对连续值</li>
<li>可以采样：遍历尽可能多的情况，得到一个最大值就好了</li>
<li>为此，重新定义一个Q网络，输出三个结果，分别是向量、矩阵、值</li>
<li>新的Q：$Q_\empty (s,a) &#x3D; -1&#x2F;2(a-\mu_\empty(s))^TP_\empty(s)(a-\mu_\empty(s)) + V_\empty(s)$</li>
<li>因为$(a-\mu_\empty(s))^TP_\empty(s)(a-\mu_\empty(s))$恒为正，所以$action &#x3D; \mu_\empty(s)$</li>
<li>此时带入Q(s,a)即可求出：$max_aQ_\empty(s,a) &#x3D; V_\empty(s)$</li>
</ul>
<h4 id="actor-critic算法-a3c"><a href="#actor-critic算法-a3c" class="headerlink" title="actor-critic算法(a3c)"></a>actor-critic算法(a3c)</h4><p>优势函数</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20221007215340539-20221007%2021:53:40.png" srcset="/blog/img/loading.gif" lazyload alt="image-20221007215340539"></p>
<ul>
<li>函数表达式：$A^\pi(s_t,a_t) &#x3D; Q^\pi(s_t,a_t) - V^\pi(s_t)$</li>
<li>就是在状态s下，选择某一动作有多好，Q相当于得到的，V是期望的（平均）</li>
<li>如果A值是正的，则正激励</li>
</ul>
<p>两个网络训练有难度，做近似简化为一个网络：$A^\pi(s_t,a_t) \approx r(s_t,a_t) + V^\pi(s_{t+1}) - V^\pi(s_t)$</p>
<p>AC整体流程</p>
<ol>
<li>获取数据：${s_i,a_i}$（不断与环境交互，通过策略$\pi_\theta (a|s)$</li>
<li>前向传播计算：$A^\pi(s_i,a_i) &#x3D; r(s_i,a_i) + V_\empty^\pi(s’_i) - V_\empty^\pi(s_i)$</li>
<li>计算梯度：$\nabla _\theta J(\theta) \approx \sum_i \nabla_\theta log \pi_\theta(a_i|s_i)A^\pi(s_i,a_i)$</li>
<li>更新参数：$\theta \longleftarrow \theta + \alpha\nabla_\theta J(\theta)$</li>
</ol>
<p>多个智能体（多线程），每一个都单独与环境交互</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20221012202537913-20221012%2020:25:38.png" srcset="/blog/img/loading.gif" lazyload alt="image-20221012202537913"></p>
<p>损失函数</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20221012202556945-20221012%2020:25:57.png" srcset="/blog/img/loading.gif" lazyload alt="image-20221012202556945"></p>
<blockquote>
<p>代码例：Super-mario（用A3C玩超级马里奥）</p>
</blockquote>
<h3 id="13-模型部署与剪枝优化"><a href="#13-模型部署与剪枝优化" class="headerlink" title="13. 模型部署与剪枝优化"></a>13. 模型部署与剪枝优化</h3><h4 id="tensorrt"><a href="#tensorrt" class="headerlink" title="tensorrt"></a>tensorrt</h4><h4 id="tensorflow-serving"><a href="#tensorflow-serving" class="headerlink" title="tensorflow-serving"></a>tensorflow-serving</h4><h4 id="network-slimming"><a href="#network-slimming" class="headerlink" title="network-slimming"></a>network-slimming</h4><h4 id="mobilenet"><a href="#mobilenet" class="headerlink" title="mobilenet"></a>mobilenet</h4><h3 id="14-自然语言处理"><a href="#14-自然语言处理" class="headerlink" title="14. 自然语言处理"></a>14. 自然语言处理</h3><h3 id="15-bert"><a href="#15-bert" class="headerlink" title="15.bert"></a>15.bert</h3><h3 id="16-知识图谱"><a href="#16-知识图谱" class="headerlink" title="16. 知识图谱"></a>16. 知识图谱</h3><p>应用领域：</p>
<ul>
<li>搜索引擎</li>
<li>医疗查询（智能问答助手、辅助决策）</li>
<li>金融：反欺诈</li>
<li>推荐系统</li>
</ul>
<p>数据从哪里来：一段话：抽取实体、语义标签、二元关系、多元关系、事件</p>
<p>大量设计nlp技术</p>
<p>关系做得准确才可靠</p>
<p>常用技术点</p>
<ul>
<li><p>命名实体识别</p>
</li>
<li><p>给词打上标签</p>
</li>
<li><p>将标签和意图转换成sql</p>
</li>
<li><p>基于实体与关系构建知识图谱网络图（关系抽取）</p>
</li>
<li><p>实体统一 &#x2F; 指代消解</p>
</li>
</ul>
<p>Graph embedding</p>
<ul>
<li>风控模型中对节点进行编码</li>
<li>根据用户关系（通讯录）建立算法模型（Deep walk），获得用户向量</li>
<li>根据特征编码做预测分析</li>
<li>难点在于如何编码（算法）才能更准确体现这个用户的情况</li>
</ul>
<p>知识融合</p>
<ul>
<li>融合特征，得到最终的向量</li>
</ul>
<h4 id="图数据库：neo4j"><a href="#图数据库：neo4j" class="headerlink" title="图数据库：neo4j"></a>图数据库：neo4j</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs neo4j">Neo4j增删改查：<br><br>增：<br>增加一个节点<br>create (n:Person &#123;name:&#x27;我&#x27;,age:31&#125;)<br>带有关系属性<br>create (p:Person&#123;name:&quot;我&quot;,age:&quot;31&quot;&#125;)-[:包工程&#123;金额:10000&#125;]-&gt;(n:Person&#123;name:&quot;好大哥&quot;,age:&quot;35&quot;&#125;)<br><br>删<br>create (n:Person &#123;name:&#x27;TYD&#x27;,age:31&#125;)<br>match (n:Person&#123;name:&quot;TYD&quot;&#125;) delete n<br>删除关系<br>match (p:Person&#123;name:&quot;我&quot;,age:&quot;31&quot;&#125;)-[f:包工程]-&gt;(n:Person&#123;name:&quot;好大哥&quot;,age:&quot;35&quot;&#125;)<br>delete f<br><br>改：<br>加上标签<br>match (t:Person) where id(t)=789 set t:好人return t<br>加上属性<br>match (a:好人) where id(a)=789 set a.战斗力=200 return a<br>修改属性<br>match (a:好人) where id(a)=789 set a.战斗力=500 return a<br><br>查：(查操作太多啦，直接参考neo4j例子就好)<br>match (p:Person) - [:包工程] -&gt; (n:Person) return p,n<br>快速清空数据库：<br>MATCH (n)<br>DETACH DELETE n<br></code></pre></td></tr></table></figure>

<blockquote>
<p>使用python操作neo4j示例：pandasDemo</p>
</blockquote>
<h4 id="基于知识图谱的医疗问答系统（可运行）"><a href="#基于知识图谱的医疗问答系统（可运行）" class="headerlink" title="基于知识图谱的医疗问答系统（可运行）"></a>基于知识图谱的医疗问答系统（可运行）</h4><blockquote>
<p>代码：QAmedicalKG</p>
<p>运行：build_medicalgraph -&gt; chatbot_graph</p>
</blockquote>
<h4 id="文本数据关系抽取"><a href="#文本数据关系抽取" class="headerlink" title="文本数据关系抽取"></a>文本数据关系抽取</h4><blockquote>
<p>代码：Extraction</p>
<p>参考：<a target="_blank" rel="noopener" href="http://ltp.ai/index.html">http://ltp.ai/index.html</a></p>
</blockquote>
<p>依存句法</p>
<p>语义角色标注</p>
<h4 id="金融平台风控模型"><a href="#金融平台风控模型" class="headerlink" title="金融平台风控模型"></a>金融平台风控模型</h4><blockquote>
<p>来源竞赛：<a target="_blank" rel="noopener" href="http://openresearch.rong360.com/#/">http://openresearch.rong360.com/#/</a></p>
</blockquote>
<p>deepwalk</p>
<h4 id="医学糖尿病数据命名实体识别"><a href="#医学糖尿病数据命名实体识别" class="headerlink" title="医学糖尿病数据命名实体识别"></a>医学糖尿病数据命名实体识别</h4><h3 id="17-语音识别"><a href="#17-语音识别" class="headerlink" title="17. 语音识别"></a>17. 语音识别</h3><h4 id="seq2seq-x2F-las模型"><a href="#seq2seq-x2F-las模型" class="headerlink" title="seq2seq&#x2F;las模型"></a>seq2seq&#x2F;las模型</h4><p>encoder - attation - decoder</p>
<p>teacher forching: 用true label传到下一个，以防错误传播</p>
<blockquote>
<p>LAS模型语音识别代码：LAS-Pytorch</p>
</blockquote>
<h4 id="语音分离-convtasnet"><a href="#语音分离-convtasnet" class="headerlink" title="语音分离 convtasnet"></a>语音分离 convtasnet</h4><p>输入为混合的声音，输出各个讲话者单独的声音</p>
<p>经典的Deep Clustering</p>
<ol>
<li>拿到声音信号</li>
<li>做embedding：每个点变成向量，当做一个样本</li>
<li>对当前样本做一个聚类（k-means等）</li>
<li>把得到的label结果当作mask矩阵即可，分离出来，就得到k个mask</li>
<li>用mask还原到原始数据，得到k个结果，分离结束</li>
</ol>
<p>Cons-TasNet</p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20221015163640876-20221015%2016:36:41.png" srcset="/blog/img/loading.gif" lazyload alt="image-20221015163640876"></p>
<p><img src="https://cdn.jsdelivr.net/gh/Yoonalis/img@master/blog/image-20221015163712334-20221015%2016:37:12.png" srcset="/blog/img/loading.gif" lazyload alt="image-20221015163712334"></p>
<p>网络结构细节</p>
<ul>
<li>全卷积</li>
<li>更大的感受野：多次重复1-D卷积、空洞卷积、DW卷积（depthwise、pointwise）</li>
<li>纯语音输入输出</li>
<li>end to end</li>
</ul>
<blockquote>
<p>代码：Conv-TasNet</p>
</blockquote>
<h4 id="语音合成-tacotron"><a href="#语音合成-tacotron" class="headerlink" title="语音合成 tacotron"></a>语音合成 tacotron</h4><blockquote>
<p>地址：<a target="_blank" rel="noopener" href="https://github.com/NVIDIA/tacotron2">https://github.com/NVIDIA/tacotron2</a></p>
<p>代码：语音合成tacotron2实战</p>
</blockquote>
<h3 id="18-推荐系统"><a href="#18-推荐系统" class="headerlink" title="18. 推荐系统"></a>18. 推荐系统</h3><h4 id="协同过滤与矩阵分解"><a href="#协同过滤与矩阵分解" class="headerlink" title="协同过滤与矩阵分解"></a>协同过滤与矩阵分解</h4><p>基于用户的协同过滤算法（算相似度）</p>
<p>基于物品的协同过滤算法（算相似度）</p>
<p>矩阵分解（让矩阵更容易）</p>
<p>矩阵分解中的隐式与显式情况解决思路</p>
<p>基于用户的协同过滤</p>
<ul>
<li>找到相似用户（相似度计算）</li>
<li>属性特征、行为特征等都可以当作计算输入</li>
<li>用的不多：用户多、数据稀疏、计算相似度矩阵较难、新用户问题</li>
</ul>
<p>基于物品的协同过滤</p>
<ul>
<li>先得到用户和商品的交互数据</li>
<li>经常被同时购买的商品</li>
<li>用的较多：商品种类比较少、属性比较固定、适合非实时场景</li>
</ul>
<p>矩阵分解</p>
<ul>
<li>推荐系统中用的最多的方法</li>
<li>User-item（m * n）分解为user（m * k）和item（k * n）两个矩阵</li>
</ul>
<p>隐向量：特征的高维表达</p>
<p>目标函数</p>
<ul>
<li><p>类似回归方程：$min_{X,Y}\sum_{r_{ui\neq0}}(r_{ui} - x_u^Ty_i)^2 + \lambda(\sum_u||x_u||^2_2 + \sum_i||y_i||^2_2)$</p>
</li>
<li><p>用户矩阵 X ，商品矩阵 Y</p>
</li>
<li><p>额外引入正则化惩罚项</p>
</li>
<li><p>再加入用户偏置项（bu）与商品偏置项（bi）</p>
</li>
</ul>
<p>隐式情况分析</p>
<ul>
<li><p>观看时间、点击次数等指标</p>
</li>
<li><p>定义置信度：$c_{ui} &#x3D; 1 + \alpha r_{ui}$</p>
</li>
<li><p>置信度默认为1，表示用户没有产生行为的商品；行为越多，置信度越大</p>
</li>
<li><p>重新定义评分：$$p_{ui} &#x3D; \begin{cases} 1&amp; {r_{ui} &gt; 0}\0&amp; {r_{ui} &#x3D; 0}\end{cases}$$</p>
</li>
<li><p>新的优化目标：$G(x_*,y_*) &#x3D; (\sum_{u,i}c_{ui}(p_{ui}-x_u^Ty_i)^2) + \lambda(\sum_u||x_u||^2 + \sum_i||y_i||^2)$</p>
</li>
<li><p>总结就是置信度越大，预测得越准</p>
</li>
<li><p>求解过程交替使用最小二乘法，先固定y优化x，再固定x优化y</p>
</li>
</ul>
<p>Embedding</p>
<ul>
<li><p>兴趣标签</p>
</li>
<li><p>用户画像</p>
</li>
<li><p>行为序列（点击历史）</p>
</li>
</ul>
<h4 id="fm与deepfm"><a href="#fm与deepfm" class="headerlink" title="fm与deepfm"></a>fm与deepfm</h4><h4 id="常用工具包"><a href="#常用工具包" class="headerlink" title="常用工具包"></a>常用工具包</h4><h3 id="19-diffusion-model"><a href="#19-diffusion-model" class="headerlink" title="19. diffusion model"></a>19. diffusion model</h3>
                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/blog/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="category-chain-item">深度学习</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/blog/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">#深度学习</a>
      
        <a href="/blog/tags/%E8%AE%BA%E6%96%87/">#论文</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>论文选题</div>
      <div>https://yoonalis.github.io/blog/2022/12/04/深度学习/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Azure</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2022年12月4日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/blog/2022/11/28/umi/" title="umi基础">
                        <span class="hidden-mobile">umi基础</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/blog/js/events.js" ></script>
<script  src="/blog/js/plugins.js" ></script>


  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/blog/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/blog/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  
<script src="//cdn.jsdelivr.net/gh/bynotes/texiao/source/js/caidai.js"></script>
<script src="//cdn.jsdelivr.net/gh/bynotes/texiao/source/js/love.js"></script>



<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/blog/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
